{
  "hash": "80aa1fce23d42968d5e5fc9b92a5ed7a",
  "result": {
    "markdown": "# Telematic Covariates\n\n## Preamble\n\n::: {.panel-tabset}\n\n### Chapter Objective\n\nWe continue our analysis of claim severity based on the available covariates in the database.  Compared to the previous chapter, we are now adding telematics variables to the exercise while removing protected variables. Thus, the following five covariates are excluded, for the moment, from the analysis:  \n\n   1) Credit.score,  \n   2) Insured age,  \n   3) Insured.sex,  \n   4) Marital, and  \n   5) Territory.\n\nWe use the same two main models as in the previous chapters, namely Generalized Linear Model (GLM) family, including elastic-net and XGBoost. For each model, the response variable is the average cost of a claim, given that at least one claim has occurred.  To analyze severities, we still use the same two scores used previously.   \n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nScore.pred.sev <- function(mu, x, phi) {\n  Sc.log  <- -sum(dgamma(x, shape = 1/phi, scale = mu*phi, log=TRUE))\n  Sc.MSE  <- sum((x - mu)^2)/1000000\n  return(c(Sc.log, Sc.MSE))\n}\n```\n:::\n\n\n\n\n### Packages\n\nFor this chapter, we need the following packages:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nlibrary(vtable)\nlibrary(rpart)\nlibrary(repr)\nlibrary(rpart.plot)\nlibrary(gam)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(glmnet)\nlibrary(scoringRules)\nlibrary(sjPlot)\n```\n:::\n\n\n### Data\n\nIn this chapter, we conduct analyses using the same data as in the previous chapters amd we use the same train/test division.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndataS <- read.csv('Data/Synthetic.csv')\n\ndata <- dataS[dataS$AMT_Claim > 0,]\ndata$M_Claim <- data$AMT_Claim/data$NB_Claim\n\n# Modifications \ndata <- data %>%\n  mutate(Territory = as.factor(Territory)) %>%\n  select(-c('Annual.pct.driven', 'Annual.miles.drive'))\n\ndata.select <- data\n\n# Train-test \nset.seed(123)\ntrain <- data.select %>% sample_frac(0.8, replace = FALSE)\ntest <- data.select %>% anti_join(train)\n\ntest <- test[-640,]\n```\n:::\n\n\n### Data Transformation\n\nAs we concluded at the end of our overview of the data, certain variables also need to be transformed.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Modif data\ntrain2 <- train %>%\n  mutate(Miles.per.day = Total.miles.driven/Duration,\n         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),\n         expo = Duration/365.25)\n\ntransform.fct <- function(var){\n  df <- train2 %>% mutate(var_ = get(var)*Total.miles.driven/(1000*Duration))\n  q99 <- quantile(df$var_, 0.99)\n  df <- df %>% mutate(var_ = ifelse(var_ > q99, q99, var_))\n  #colnames(df)[ncol(df)] <- paste0(var, '_')\n  return(df)\n}\n\ntrain2 <- transform.fct(\"Brake.06miles\")\ntrain2 <- transform.fct(\"Brake.08miles\")\ntrain2 <- transform.fct(\"Brake.09miles\")\ntrain2 <- transform.fct(\"Brake.11miles\")\ntrain2 <- transform.fct(\"Brake.14miles\")\ntrain2 <- transform.fct(\"Accel.06miles\")\ntrain2 <- transform.fct(\"Accel.08miles\")\ntrain2 <- transform.fct(\"Accel.09miles\")\ntrain2 <- transform.fct(\"Accel.11miles\")\ntrain2 <- transform.fct(\"Accel.12miles\")\ntrain2 <- transform.fct(\"Accel.14miles\")\ntrain2 <- transform.fct(\"Left.turn.intensity08\")\ntrain2 <- transform.fct(\"Left.turn.intensity09\")\ntrain2 <- transform.fct(\"Left.turn.intensity10\")\ntrain2 <- transform.fct(\"Left.turn.intensity11\")\ntrain2 <- transform.fct(\"Left.turn.intensity12\")\ntrain2 <- transform.fct(\"Right.turn.intensity08\")\ntrain2 <- transform.fct(\"Right.turn.intensity09\")\ntrain2 <- transform.fct(\"Right.turn.intensity10\")\ntrain2 <- transform.fct(\"Right.turn.intensity11\")\ntrain2 <- transform.fct(\"Right.turn.intensity12\")\n\n# Create folds\nnb.fold <- 5\nfold <- sample(1:nb.fold, nrow(train2), replace = TRUE)\ntrain2$fold <- fold\n\n##\n\ntest2 <- test %>%\n  mutate(Miles.per.day = Total.miles.driven/Duration,\n         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),\n         expo = Duration/365.25)\n\ntransform.fct <- function(var){\n  df <- test2 %>% mutate(var_ = get(var)*Total.miles.driven/(1000*Duration))\n  q99 <- quantile(df$var_, 0.99)\n  df <- df %>% mutate(var_ = ifelse(var_ > q99, q99, var_))\n  #colnames(df)[ncol(df)] <- paste0(var, '_')\n  return(df)\n}\n\ntest2 <- transform.fct(\"Brake.06miles\")\ntest2 <- transform.fct(\"Brake.08miles\")\ntest2 <- transform.fct(\"Brake.09miles\")\ntest2 <- transform.fct(\"Brake.11miles\")\ntest2 <- transform.fct(\"Brake.14miles\")\ntest2 <- transform.fct(\"Accel.06miles\")\ntest2 <- transform.fct(\"Accel.08miles\")\ntest2 <- transform.fct(\"Accel.09miles\")\ntest2 <- transform.fct(\"Accel.11miles\")\ntest2 <- transform.fct(\"Accel.12miles\")\ntest2 <- transform.fct(\"Accel.14miles\")\ntest2 <- transform.fct(\"Left.turn.intensity08\")\ntest2 <- transform.fct(\"Left.turn.intensity09\")\ntest2 <- transform.fct(\"Left.turn.intensity10\")\ntest2 <- transform.fct(\"Left.turn.intensity11\")\ntest2 <- transform.fct(\"Left.turn.intensity12\")\ntest2 <- transform.fct(\"Right.turn.intensity08\")\ntest2 <- transform.fct(\"Right.turn.intensity09\")\ntest2 <- transform.fct(\"Right.turn.intensity10\")\ntest2 <- transform.fct(\"Right.turn.intensity11\")\ntest2 <- transform.fct(\"Right.turn.intensity12\")\n\n# Mean Encoding with White Noise pour les territoires\ncardi <- length(unique(train$Territory))\n\nenc.terr <- train2 %>%\n  group_by(Territory) %>%\n  summarize(freq = sum(NB_Claim)/sum(expo)) %>%\n  arrange(freq) %>%\n  mutate(terr.code= row_number()/(cardi+1)) %>%\n  select(Territory, terr.code)\n\ntrain2 <- train2 %>%\n  group_by(Territory) %>%\n  left_join(enc.terr, by='Territory') %>%\n  ungroup()\n\ntest2 <- test2 %>%\n  group_by(Territory) %>%\n  left_join(enc.terr, by='Territory') %>%\n  ungroup()\n```\n:::\n\n\n\n:::\n\n\n\n\n## Basic GLM Models\n\n::: {.panel-tabset}\n\n### Single intercept\n\nFor comparison, we use a baseline model corresponding to a GLM with an intercept that predicts only the mean severity multiplied by the observed frequency for each contract. We do not present the results based on the *train* dataset to avoid burdening the report unnecessarily. However, if necessary, uncomment the last portion of the code to produce the results table.\n\n\n::: {#tbl-Pscore_base_sev_tel .cell tbl-cap='Prediction scores for the base model (severity)'}\n\n```{.r .cell-code  code-fold=\"true\"}\n## Model on each fold\nResult_  <- data.frame()\nResult2_  <- data.frame()\nfor(i in 1:nb.fold) {\n  learn <- train2[train2$fold != i,]\n  valid <- train2[train2$fold == i,]\n  \n  mean <- sum(learn$AMT_Claim)/sum(learn$NB_Claim) \n  variance <- sd(learn$AMT_Claim)^2\n  phi <- variance/mean(learn$AMT_Claim)^2\n  \n  learn$pred.base <- mean*learn$NB_Claim\n  valid$pred.base <- mean*valid$NB_Claim\n  \n  Result_ <- rbind(Result_, c(i, Score.pred.sev(valid$pred.base, valid$AMT_Claim, phi)/nrow(valid)))\n  Result2_ <- rbind(Result2_, c(i, Score.pred.sev(valid$pred.base, valid$AMT_Claim, phi)))\n}\n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ncolnames(Result2_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ntot <- colSums(Result2_)/nrow(train2)\ntot$Fold <- 'Total'\nResult_ <- rbind(Result_ , tot)\n\nResult.base <- Result_  \nBase <- Result.base[nb.fold+1,]\n\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> Fold </th>\n   <th style=\"text-align:center;\"> Sc.log </th>\n   <th style=\"text-align:center;\"> Sc.MSE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> 1 </td>\n   <td style=\"text-align:center;\"> 9.28096 </td>\n   <td style=\"text-align:center;\"> 48.72725 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 2 </td>\n   <td style=\"text-align:center;\"> 9.28378 </td>\n   <td style=\"text-align:center;\"> 21.80556 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 3 </td>\n   <td style=\"text-align:center;\"> 9.30844 </td>\n   <td style=\"text-align:center;\"> 31.17175 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 4 </td>\n   <td style=\"text-align:center;\"> 9.34049 </td>\n   <td style=\"text-align:center;\"> 30.61554 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 5 </td>\n   <td style=\"text-align:center;\"> 9.26105 </td>\n   <td style=\"text-align:center;\"> 18.96634 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> Total </td>\n   <td style=\"text-align:center;\"> 9.29513 </td>\n   <td style=\"text-align:center;\"> 30.43693 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe model is, therefore, estimated on the entire *train* database, and the predictions are made on the *test* database, which was not used during the calibration phase.\n\n\n::: {#tbl-Pscore_basetest_sev_tel .cell tbl-cap='Prediction scores for the base model (testing set) (severity)'}\n\n```{.r .cell-code  code-fold=\"true\"}\nmean <- sum(train2$AMT_Claim)/sum(train2$NB_Claim) \nvariance <- sd(train2$AMT_Claim)^2\nphi <- variance/mean(train2$AMT_Claim)^2 \n  \ntest2$pred.base <- mean*test2$NB_Claim\n\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('Base', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- Result_\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> Model </th>\n   <th style=\"text-align:center;\"> Sc.log </th>\n   <th style=\"text-align:center;\"> Sc.MSE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> Base </td>\n   <td style=\"text-align:center;\"> 9.29504 </td>\n   <td style=\"text-align:center;\"> 21.82679 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n### Traditional covariates already used (without protected variables)\n\nWe construct a first GLM with only the following covariates:  \n\n  - Car.use,  \n  - Region,  \n  - Car.age, and\n  - Years.noclaims.  \n\nWe calculate the prediction scores of the model with all categorical covariates on the *train* and the *test* dataset. As expected, adding segmentation variables improves the prediction scores compared to the simple baseline model with only an intercept.\n\n\n::: {#tbl-TeleGLM2_sev .cell tbl-cap='Prediction scores for the GLM model with traditional covariates (without protected variables) (severity)'}\n\n```{.r .cell-code  code-fold=\"true\"}\n## Model \nscore.base <- as.formula(M_Claim ~ 1)\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2))\n## Model on each fold\nResult_  <- data.frame()\nResult2_  <- data.frame()\nfor(i in 1:nb.fold) {\n  learn <- train2[train2$fold != i,]\n  valid <- train2[train2$fold == i,]\n  glm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = learn)\n  \n  learn$pred.base <- predict(glm.fit, newdata=learn, type='response')*learn$NB_Claim\n  valid$pred.base <- predict(glm.fit, newdata=valid, type='response')*valid$NB_Claim\n  phi <- summary(glm.fit)$dispersion\n  \n  Result_ <- rbind(Result_, c(i, Score.pred.sev(valid$pred.base, valid$AMT_Claim, phi)/nrow(valid)))\n  Result2_ <- rbind(Result2_, c(i, Score.pred.sev(valid$pred.base, valid$AMT_Claim, phi)))\n}\n\n## Model on all data from train\nglm.base <- glm(score.base, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\ntrain2$pred.glm1 <- predict(glm.fit, newdata=train2, type='response')*train2$NB_Claim\nphi <- summary(glm.fit)$dispersion\nResult.glm1 <- Result_  \n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ncolnames(Result2_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ntot <- colSums(Result2_)/nrow(train2)\ntot$Fold <- 'Total'\nResult_ <- rbind(Result_ , tot)\nResult_ <- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] <- 'Improvement'\n\nfor(i in 2:3){\n  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) <- NULL\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> Fold </th>\n   <th style=\"text-align:center;\"> Sc.log </th>\n   <th style=\"text-align:center;\"> Sc.MSE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> 1 </td>\n   <td style=\"text-align:center;\"> 9.22622 </td>\n   <td style=\"text-align:center;\"> 47.33748 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 2 </td>\n   <td style=\"text-align:center;\"> 9.22946 </td>\n   <td style=\"text-align:center;\"> 22.23070 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 3 </td>\n   <td style=\"text-align:center;\"> 9.25050 </td>\n   <td style=\"text-align:center;\"> 30.70171 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 4 </td>\n   <td style=\"text-align:center;\"> 9.27857 </td>\n   <td style=\"text-align:center;\"> 29.97837 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 5 </td>\n   <td style=\"text-align:center;\"> 9.20165 </td>\n   <td style=\"text-align:center;\"> 17.97995 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> Total </td>\n   <td style=\"text-align:center;\"> 9.23751 </td>\n   <td style=\"text-align:center;\"> 29.82819 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> Improvement </td>\n   <td style=\"text-align:center;\"> -0.05762 </td>\n   <td style=\"text-align:center;\"> -0.60874 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n::: {#tbl-Pscore_basetest2_sev .cell tbl-cap='Prediction scores for the GLM model with traditional covariates (testing set) (severity)'}\n\n```{.r .cell-code  code-fold=\"true\"}\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2))\n\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\ntest2$pred.base <- predict(glm.fit, newdata=test2, type='response')*test2$NB_Claim\nphi <- summary(glm.fit)$dispersion\n\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('GLM (trad.)', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> Model </th>\n   <th style=\"text-align:center;\"> Sc.log </th>\n   <th style=\"text-align:center;\"> Sc.MSE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> Base </td>\n   <td style=\"text-align:center;\"> 9.29504 </td>\n   <td style=\"text-align:center;\"> 21.82679 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> GLM (trad.) </td>\n   <td style=\"text-align:center;\"> 9.23655 </td>\n   <td style=\"text-align:center;\"> 21.16556 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n### Estimated parameters\n\nThe table below shows the estimators obtained for the GLM-Gamma approach and compares them with the baseline model, which has only an intercept. We note that except for the variable \"Car.use\", the traditional variables do not seem significant in the model.\n\n\n::: {#tbl-TeleGLM1_sev .cell tbl-cap='Estimated parameters for the GLM model with traditional covariates (without protected variables) (severity)'}\n\n```{.r .cell-code  code-fold=\"true\"}\n## Model \nscore.base <- as.formula(M_Claim ~ 1)\n\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2))\n\n## Model on all data from train\nglm.base <- glm(score.base, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ntab_model(glm.base, glm.fit, transform = NULL)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">M Claim</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">M Claim</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  col7\">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">8.12</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">8.07&nbsp;&ndash;&nbsp;8.18</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">8.30</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">7.95&nbsp;&ndash;&nbsp;8.66</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Car use [Commute]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.19</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.10&nbsp;&ndash;&nbsp;0.46</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\">0.186</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Car use [Farmer]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.91</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;1.61&nbsp;&ndash;&nbsp;-0.07</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\"><strong>0.019</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Car use [Private]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.04</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.26&nbsp;&ndash;&nbsp;0.32</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\">0.773</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Region [Urban]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.05</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.09&nbsp;&ndash;&nbsp;0.19</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\">0.440</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Car age</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.03</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.07&nbsp;&ndash;&nbsp;0.01</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\">0.141</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Car age^2</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.00</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.00&nbsp;&ndash;&nbsp;0.00</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\">0.973</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Years noclaims</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.01</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.02&nbsp;&ndash;&nbsp;0.01</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\">0.421</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Years noclaims^2</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.00</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.00&nbsp;&ndash;&nbsp;0.00</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\">0.181</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">3091</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">3091</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">R<sup>2</sup> Nagelkerke</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.000</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.098</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n\n:::\n\n  \n\n\n## GLM-Net\n\nFirst, we consider the GLM-net model. To make the approach as effective as possible, we need to adjust the continuous segmentation variables.\n\n### Parametric transformation of telematic covariates\n  \nAs we did in the other chapter, we first introduce an approach using the Generalized Additive Models (GAM) theory for all continuous variables. \nThis approach allows us to observe the general form of the covariate to explain the severity. A parametric form will then be proposed to achieve the best possible correspondence with the spline obtained by the GAM.\n\n::: {.panel-tabset}\n\n### Vehicle Usage level\n\nFor the two covariates related to usage level, the proposed parametric forms are as follows:\n\n\\begin{align*}\ns(Miles.per.day) \\approx& Miles.per.day + log(Miles.per.day)\\\\\ns(Avgdays.week) \\approx& Avgdays.week + Avgdays.week^2 + Avgdays.week^3\n\\end{align*}\n\nThe graphs below compare the fit of the parametric approach with that of the GAM model.\n\n\n::: {#fig-MpD_GAM_sev_tel .cell layout-ncol=\"2\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nmin_ <- min(train2$Miles.per.day) \nmax_ <- max(train2$Miles.per.day) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'Miles.per.day'\n\nq99 <- quantile(train2$Miles.per.day, 0.99)\n\ndb <- train2 %>%\n  select(-'Miles.per.day') %>%\n  dplyr::slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)\n                        + s(Miles.per.day))\n\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)\n                        + Miles.per.day + log(Miles.per.day))\n\ngam.fit <- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase <- db %>%\n  mutate(diff = abs(Miles.per.day - mean(train2$Miles.per.day))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=Miles.per.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=Miles.per.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Miles per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n##\n\nmin_ <- quantile(train2$Avgdays.week, 0.01) \nmax_ <- max(train2$Avgdays.week) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'Avgdays.week'\n\nq99 <- quantile(train2$Avgdays.week, 0.99)\n\ndb <- train2 %>%\n  select(-'Avgdays.week') %>%\n  dplyr::slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)\n                        + s(Avgdays.week))\n\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)\n                        + Avgdays.week + I(Avgdays.week^2) + I(Avgdays.week^3))\n\ngam.fit <- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase <- db %>%\n  mutate(diff = abs(Avgdays.week - mean(train2$Avgdays.week))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=Avgdays.week, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=Avgdays.week, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Avgdays.week',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n```\n\n::: {.cell-output-display}\n![Miles.per.day](severityVarTelematique_files/figure-html/fig-MpD_GAM_sev_tel-1.png){#fig-MpD_GAM_sev_tel-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Avgdays.week](severityVarTelematique_files/figure-html/fig-MpD_GAM_sev_tel-2.png){#fig-MpD_GAM_sev_tel-2 width=672}\n:::\n\nSmoothing of Usage level covariates (severity)\n:::\n\n\n### Type of vehicle usage\n\nSeveral covariates are available in the category *Type of vehicle usage*:  \n\n- We propose the same parametric form for all variants of the variable *Pct.drive.day* (Monday to Sunday);  \n- The same parametric form will also be proposed For *Pct.drive.rush.am*, *Pct.drive.rush.pm*, *Pct.drive.2hrs*, *Pct.drive.3hrs*, and *Pct.drive.4hrs*;  \n- The other 3 covariates have their own parametric form.\n\nWe then have:\n\n\\begin{align*}\ns(Pct.drive.day) &\\approx Pct.drive.day + Pct.drive.day^2 \\\\\ns(Pct.drive) &\\approx Pct.drive + \\sqrt{Pct.drive} \\\\\ns(max.day) &\\approx max.day + max.day^2 + max.day^3 \\\\\ns(min.day) &\\approx min.day + min.day^2 + min.day^3 \\\\\ns(max.min) &\\approx max.min + max.min^2 \n\\end{align*}\n\n\n\n::: {#fig-MpD_GAM_sev_tel3 .cell layout-nrow=\"3\" layout-ncol=\"2\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ntrain2$Pct.drive <- train2$Pct.drive.mon\n\nmin_ <- min(train2$Pct.drive) \nmax_ <- max(train2$Pct.drive) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'Pct.drive'\n\nq99 <- quantile(train2$Pct.drive, 0.99)\n\ndb <- train2 %>%\n  select(-'Pct.drive') %>%\n  dplyr::slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)\n                        + s(Pct.drive))\n\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)\n                        + Pct.drive + I(Pct.drive^2))\n\ngam.fit <- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase <- db %>%\n  mutate(diff = abs(Pct.drive - mean(train2$Pct.drive))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=Pct.drive, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=Pct.drive, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Pct.drive',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Rush\n\ntrain2$use.day <- train2$Pct.drive.rush.am\n\nmin_ <- min(train2$use.day) \nmax_ <- max(train2$use.day) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'use.day'\n\nq99 <- quantile(train2$use.day, 0.99)\n\ndb <- train2 %>%\n  select(-'use.day') %>%\n  dplyr::slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2) \n                        + s(use.day))\n\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  \n                        + use.day + I(use.day^0.5))\n\ngam.fit <- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase <- db %>%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Max.day\n\ntrain2$use.day <- train2$max.day\n\nmin_ <- min(train2$use.day) \nmax_ <- max(train2$use.day) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'use.day'\n\nq99 <- quantile(train2$use.day, 0.99)\n\ndb <- train2 %>%\n  select(-'use.day') %>%\n  dplyr::slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2) \n                        + s(use.day))\n\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  \n                        + use.day + I(use.day^2) + I(use.day^3) )\n\ngam.fit <- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase <- db %>%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, 1) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Min.day\n\ntrain2$use.day <- train2$min.day\n\nmin_ <- min(train2$use.day) \nmax_ <- max(train2$use.day) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'use.day'\n\nq99 <- quantile(train2$use.day, 0.99)\n\ndb <- train2 %>%\n  select(-'use.day') %>%\n  dplyr::slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  \n                        + s(use.day))\n\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  \n                        + use.day + I(use.day^2)+ I(use.day^3) )\n\ngam.fit <- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase <- db %>%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Max.min\n\ntrain2$use.day <- train2$max.min\n\nmin_ <- min(train2$use.day) \nmax_ <- max(train2$use.day) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'use.day'\n\nq99 <- quantile(train2$use.day, 0.99)\n\ndb <- train2 %>%\n  select(-'use.day') %>%\n  dplyr::slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  \n                        + s(use.day))\n\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  \n                        + use.day + I(use.day^2) )\n\ngam.fit <- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase <- db %>%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n```\n\n::: {.cell-output-display}\n![Pct.drive.mon](severityVarTelematique_files/figure-html/fig-MpD_GAM_sev_tel3-1.png){#fig-MpD_GAM_sev_tel3-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Pct.drive.rush.am](severityVarTelematique_files/figure-html/fig-MpD_GAM_sev_tel3-2.png){#fig-MpD_GAM_sev_tel3-2 width=672}\n:::\n\n::: {.cell-output-display}\n![max.day](severityVarTelematique_files/figure-html/fig-MpD_GAM_sev_tel3-3.png){#fig-MpD_GAM_sev_tel3-3 width=672}\n:::\n\n::: {.cell-output-display}\n![min.day](severityVarTelematique_files/figure-html/fig-MpD_GAM_sev_tel3-4.png){#fig-MpD_GAM_sev_tel3-4 width=672}\n:::\n\n::: {.cell-output-display}\n![max.min](severityVarTelematique_files/figure-html/fig-MpD_GAM_sev_tel3-5.png){#fig-MpD_GAM_sev_tel3-5 width=672}\n:::\n\nSmoothing of Type of vehicle usage covariates (severity)\n:::\n\n\n\n\n\n\n\n### Driving behavior\n\nThe same parametric form is proposed for the different variants of the *Accel* and *Brake* variables, i.e., *Accel.06miles* to *Accel.14miles*, and *Brake.06miles* to *Brake.14miles*.  For the different variants of the *turn* variable, a single parametric form is also used: \n\n\\begin{align*}\ns(Brake.Accel) &\\approx Brake.Accel + Brake.Accel^2 + Brake.Accel^3\\\\\ns(Turn) &\\approx Turn + log(Turn)\n\\end{align*}\n\nThe graphs below compare the fit of the parametric approach for *Accel.06miles* and *Right.turn.intensity08* with that of the GAM model. \n  \n\n::: {#fig-MpD_GAM_sev_tel2 .cell layout-ncol=\"2\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ntrain2$use.day <- train2$Accel.06miles\n\nmin_ <- min(train2$use.day) \nmax_ <- max(train2$use.day) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'use.day'\n\nq99 <- quantile(train2$use.day, 0.99)\n\ndb <- train2 %>%\n  select(-'use.day') %>%\n  dplyr::slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2) \n                        + s(use.day))\n\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2) \n                        + use.day + I(use.day^2) + I(use.day^3) )\n\ngam.fit <- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase <- db %>%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\ntrain2$use.day <- train2$Right.turn.intensity08\n\nq99 <- quantile(train2$use.day, 0.99)\n\nmin_ <- min(train2$use.day) \nmax_ <- q99\nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'use.day'\n\ndb <- train2 %>%\n  select(-'use.day') %>%\n  dplyr::slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\ntemp <- train2 %>%\n  mutate(use.day = pmin(q99, use.day))\n\nscore.gam <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2) \n                        + s(use.day))\n\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)\n                        + use.day + log1p(use.day))\n\ngam.fit <- gam(score.gam, family = Gamma(link = \"log\"), data = temp)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = temp)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase <- db %>%\n  mutate(diff = abs(use.day - mean(temp$use.day))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  # xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n```\n\n::: {.cell-output-display}\n![Accel.06miles](severityVarTelematique_files/figure-html/fig-MpD_GAM_sev_tel2-1.png){#fig-MpD_GAM_sev_tel2-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Right.turn.intensity08](severityVarTelematique_files/figure-html/fig-MpD_GAM_sev_tel2-2.png){#fig-MpD_GAM_sev_tel2-2 width=672}\n:::\n\nSmoothing of Driving behavior covariates (severity)\n:::\n\n\n\n\n\n:::\n  \n  \n  \n  \n\n::: {.cell}\n\n:::\n\n\n\n### Fitting the GLM-Net model\n\nTo solve some convergence issues, we remove from all GML-Net models *Accel.* and *Brake.* terms.  \n\n\n::: {.cell}\n\n:::\n\n\n::: {.panel-tabset}\n\n\n### Optimal value\n\nThe parameters of the GLM-net were calibrated using cross-validation to obtain the model hyperparameters. It leads to a LASSO approach ($\\alpha = 1$). Using the optimal value of the penalty $\\lambda$, we can calculate the model's prediction scores based on all covariates.\n\n\n::: {#tbl-Pscore_teleGLMnet1_sev_tel .cell tbl-cap='Prediction scores for the GLM-net model'}\n\n```{.r .cell-code  code-fold=\"true\"}\nglm.score <- as.formula(M_Claim ~ \n                          Car.use + Region + Car.age + I(Car.age^2) + I(Car.age^3) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2) + I(Avgdays.week^3)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + I(max.day^2) + I(max.day^3) \n                        + min.day + I(min.day^2) + I(min.day^3)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\nResult_  <- data.frame()\nResult2_  <- data.frame()\nfor(i in 1:nb.fold) {\n    learn <- train2[train2$fold != i,]\n    valid <- train2[train2$fold == i,]\n    \n    matrix.x <- model.matrix(glm.score, data=learn)[,-1]\n    y <- learn$M_Claim\n\n    lambda.min <- 0.003162278\n    lambda.1se <- 0.05011872\n    \n    lambda.select <- lambda.min\n    fit <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, alpha = 1, lambda = lambda.select)\n    #fit <- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n    learn$pred <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*learn$NB_Claim\n    \n  \n    matrix.x <- model.matrix(glm.score, data=valid)[,-1]\n    y <- valid$M_Claim\n\n    valid$pred <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*valid$NB_Claim\n    variance <- (sum((learn$AMT_Claim - learn$pred)^2)/(nrow(learn) - length(fit$beta)))\n    phi <- variance/mean(learn$AMT_Claim)^2\n    \n    \n    Result_ <- rbind(Result_, c(i, Score.pred.sev(valid$pred, valid$AMT_Claim, phi)/nrow(valid)))\n    Result2_ <- rbind(Result2_, c(i, Score.pred.sev(valid$pred, valid$AMT_Claim, phi)))\n}\n\n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ncolnames(Result2_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ntot <- colSums(Result2_)/nrow(train2)\ntot$Fold <- 'Total'\nResult_ <- rbind(Result_ , tot)\nResult_ <- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] <- 'Improvement'\n\nfor(i in 2:3){\n  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\nrownames(Result_) <- NULL\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> Fold </th>\n   <th style=\"text-align:center;\"> Sc.log </th>\n   <th style=\"text-align:center;\"> Sc.MSE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> 1 </td>\n   <td style=\"text-align:center;\"> 9.22057 </td>\n   <td style=\"text-align:center;\"> 47.19565 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 2 </td>\n   <td style=\"text-align:center;\"> 9.23640 </td>\n   <td style=\"text-align:center;\"> 21.15998 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 3 </td>\n   <td style=\"text-align:center;\"> 9.24562 </td>\n   <td style=\"text-align:center;\"> 28.91845 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 4 </td>\n   <td style=\"text-align:center;\"> 9.27703 </td>\n   <td style=\"text-align:center;\"> 27.73318 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 5 </td>\n   <td style=\"text-align:center;\"> 9.21091 </td>\n   <td style=\"text-align:center;\"> 17.12773 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> Total </td>\n   <td style=\"text-align:center;\"> 9.23828 </td>\n   <td style=\"text-align:center;\"> 28.61542 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> Improvement </td>\n   <td style=\"text-align:center;\"> -0.05685 </td>\n   <td style=\"text-align:center;\"> -1.82152 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe same model can be used to compute the scores on the *test* set.\n  \n\n::: {#tbl-Pscore_basetest3_sev_tel .cell tbl-cap='Prediction scores for the GLM-net model  (testing set)'}\n\n```{.r .cell-code  code-fold=\"true\"}\nglm.score <- as.formula(M_Claim ~ \n                          Car.use + Region + Car.age + I(Car.age^2) + I(Car.age^3) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2) + I(Avgdays.week^3)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + I(max.day^2) + I(max.day^3) \n                        + min.day + I(min.day^2) + I(min.day^3)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$M_Claim\n\nlambda.min <- 0.003162278\nlambda.1se <- 0.05011872\n\nlambda.select <- lambda.min\nfit <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, alpha = 1, lambda = lambda.select)\n#fit <- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n\ntrain2$pred <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*train2$NB_Claim\ntrain2$pred.tele <- train2$pred\n\nmatrix.x <- model.matrix(glm.score, data=test2)[,-1]\ny <- test2$M_Claim\n\ntest2$pred.base <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*test2$NB_Claim\nvariance <- (sum((train2$AMT_Claim - train2$pred)^2)/(nrow(train2) - length(fit$beta)))\nphi <- variance/mean(train2$AMT_Claim)^2\n\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('LASSO (optimal)', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> Model </th>\n   <th style=\"text-align:center;\"> Sc.log </th>\n   <th style=\"text-align:center;\"> Sc.MSE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> Base </td>\n   <td style=\"text-align:center;\"> 9.29504 </td>\n   <td style=\"text-align:center;\"> 21.82679 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> GLM (trad.) </td>\n   <td style=\"text-align:center;\"> 9.23655 </td>\n   <td style=\"text-align:center;\"> 21.16556 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> LASSO (optimal) </td>\n   <td style=\"text-align:center;\"> 9.22435 </td>\n   <td style=\"text-align:center;\"> 19.83764 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n### Parsimonious model\n\nInstead of using the optimal value of the penalty $\\lambda$  in the elastic-net approach, it is often advised to use a penalty value located at one standard error ($\\lambda_{1se}$). This approach helps to obtain a more parsimonious model. The prediction scores of such a model are displayed below.\n\n\n::: {#tbl-Pscore_GLMnet2_sev_tel .cell tbl-cap='Prediction scores for the GLM-net model'}\n\n```{.r .cell-code  code-fold=\"true\"}\nResult_  <- data.frame()\nResult2_  <- data.frame()\nfor(i in 1:nb.fold) {\n  learn <- train2[train2$fold != i,]\n  valid <- train2[train2$fold == i,]\n  \n  matrix.x <- model.matrix(glm.score, data=learn)[,-1]\n  y <- learn$M_Claim\n  \n  lambda.min <- 0.003162278\n  lambda.1se <- 0.05011872\n  \n  lambda.select <- lambda.1se\n  #fit <- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\n  fit <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=TRUE, alpha = 1, lambda = lambda.select)\n  learn$pred <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*learn$NB_Claim\n  \n  \n  matrix.x <- model.matrix(glm.score, data=valid)[,-1]\n  y <- valid$M_Claim\n\n  \n  valid$pred <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*valid$NB_Claim\n  variance <- (sum((learn$AMT_Claim - learn$pred)^2)/(nrow(learn) - length(fit$beta)))\n  phi <-  variance/mean(learn$AMT_Claim)^2\n\n  Result_ <- rbind(Result_, c(i, Score.pred.sev(valid$pred, valid$AMT_Claim, phi)/nrow(valid)))\n  Result2_ <- rbind(Result2_, c(i, Score.pred.sev(valid$pred, valid$AMT_Claim, phi)))\n}\n\n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ncolnames(Result2_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ntot <- colSums(Result2_)/nrow(train2)\ntot$Fold <- 'Total'\nResult_ <- rbind(Result_ , tot)\nResult_ <- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] <- 'Improvement'\n\nfor(i in 2:3){\n  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) <- NULL\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> Fold </th>\n   <th style=\"text-align:center;\"> Sc.log </th>\n   <th style=\"text-align:center;\"> Sc.MSE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> 1 </td>\n   <td style=\"text-align:center;\"> 9.24111 </td>\n   <td style=\"text-align:center;\"> 47.51428 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 2 </td>\n   <td style=\"text-align:center;\"> 9.25252 </td>\n   <td style=\"text-align:center;\"> 21.11620 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 3 </td>\n   <td style=\"text-align:center;\"> 9.27098 </td>\n   <td style=\"text-align:center;\"> 29.80730 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 4 </td>\n   <td style=\"text-align:center;\"> 9.30253 </td>\n   <td style=\"text-align:center;\"> 29.03769 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 5 </td>\n   <td style=\"text-align:center;\"> 9.22546 </td>\n   <td style=\"text-align:center;\"> 17.06356 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> Total </td>\n   <td style=\"text-align:center;\"> 9.25871 </td>\n   <td style=\"text-align:center;\"> 29.09671 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> Improvement </td>\n   <td style=\"text-align:center;\"> -0.03642 </td>\n   <td style=\"text-align:center;\"> -1.34022 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe value of $\\lambda_{1se}$ is also used to compute the scores on the *test* set.\n\n\n::: {#tbl-Pscore_basetest4_sev_tel .cell tbl-cap='Prediction scores for the GLM-net model  (testing set)'}\n\n```{.r .cell-code  code-fold=\"true\"}\nglm.score <- as.formula(M_Claim ~ \n                          Car.use + Region + Car.age + I(Car.age^2) + I(Car.age^3) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2) + I(Avgdays.week^3)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + I(max.day^2) + I(max.day^3) \n                        + min.day + I(min.day^2) + I(min.day^3)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$M_Claim\n\nlambda.min <- 0.003162278\nlambda.1se <- 0.05011872\n\nlambda.select <- lambda.1se\n#fit <- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\nfit <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=TRUE, alpha = 1, lambda = lambda.select)\n\ntrain2$pred <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*train2$NB_Claim\n\nmatrix.x <- model.matrix(glm.score, data=test2)[,-1]\ny <- test2$M_Claim\n\n\ntest2$pred.base <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*test2$NB_Claim\nvariance <- (sum((train2$AMT_Claim - train2$pred)^2)/(nrow(train2) - length(fit$beta)))\nphi <- variance/mean(train2$AMT_Claim)^2\n\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('LASSO (parsimonious)', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> Model </th>\n   <th style=\"text-align:center;\"> Sc.log </th>\n   <th style=\"text-align:center;\"> Sc.MSE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> Base </td>\n   <td style=\"text-align:center;\"> 9.29504 </td>\n   <td style=\"text-align:center;\"> 21.82679 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> GLM (trad.) </td>\n   <td style=\"text-align:center;\"> 9.23655 </td>\n   <td style=\"text-align:center;\"> 21.16556 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> LASSO (optimal) </td>\n   <td style=\"text-align:center;\"> 9.22435 </td>\n   <td style=\"text-align:center;\"> 19.83764 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> LASSO (parsimonious) </td>\n   <td style=\"text-align:center;\"> 9.24845 </td>\n   <td style=\"text-align:center;\"> 20.32402 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n### Residuals and protected variables\n\nBy comparing the results obtained with pricing models based solely on traditional variables, we observe an enhancement in prediction quality upon incorporating telematics variables. Nonetheless, we aim to assess whether there is an added advantage in retaining protected segmentation variables even with the availability of telematics data. To gauge this benefit, we compute the residuals of the GLM-Net approach. Specifically, we proceed as follows:  \n\n1) We fit a GLM-Net model using the available covariates, as previously outlined.    \n2) We predict the expected severity of the model based on the *train* data.    \n3) We utilize these predictions as an *offset* variable.   \n\nWith this modeling approach, we can now assess whether telematics data effectively eliminates the predictive power of protected variables.\n\nThe graphs below depict the extent to which protected variables contribute to the residuals from a model utilizing telematics covariates:  \n\n- Credit score and territory still exhibit a (slight) impact on severity.  \n- The impact of insured age appears to have been absorbed by telematic covariates.  \n- Insured age and marital status still appear to explain severity.  \n\n\n::: {#fig-CreditScore_sev .cell layout-ncol=\"2\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n## Credit Score\n\nmeansev.inv <- sum(train2$NB_Claim)/sum(train2$M_Claim)\nmeanpred.inv <- sum(train2$pred.tele)/sum(train2$M_Claim)\n\ntemp2 <- train2 %>%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(Credit.score/25) * 25) %>%\n  group_by(Group) %>% \n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.tele),\n            nbclaim = n()) %>% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*M_Claim/pred)\n\n\nggplot() + \n  geom_smooth(aes(x=Group, y=sev, weight = nbclaim, color='Observed'),se=F, size=1, data=temp2) + \n  geom_smooth(aes(x=Group, y=sev2, weight = nbclaim, color='Residuals'),se=F, size=1, data=temp2) + \n  labs(x = 'Credit Score',\n       y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  guides(color = guide_legend(title = \"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n## Insured Age\n\ntemp2 <- train2 %>%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Insured.age = pmin(Insured.age, 80),\n                Group = ceiling(Insured.age/5) * 5) %>%\n  group_by(Group) %>% \n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.tele),\n            nbclaim = n()) %>% \n   mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*M_Claim/pred)\n\nggplot() + \n  geom_smooth(aes(x=Group, y=sev, weight = nbclaim, color='Observed'),se=F, size=1, data=temp2) + \n  geom_smooth(aes(x=Group, y=sev2, weight = nbclaim, color='Residuals'),se=F, size=1, data=temp2) + \n  labs(x = 'Age of the insured',\n       y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  guides(color = guide_legend(title = \"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n## Sex of the insured\n\ntemp <- train2 %>%\n  mutate(Var_ = Insured.sex) %>%\n  group_by(Var_) %>%\n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.tele),\n            nbclaim = n()) %>% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n          sev2 = meanpred.inv*M_Claim/pred)\n\ntemp$sev <- temp$sev/temp$sev[1]\ntemp$sev2 <- temp$sev2/temp$sev2[1]\n\nggplot() + #start plot by by plotting bars\n  geom_point(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=0.7) +\n  geom_point(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=0.7) +\n  labs(x = 'Sex of the insured', y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  #ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+\n  guides(color=guide_legend(title=\"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n## Marital\n\ntemp <- train2 %>%\n  mutate(Var_ = Marital) %>%\n  group_by(Var_) %>%\n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.tele),\n            nbclaim = n()) %>% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*pred/nbclaim)\n\ntemp$sev <- temp$sev/temp$sev[1]\ntemp$sev2 <- temp$sev2/temp$sev2[1]\n\nggplot() + #start plot by by plotting bars\n  geom_point(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=0.7) +\n  geom_point(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=0.7) +\n  labs(x = 'Marital status of the insured', y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  #ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+\n  guides(color=guide_legend(title=\"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n## Territory\n\ntemp <- train2 %>%\n  mutate(Var_ = Territory) %>%\n  group_by(Var_) %>%\n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.tele),\n            nbclaim = n()) %>% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*M_Claim/pred)\n\n#temp$sev <- temp$sev/temp$sev[1]\n#temp$sev2 <- temp$sev2/temp$sev2[1]\n\nggplot() + #start plot by by plotting bars\n  #geom_point(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=0.7) +\n  #geom_point(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=0.7) +\n  labs(x = 'Territory', y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  scale_x_discrete(labels = NULL, breaks = NULL)+\n  #ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+\n  guides(color=guide_legend(title=\"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n```\n\n::: {.cell-output-display}\n![Credit Score](severityVarTelematique_files/figure-html/fig-CreditScore_sev-1.png){#fig-CreditScore_sev-1 width=864}\n:::\n\n::: {.cell-output-display}\n![Age of the Insured](severityVarTelematique_files/figure-html/fig-CreditScore_sev-2.png){#fig-CreditScore_sev-2 width=864}\n:::\n\n::: {.cell-output-display}\n![Sex of the Insured](severityVarTelematique_files/figure-html/fig-CreditScore_sev-3.png){#fig-CreditScore_sev-3 width=864}\n:::\n\n::: {.cell-output-display}\n![Marital Status of the Insured](severityVarTelematique_files/figure-html/fig-CreditScore_sev-4.png){#fig-CreditScore_sev-4 width=864}\n:::\n\n::: {.cell-output-display}\n![Territory](severityVarTelematique_files/figure-html/fig-CreditScore_sev-5.png){#fig-CreditScore_sev-5 width=864}\n:::\n\nObserved Relativity vs. Residuals Relativity\n:::\n\n\n### GLM-net on residuals\n\nBy utilizing the GLM-Net prediction as an offset variable, we can fit another GLM-Net model, this time exclusively employing the protected variables. The table below presents the predicted scores of the two new models: the LASSO* with optimal $\\lambda$, and the LASSO* with a penalty value situated at one standard error. Overall, we observe a slight improvement in the various scores with the addition of protected variables.\n\n\n::: {.cell}\n\n:::\n\n\n\n\nWe thus fit a GLM-net model using telematics and traditional covariates, and we predict the expected severity of the model on the *train* database. Using the prediction as an *offset* variable, we analyze the impact of each variable.\n\n\n\n\n::: {#tbl-Pscore_basetest4b_sev_tel .cell tbl-cap='Prediction scores for the GLM-net model  (testing set)'}\n\n```{.r .cell-code  code-fold=\"true\"}\nglm.score <- as.formula(M_Claim ~ \n                          Car.use + Region + Car.age + I(Car.age^2) + I(Car.age^3) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2) + I(Avgdays.week^3)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + I(max.day^2) + I(max.day^3) \n                        + min.day + I(min.day^2) + I(min.day^3)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$M_Claim\n\n\nlambda.min <- 0.003162278\nlasso.min <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, alpha = 1, lambda = lambda.min)\ntrain2$pred.tele <- predict(lasso.min, newx = matrix.x, type='response', lambda = lambda.min)\n\n\nmatrix.x <- model.matrix(glm.score, data=test2)[,-1]\ny <- test2$M_Claim\n\ntest2$pred.tele <- predict(lasso.min, newx = matrix.x, type='response', lambda = lambda.min)\n\nglm.score <- as.formula(M_Claim ~ Insured.sex + Marital \n                        + Credit.score +  I(Credit.score^2) \n                        + Insured.age +  I(Insured.age^2) \n                        + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$M_Claim\noffset <- log(train2$pred.tele)\n\nlambda.min <- 0.003162278\nlambda.1se <- 0.1258925\n\nlambda.select <- lambda.min\n#fit <- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\nfit <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n\n\ntrain2$pred <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)*train2$NB_Claim\n\nmatrix.x <- model.matrix(glm.score, data=test2)[,-1]\ny <- test2$M_Claim\noffset <- log(test2$pred.tele)\n\ntest2$pred.base <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)*test2$NB_Claim\nvariance <- (sum((train2$AMT_Claim - train2$pred)^2)/(nrow(train2) - length(fit$beta)))\nphi <- variance/mean(train2$AMT_Claim)^2\n\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('LASSO* (optimal)', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- rbind(Result_all, Result_)\n\n###\n\nglm.score <- as.formula(M_Claim ~ Insured.sex + Marital \n                        + Credit.score +  I(Credit.score^2) \n                        + Insured.age + I(Insured.age^2) \n                        + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$M_Claim\noffset <- log(train2$pred.tele)\n\nlambda.min <- 0.003162278\nlambda.1se <- 0.1258925\n\nlambda.select <- lambda.1se\nfit <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\n#fit <- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n\ntrain2$pred <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)*train2$NB_Claim\n\nmatrix.x <- model.matrix(glm.score, data=test2)[,-1]\ny <- test2$M_Claim\noffset <- log(test2$pred.tele)\n\ntest2$pred.base <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)*test2$NB_Claim\nvariance <- (sum((train2$AMT_Claim - train2$pred)^2)/(nrow(train2) - length(fit$beta)))\nphi <- variance/mean(train2$AMT_Claim)^2\n\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('LASSO* (parsimonious)', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> Model </th>\n   <th style=\"text-align:center;\"> Sc.log </th>\n   <th style=\"text-align:center;\"> Sc.MSE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> Base </td>\n   <td style=\"text-align:center;\"> 9.29504 </td>\n   <td style=\"text-align:center;\"> 21.82679 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> GLM (trad.) </td>\n   <td style=\"text-align:center;\"> 9.23655 </td>\n   <td style=\"text-align:center;\"> 21.16556 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> LASSO (optimal) </td>\n   <td style=\"text-align:center;\"> 9.22435 </td>\n   <td style=\"text-align:center;\"> 19.83764 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> LASSO (parsimonious) </td>\n   <td style=\"text-align:center;\"> 9.24845 </td>\n   <td style=\"text-align:center;\"> 20.32402 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> LASSO* (optimal) </td>\n   <td style=\"text-align:center;\"> 9.20054 </td>\n   <td style=\"text-align:center;\"> 19.31021 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> LASSO* (parsimonious) </td>\n   <td style=\"text-align:center;\"> 9.20843 </td>\n   <td style=\"text-align:center;\"> 19.59126 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n::: {#tbl-Pscore_basetest4b_sev_tel_2 .cell tbl-cap='Prediction scores for the GLM-net model without Credit Score  (testing set)'}\n\n:::\n\n\n:::\n  \n\n## XGBoost\n\nWe now consider an XGBoost model. As with the frequency model, hyperparameter values are obtained by performing a Bayesian search on a grid of possible values.\n\n\n::: {.cell}\n\n:::\n\n\n\n\n::: {.panel-tabset}\n\n### Prediction Scores\n\nUsing these values, we can calculate the models prediction scores based on all classical and telematics covariates.  One can see that the XGBoost approach is particularly effective in capturing the effect of all available telematic covariates. Indeed, the scores obtained are significantly improved compared to other tested approaches.\n\n\n::: {.cell}\n\n:::\n\n::: {#tbl-Pscore_XGBoosttele_sev_tel .cell tbl-cap='Prediction scores for the XGBoost model with telematics (severity)'}\n\n```{.r .cell-code  code-fold=\"true\"}\nparam <- list(\n  eta = 0.1914908,\n  max_depth = 5,\n  subsample = 0.8674295,\n  min_child_weight = 100.0327,\n  booster = \"gbtree\",\n  objective = \"reg:gamma\",\n  eval_metric = \"gamma-nloglik\")\n\nset.seed(133)\nxgbcv <- xgb.cv(params = param,\n                nrounds = 145,\n                data = dtrain,\n                folds = folds,\n                prediction = TRUE,\n                early_stopping_rounds = 10,\n                verbose = 0,\n                maximize = F)\n  \nvariance <- sapply(xgbcv$folds, function(x){sum((train2$AMT_Claim[x]-unlist(xgbcv$pred[x])*train2$NB_Claim[x])^2)/length(train2$AMT_Claim[x])})  \nmean <- sapply(xgbcv$folds, function(x){mean(train2$AMT_Claim[x])})\nphi <- unlist(variance)/mean^2\n\nSc.log1 <- -dgamma(train2$AMT_Claim[xgbcv$folds$fold1], shape = 1/phi[1], scale = unlist(xgbcv$pred[xgbcv$folds$fold1])*train2$NB_Claim[xgbcv$folds$fold1]*phi[1], log=TRUE)\nSc.log2 <- -dgamma(train2$AMT_Claim[xgbcv$folds$fold2], shape = 1/phi[2], scale = unlist(xgbcv$pred[xgbcv$folds$fold2])*train2$NB_Claim[xgbcv$folds$fold2]*phi[2], log=TRUE)\nSc.log3 <- -dgamma(train2$AMT_Claim[xgbcv$folds$fold3], shape = 1/phi[3], scale = unlist(xgbcv$pred[xgbcv$folds$fold3])*train2$NB_Claim[xgbcv$folds$fold3]*phi[3], log=TRUE)\nSc.log4 <- -dgamma(train2$AMT_Claim[xgbcv$folds$fold4], shape = 1/phi[4], scale = unlist(xgbcv$pred[xgbcv$folds$fold4])*train2$NB_Claim[xgbcv$folds$fold4]*phi[4], log=TRUE)\nSc.log5 <- -dgamma(train2$AMT_Claim[xgbcv$folds$fold5], shape = 1/phi[5], scale = unlist(xgbcv$pred[xgbcv$folds$fold5])*train2$NB_Claim[xgbcv$folds$fold5]*phi[5], log=TRUE)\n\nSc.MSE <- sapply(xgbcv$folds, function(x){(train2$AMT_Claim[x]-unlist(xgbcv$pred[x])*train2$NB_Claim[x])^2/1000000})\n\n \nResult_  <- rbind(\nc(1,mean(Sc.log1), mean(Sc.MSE[1]$fold1)),\nc(2,mean(Sc.log2), mean(Sc.MSE[2]$fold2)),\nc(3,mean(Sc.log3), mean(Sc.MSE[3]$fold3)),\nc(4,mean(Sc.log4), mean(Sc.MSE[4]$fold4)),\nc(5,mean(Sc.log5), mean(Sc.MSE[5]$fold5))\n)\n\nRes.sum  <- rbind(\nc(sum(Sc.log1), sum(Sc.MSE[1]$fold1)),\nc(sum(Sc.log2), sum(Sc.MSE[2]$fold2)),\nc(sum(Sc.log3), sum(Sc.MSE[3]$fold3)),\nc(sum(Sc.log4), sum(Sc.MSE[4]$fold4)),\nc(sum(Sc.log5), sum(Sc.MSE[5]$fold5))\n)\nsum <- c('Total', colSums(Res.sum)/nrow(train2))\n\nResult_  <- data.frame(rbind(Result_, sum)) \n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\nResult_ <- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] <- 'Improvement'\n\nfor(i in 2:3){\n  Result_[,i] <- as.numeric(Result_[,i])  \n  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\nrownames(Result_) <- NULL\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> Fold </th>\n   <th style=\"text-align:center;\"> Sc.log </th>\n   <th style=\"text-align:center;\"> Sc.MSE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> 1 </td>\n   <td style=\"text-align:center;\"> 9.32070 </td>\n   <td style=\"text-align:center;\"> 41.62366 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 2 </td>\n   <td style=\"text-align:center;\"> 9.03860 </td>\n   <td style=\"text-align:center;\"> 16.25485 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 3 </td>\n   <td style=\"text-align:center;\"> 9.09227 </td>\n   <td style=\"text-align:center;\"> 22.19734 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 4 </td>\n   <td style=\"text-align:center;\"> 9.06896 </td>\n   <td style=\"text-align:center;\"> 20.72986 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 5 </td>\n   <td style=\"text-align:center;\"> 8.98434 </td>\n   <td style=\"text-align:center;\"> 14.54670 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> Total </td>\n   <td style=\"text-align:center;\"> 9.10319 </td>\n   <td style=\"text-align:center;\"> 23.23889 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> Improvement </td>\n   <td style=\"text-align:center;\"> -0.19194 </td>\n   <td style=\"text-align:center;\"> -7.19805 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe same model can be used to compute the scores on the *test* set.  We also observe that the XGBoost approach is the most effective.\n\n\n::: {#tbl-Pscore_XGBoost_correction333_sev_tel .cell tbl-cap='Prediction scores for the XGBoost model with telematics (severity)'}\n\n```{.r .cell-code  code-fold=\"true\"}\ndtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(all.vars2)]), label = train2$M_Claim)\n#setinfo(dtrain,\"base_margin\",log(train2$expo))\nfolds <-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n\nparam <- list(\n  eta = 0.1914908,\n  max_depth = 5,\n  subsample = 0.8674295,\n  min_child_weight = 100.0327,\n  booster = \"gbtree\",\n  objective = \"reg:gamma\",\n  eval_metric = \"gamma-nloglik\")\n\nset.seed(133)\nfit.xgb <- xgb.train(params = param,\n                     nrounds = 145,\n                     data = dtrain)\n\ntrain2$pred.xgb <- predict(fit.xgb, dtrain, type='response')*train2$NB_Claim\ntrain2$pred.xgb.off <- predict(fit.xgb, dtrain, type='response')\n\ndtest <- xgb.DMatrix(data = data.matrix(test2[, paste(all.vars2)]), label = test2$M_Claim)\n#setinfo(dtest,\"base_margin\",log(test2$expo))\ntest2$pred.xgb <- predict(fit.xgb, dtest, type='response')*test2$NB_Claim\ntest2$pred.xgb.off <- predict(fit.xgb, dtest, type='response')\n\ntest2$pred.base <- test2$pred.xgb\n\nvariance <- (sum((train2$pred.xgb - (train2$AMT_Claim))^2)/(length(train2$AMT_Claim)))\nphi <- variance/mean(train2$AMT_Claim)^2\n\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('XGBoost', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> Model </th>\n   <th style=\"text-align:center;\"> Sc.log </th>\n   <th style=\"text-align:center;\"> Sc.MSE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> Base </td>\n   <td style=\"text-align:center;\"> 9.29504 </td>\n   <td style=\"text-align:center;\"> 21.82679 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> GLM (trad.) </td>\n   <td style=\"text-align:center;\"> 9.23655 </td>\n   <td style=\"text-align:center;\"> 21.16556 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> LASSO (optimal) </td>\n   <td style=\"text-align:center;\"> 9.22435 </td>\n   <td style=\"text-align:center;\"> 19.83764 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> LASSO (parsimonious) </td>\n   <td style=\"text-align:center;\"> 9.24845 </td>\n   <td style=\"text-align:center;\"> 20.32402 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> LASSO* (optimal) </td>\n   <td style=\"text-align:center;\"> 9.20054 </td>\n   <td style=\"text-align:center;\"> 19.31021 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> LASSO* (parsimonious) </td>\n   <td style=\"text-align:center;\"> 9.20843 </td>\n   <td style=\"text-align:center;\"> 19.59126 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> XGBoost </td>\n   <td style=\"text-align:center;\"> 9.03389 </td>\n   <td style=\"text-align:center;\"> 15.72516 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n### Variables Importance\n\nThe graph below illustrates the most important variables in the XGBoost model. We see that the most important telematic variable is related to the vehicle usage intensity, even for severity. Nevertheless, the XGBoost model is capable of identifying a few other important covariates.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nimportance_matrix <- xgb.importance(dimnames(dtrain)[[2]], model = fit.xgb)\nxgb.ggplot.importance(importance_matrix,top_n=15) + theme(text = element_text(size=15))\n```\n\n::: {.cell-output-display}\n![](severityVarTelematique_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n\n### RESIDUALS AND PROTECTED VARIABLES\n\nAs we did for the GLM-net model, we can check whether the protected variables we excluded from the analysis retain predictive capacity.\n\nThe graphs below depict the extent to which protected variables contribute to the residuals from a model utilizing telematics covariates.  The conclusions are consistent with those obtained with the GLM-net model: most of the effect seems to be captured by the traditional and telematic variables. However, we see that for the *Credit Score* variable, there is a loss of information if the variable is not used in the model and that this loss is not completely compensated by the telematics covariates.\n\n\n::: {#fig-CreditScore_sev2 .cell layout-ncol=\"2\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nmoy.xgb <- sum(train2$pred.xgb.off)/sum(train2$M_Claim)\n\nmeansev.inv <- sum(train2$NB_Claim)/sum(train2$M_Claim)\nmeanpred.inv <- sum(train2$pred.xgb.off)/sum(train2$M_Claim)\n\n\ntemp2 <- train2 %>%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Insured.age = pmin(Insured.age, 80),\n                Group = ceiling(Credit.score/25) * 25) %>%\n  group_by(Group) %>% \n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.xgb.off),\n            nbclaim = n()) %>% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*M_Claim/pred)\n\nGraph_resCS <- ggplot() + \n  geom_smooth(aes(x=Group, y=sev, weight = nbclaim, color='Observed'),se=F, size=1, data=temp2) + \n  geom_smooth(aes(x=Group, y=sev2, weight = nbclaim, color='Residuals'),se=F, size=1, data=temp2) + \n  labs(x = 'Credit Score',\n       y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  guides(color = guide_legend(title = \"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\nprint(Graph_resCS)\nsave(Graph_resCS, file = \"Data/Graph_resCS_sev.rdata\")\n\n\n### Age of the insured\n\n\nmeansev.inv <- sum(train2$NB_Claim)/sum(train2$M_Claim)\nmeanpred.inv <- sum(train2$pred.xgb.off)/sum(train2$M_Claim)\n\n\ntemp2 <- train2 %>%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Insured.age = pmin(Insured.age, 80),\n                Group = ceiling(Insured.age/5) * 5) %>%\n  group_by(Group) %>% \n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.xgb.off),\n            nbclaim = n()) %>% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*M_Claim/pred)\n\nGraph_resAge <- ggplot() + \n  geom_smooth(aes(x=Group, y=sev, weight = nbclaim, color='Observed'),se=F, size=1, data=temp2) + \n  geom_smooth(aes(x=Group, y=sev2, weight = nbclaim, color='Residuals'),se=F, size=1, data=temp2) + \n  labs(x = 'Insured.age',\n       y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  guides(color = guide_legend(title = \"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\nprint(Graph_resAge)\nsave(Graph_resAge, file = \"Data/Graph_resAge_sev.rdata\")\n\n###\n\ntemp <- train2 %>%\n  mutate(Var_ = Insured.sex) %>%\n  group_by(Var_) %>%\n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.xgb),\n            nbclaim = n()) %>% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*M_Claim/pred)\n\ntemp$sev <- temp$sev/temp$sev[1]\ntemp$sev2 <- temp$sev2/temp$sev2[1]\n\nggplot() + #start plot by by plotting bars\n  geom_point(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=0.7) +\n  geom_point(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=0.7) +\n  labs(x = 'Sex of the insured', y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  #ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+\n  guides(color=guide_legend(title=\"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n###\n\n temp <- train2 %>%\n   mutate(Var_ = Marital) %>%\n   group_by(Var_) %>%\n   summarize(M_Claim=sum(M_Claim),\n             pred=sum(pred.xgb),\n             nbclaim = n()) %>% \n   mutate(sev = meansev.inv*M_Claim/nbclaim,\n          sev2 = meanpred.inv*M_Claim/pred)\n \n temp$sev <- temp$sev/temp$sev[1]\n temp$sev2 <- temp$sev2/temp$sev2[1]\n \n ggplot() + #start plot by by plotting bars\n   geom_point(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=3) +\n   geom_line(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=0.7) +\n   geom_point(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=3) +\n   geom_line(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=0.7) +\n   labs(x = 'Marital status of the insured', y = 'Relativity') +\n   geom_hline(yintercept = 1, linetype='dashed')+\n   #ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+\n   guides(color=guide_legend(title=\"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n \n###\n\ntemp <- train2 %>%\n  mutate(Var_ = Territory) %>%\n  group_by(Var_) %>%\n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.xgb.off),\n            nbclaim = n()) %>% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*M_Claim/pred)\n\n#temp$sev <- temp$sev/temp$sev[1]\n#temp$sev2 <- temp$sev2/temp$sev2[1]\n\nGraph_resTerr <- ggplot() + #start plot by by plotting bars\n  #geom_point(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=0.7) +\n  #geom_point(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=0.7) +\n  labs(x = 'Territory', y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  scale_x_discrete(labels = NULL, breaks = NULL)+\n  #ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+\n  guides(color=guide_legend(title=\"\")) +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\nprint(Graph_resTerr)\nsave(Graph_resTerr, file = \"Data/Graph_resTerr_sev.rdata\")\n```\n\n::: {.cell-output-display}\n![Credit Score](severityVarTelematique_files/figure-html/fig-CreditScore_sev2-1.png){#fig-CreditScore_sev2-1 width=864}\n:::\n\n::: {.cell-output-display}\n![Age of the Insured](severityVarTelematique_files/figure-html/fig-CreditScore_sev2-2.png){#fig-CreditScore_sev2-2 width=864}\n:::\n\n::: {.cell-output-display}\n![Sex of the Insured](severityVarTelematique_files/figure-html/fig-CreditScore_sev2-3.png){#fig-CreditScore_sev2-3 width=864}\n:::\n\n::: {.cell-output-display}\n![Marital Status of the Insured](severityVarTelematique_files/figure-html/fig-CreditScore_sev2-4.png){#fig-CreditScore_sev2-4 width=864}\n:::\n\n::: {.cell-output-display}\n![Territory](severityVarTelematique_files/figure-html/fig-CreditScore_sev2-5.png){#fig-CreditScore_sev2-5 width=864}\n:::\n\nObserved Relativity vs. Residuals Relativity\n:::\n\n\n\n### XGBOOST ON RESIDUALS\n\nAs we did with the GLM-Net approach, we use the predictions of the XGBoost model as an offset variable and fit another XGBoost model on the data, using only the protected covariates.  The prediction scores are show in the table below.  We still observe a slight improvement in prediction scores, indicating that protected variables still retain some predictive power despite the use of telematic information.\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nvar.sens <- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\")    \n\ndtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(var.sens)]), label = train2$M_Claim)\nsetinfo(dtrain,\"base_margin\",log(train2$pred.xgb.off))\nfolds <-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n\nparam <- list(\n  eta = 0.02337437,\n  max_depth = 26,\n  subsample = 0.8097923,\n  min_child_weight = 123.6033,\n  booster = \"gbtree\",\n  objective = \"reg:gamma\",\n  eval_metric = \"gamma-nloglik\")\n\nset.seed(533)\nfit.xgb2 <- xgb.train(params = param,\n                      nrounds = 103,\n                      data = dtrain)\n\nvar.sens <- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\")    \n\ntrain2$pred.xgb <- predict(fit.xgb2, dtrain, type='response')*train2$NB_Claim\n\ndtest <- xgb.DMatrix(data = data.matrix(test2[, paste(var.sens)]), label = test2$M_Claim)\nsetinfo(dtest,\"base_margin\",log(test2$pred.xgb.off))\n\ntest2$pred.base <- predict(fit.xgb2, dtest, type='response')*test2$NB_Claim\n\nvariance <- (sum((train2$pred.xgb - (train2$AMT_Claim))^2)/(length(train2$AMT_Claim)))\nphi <- variance/mean(train2$AMT_Claim)^2\n```\n:::\n\n::: {#tbl-Pscore_XGBoost_correction33_sev_tel .cell tbl-cap='Prediction scores for the XGBoost model with telematics. (severity)'}\n\n```{.r .cell-code  code-fold=\"true\"}\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('XGBoost*', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- rbind(Result_all, Result_)\n\nsave(Result_all, file='Data/ResultsSynth_sev.Rda')\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> Model </th>\n   <th style=\"text-align:center;\"> Sc.log </th>\n   <th style=\"text-align:center;\"> Sc.MSE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> Base </td>\n   <td style=\"text-align:center;\"> 9.29504 </td>\n   <td style=\"text-align:center;\"> 21.82679 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> GLM (trad.) </td>\n   <td style=\"text-align:center;\"> 9.23655 </td>\n   <td style=\"text-align:center;\"> 21.16556 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> LASSO (optimal) </td>\n   <td style=\"text-align:center;\"> 9.22435 </td>\n   <td style=\"text-align:center;\"> 19.83764 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> LASSO (parsimonious) </td>\n   <td style=\"text-align:center;\"> 9.24845 </td>\n   <td style=\"text-align:center;\"> 20.32402 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> LASSO* (optimal) </td>\n   <td style=\"text-align:center;\"> 9.20054 </td>\n   <td style=\"text-align:center;\"> 19.31021 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> LASSO* (parsimonious) </td>\n   <td style=\"text-align:center;\"> 9.20843 </td>\n   <td style=\"text-align:center;\"> 19.59126 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> XGBoost </td>\n   <td style=\"text-align:center;\"> 9.03389 </td>\n   <td style=\"text-align:center;\"> 15.72516 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> XGBoost* </td>\n   <td style=\"text-align:center;\"> 9.01906 </td>\n   <td style=\"text-align:center;\"> 15.40489 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n::: {.cell}\n\n:::\n\n::: {#tbl-Pscore_XGBoost_correction33_sev_tel_WO .cell tbl-cap='Prediction scores for the XGBoost model with telematics. (severity)'}\n\n:::\n\n\n\nThe graph below illustrates the most important protected variables in the XGBoost model. Unsurprisingly, the insured's sex and marital status come at the very bottom. Additionally, the most important protected variable is credit score. These results are consistent with the conclusions of the GLM-net model.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nimportance_matrix <- xgb.importance(dimnames(dtrain)[[2]], model = fit.xgb2)\nxgb.ggplot.importance(importance_matrix,top_n=15) + theme(text = element_text(size=15))\n```\n\n::: {.cell-output-display}\n![](severityVarTelematique_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\n\n\n\n",
    "supporting": [
      "severityVarTelematique_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}