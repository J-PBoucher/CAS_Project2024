[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Telematics and Algorithmic Bias",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "index.html#data-used-for-the-project",
    "href": "index.html#data-used-for-the-project",
    "title": "Telematics and Algorithmic Bias",
    "section": "Data used for the project",
    "text": "Data used for the project\n\nSynthetic datasetNon-representative sample\n\n\nThe data that could have been used in this project comes from a major Canadian insurer and is highly confidential. Instead, we use the synthetic database generated with the original data. Details can be found here:\n\nB. So, J.-P. Boucher & E. Valdez (2021). Synthetic Dataset Generation of Driver Telematics. Risks, 9(4), 58.\n\nThe database can be accessed here. The synthetic dataset generated has 100,000 policies, including observations about driver’s claims experience and associated classical risk variables and telematics-related variables. The paper shows that the following variables are available in the synthetic database.\n\n\n\n\n\nAn analysis of the results obtained with synthetic data is compared to those obtained with the original data to ensure the accuracy of the conclusions. In this regard, it is noteworthy that, in order to achieve the closest match possible with the results obtained from the original data (used to generate the synthetic database), we determined that two covariates should not be utilized in the study: Annual.miles.driven and Annual.pct.driven. Finally, as the synthetic database does not allow the different coverages to be distinguished, it will not be possible to analyze them separately. However, the analysis and the code used could easily be reused to include this information if available.\n\n\nIn analyzing telematics data, we must be careful before jumping to general conclusions about the driving behavior of the whole portfolio. Indeed, policyholders who decided to place a telematics device on their car or to download an application on their phone that tracks all their car trips do not correspond to the general driver population. In our case, approximately \\(10\\%\\) to \\(15\\%\\) of the insurance company’s portfolio chose to use the telematics option for their car insurance. Typically, these insureds correspond to one of the two following profiles:\n\nTechnophile policyholders. They love new telematics technology and want detailed information about their driving habits. Summary driving data is indeed continuously available to policyholders via a website.\nYoung and/or bad drivers. To motivate policyholders to buy the telematics option, insurance companies often offer an initial discount, and the renewal discounts range from \\(0\\%\\) to \\(25\\%\\) depending on the driving experience. Because auto insurance in Ontario is expensive and often unaffordable for some drivers, all discounts are welcome for policyholders with high insurance premiums. As a result, an unusually high proportion of risky insureds use telematics devices or telematics apps."
  },
  {
    "objectID": "index.html#other",
    "href": "index.html#other",
    "title": "Telematics and Algorithmic Bias",
    "section": "Other",
    "text": "Other\n\nThe content in this project was generated with the assistance of artificial intelligence.\nComputer applications do not claim to be the most efficient or elegant. The goal is mainly to demonstrate how theory can be applied to actual data. If it is possible to optimize the code, one should not hesitate.\n\nFinally, it is worth noting that the report was written in Quarto (for details, see: https://quarto.org/docs/books), which means that all the codes used to create this report are available and directly replicable.\n\n\n\n\n\nAyuso, Mercedes, Montserrat Guillen, and Ana Maria Perez Marin. 2016. “Using GPS Data to Analyse the Distance Travelled to the First Accident at Fault in Pay-as-You-Drive Insurance.” Transportation Research Part C: Emerging Technologies 68: 160–67.\n\n\nAyuso, Mercedes, Montserrat Guillen, and Jens Perch Nielsen. 2019. “Improving Automobile Insurance Ratemaking Using Telematics: Incorporating Mileage and Driver Behaviour Data.” Transportation 46 (3): 735–52.\n\n\nAyuso, Mercedes, Montserrat Guillen, and Ana Maria Perez-Marin. 2014. “Time and Distance to First Accident and Driving Patterns of Young Drivers with Pay-as-You-Drive Insurance.” Accident Analysis & Prevention 73: 125–31.\n\n\n———. 2016. “Telematics and Gender Discrimination: Some Usage-Based Evidence on Whether Men’s Risk of Accidents Differs from Women’s.” Risks 4 (2).\n\n\nBoucher, Jean-Philippe, Steven Côte, and Montserrat Guillen. 2017. “Exposure as Duration and Distance in Telematics Motor Insurance Using Generalized Additive Models.” Risks 5 (4).\n\n\nBoucher, Jean-Philippe, Ana Maria Perez-Marin, and Miguel Santolino. 2013. “Pay-as-You-Drive Insurance: The Effect of the Kilometers on the Risk of Accident.” In Anales Del Instituto de Actuarios Espanoles, 19:135–54. 3. Instituto de Actuarios Espanoles Madrid.\n\n\nBoucher, Jean-Philippe, and Roxane Turcotte. 2020. “A Longitudinal Analysis of the Impact of Distance Driven on the Probability of Car Accidents.” Risks 8 (3): 91.\n\n\nChibanda, Kudakwashe F. 2022. “Defining Discrimination in Insurance.” Cas Research Paper: A Special Series On Race And Insurance Pricing.\n\n\nDuval, Francis, Jean-Philippe Boucher, and Mathieu Pigeon. 2022. “How Much Telematics Information Do Insurers Need for Claim Classification?” North American Actuarial Journal 26 (4): 570–90.\n\n\n———. 2023. “Enhancing Claim Classification with Feature Extraction from Anomaly-Detection-Derived Routine and Peculiarity Profiles.” Journal of Risk and Insurance 90 (2): 421–58.\n\n\nEmbrechts, Paul, and Mario V Wuthrich. 2022. “Recent Challenges in Actuarial Science.” Annual Review of Statistics and Its Application 9: 119–40.\n\n\nGao, Guangyuan, Shengwang Meng, and Mario V Wuthrich. 2019. “Claims Frequency Modeling Using Telematics Car Driving Data.” Scandinavian Actuarial Journal 2019 (2): 143–62.\n\n\nGao, Guangyuan, He Wang, and Mario V Wuthrich. 2022. “Boosting Poisson Regression Models with Telematics Car Driving Data.” Machine Learning 111 (2): 243–72.\n\n\nGao, Guangyuan, and Mario V Wuthrich. 2019. “Convolutional Neural Network Classification of Telematics Car Driving Data.” Risks 7 (1).\n\n\nGuillen, Montserrat, Jens Perch Nielsen, Mercedes Ayuso, and Ana M Perez-Marin. 2019. “The Use of Telematics Devices to Improve Automobile Insurance Rates.” Risk Analysis 39 (3): 662–72.\n\n\nGuillen, Montserrat, Jens Perch Nielsen, and Ana M Perez-Marin. 2021. “Near-Miss Telematics in Motor Insurance.” Journal of Risk and Insurance 88 (3): 569–89.\n\n\nHuang, Yifan, and Shengwang Meng. 2019. “Automobile Insurance Classification Ratemaking Based on Telematics Driving Data.” Decision Support Systems 127: 113–56.\n\n\nLemaire, Jean, Sojung Carol Park, and Kili C. Wang. 2016. “The Use of Annual Mileage as a Rating Variable.” ASTIN Bulletin 46 (1): 39–69.\n\n\nLichtenstein, Ellen. 2022. “Which States Ban Gender-Rating in Insurance Premiums?” AgentSync.\n\n\nLindholm, Mathias, Ronald Richman, Andreas Tsanakas, and Mario V Wuthrich. 2022. “Discrimination-Free Insurance Pricing.” ASTIN Bulletin: The Journal of the IAA 52 (1): 55–89.\n\n\nPaefgen, Johannes, Thorsten Staake, and Elgar Fleisch. 2014. “Multivariate Exposure Modeling of Accident Risk: Insights from Pay-as-You-Drive Insurance Data.” Transportation Research Part A: Policy and Practice 61: 27–40.\n\n\nPaefgen, Johannes, Thorsten Staake, and Frederic Thiesse. 2013. “Evaluation and Aggregation of Pay-as-You-Drive Insurance Rate Factors: A Classification Analysis Approach.” Decision Support Systems 56: 192–201.\n\n\nReid, T. R. 1985. “Montana Implements Policy of ‘Unisex‘ Insurance.” Washington Post.\n\n\nSo, Banghee, Jean-Philippe Boucher, and Emiliano A Valdez. 2021. “Cost-Sensitive Multi-Class Adaboost for Understanding Driving Behavior Based on Telematics.” ASTIN Bulletin: The Journal of the IAA 51 (3): 719–51.\n\n\nTselentis, Dimitrios I, George Yannis, and Eleni I Vlahogianni. 2016. “Innovative Insurance Schemes: Pay as/How You Drive.” Transportation Research Procedia 14: 362–71.\n\n\nVerbelen, Roel, Katrien Antonio, and Gerda Claeskens. 2018. “Unravelling the Predictive Power of Telematics Data in Car Insurance Pricing.” Journal of the Royal Statistical Society Series C: Applied Statistics 67 (5): 1275–1304.\n\n\nWuthrich, Mario V. 2017. “Covariate Selection from Telematics Car Driving Data.” European Actuarial Journal 7: 89–108."
  },
  {
    "objectID": "Theory.html#ratemaking-theory",
    "href": "Theory.html#ratemaking-theory",
    "title": "1  Theoretical concepts",
    "section": "1.1 Ratemaking Theory",
    "text": "1.1 Ratemaking Theory\nFor an insurance contract \\(t\\), \\(t=1, \\ldots, T\\), we define the following components:\n\nthe random variable \\(N_{t}\\) represents the annual claims number;\nfor \\(n_{t} = n &gt; 0\\), let \\(\\boldsymbol{Z}_{t} = \\left(Z_{t,1}, \\ldots, Z_{t, n}\\right)\\) be the random vector of claims costs associated with this contract. We define this vector only for a positive observed claims number;\nthe premium is generally calculated considering specific observable characteristics of each contract. We denote these characteristics by \\(\\boldsymbol{X}_{t} =\\left(x_{t, 0}, \\ldots ,x_{t,q}\\right)\\);\nthe random variable \\(Y_{t}\\) represents total cost associated with contract \\(t\\):\n\n\\[Y_{t}=\n\\begin{cases}\n\\sum_{k=1}^{N_{t}}Z_{t,k}&\\text{if  $N_{t} &gt;0$}\\\\ \\\\\n    0 & \\text{if $N_{t}=0$.}\n\\end{cases}\\]\nAn insured person exchanges his risk \\(Y_t\\) against a constant \\(\\pi\\) corresponding to an insurance premium. Generally, in actuarial science, we minimize the squared error \\(\\min_p E[(Y_t-p)^2|\\boldsymbol{X}_{t}]\\) to obtain the premium: \\(\\pi_{t}^{(Y)} = E[Y_t|\\boldsymbol{X}_{t}]\\). We can consider two strategies to evaluate this value:\n\nhe frequency-severity approach where we multiply the frequency component premium and the severity component premium according to some assumptions, i.e., \\[\\underbrace{E[Y_{t}|\\boldsymbol{X}_{t}]}_{\\pi_{t}^{(Y)}} = \\underbrace{E[N_{t}|\\boldsymbol{X}_{t}]}_{\\pi_{t}^{(N)}}\\underbrace{E[Z_{t,k}|\\boldsymbol{X}_{t}]}_{\\pi_{t}^{(Z)}}.\\] In this approach, for a contract \\(t\\), we generally assume independence between the frequency and the severity components. Moreover, we assume that \\(Z_{t,k}|\\boldsymbol{X}_{t}\\) are identically distributed.\nThe conditional approach where we directly model the total loss distribution, i.e.,\n\\[\\pi_{t}^{(Y)} = E[Y_{t}|\\boldsymbol{X}_{t}] = \\int y f_{Y_{t}|\\boldsymbol{X}_{t}}(y) \\, dy.\\] Although this approach is possible, usually using a Tweedie family distribution, it complicates the interpretation of the results. For example, the effect of the same covariate on severity may hide the effect of a covariate on frequency.\n\nThe Poisson distribution is the base model for the number of claims in P&C insurance, which has valuable and well-known statistical properties. The probability mass function is\n\\[\\Pr(N_{t} = n| \\boldsymbol{X}_{t}) = \\left(\\lambda_{i,t}\\right)^n\\exp\\left(-\\lambda_{i,t}\\right)/n!,\\, n = 0, 1, 2, \\ldots,\\]\nwhere \\(\\lambda_t\\) is a function. Traditionally, we assume a log-linear relationship between the mean parameter and the policyholder’s and claim’s characteristics such as sex, age, and marital status, e.g., \\(\\lambda_{t} = \\exp\\left(\\boldsymbol{X}_{t}\\boldsymbol{\\beta}\\right)\\) and \\(\\boldsymbol{\\beta}\\) is a column vector containing parameters. The Poisson distribution implies equidispersion, i.e., \\(E[N_{t}| \\boldsymbol{X}_{t}] = Var[N_{t}| \\boldsymbol{X}_{t}]\\) which is, usually, a too strong assumption in ratemaking. However, for our project, we restrict ourselves to the overdispersed Poisson, where \\(Var[N_{t}| \\boldsymbol{X}_{t}] = \\phi E[N_{t}| \\boldsymbol{X}_{t}] &gt; E[N_{t}| \\boldsymbol{X}_{t}]\\).\nFor our analysis of claim severity, we will use the Gamma distribution. The probability density function is\n\\[f_{Z|\\boldsymbol{X}_{t}}(z) = \\frac{z^{\\alpha - 1}e^{-z/\\theta}}{\\Gamma(\\alpha)\\theta^\\alpha},\\, z &gt;  0,\\] where \\(\\alpha\\) is the shape parameter and \\(\\theta\\) is the scale parameter. The expected value is \\(\\alpha\\theta\\) and the variance is \\(\\alpha\\theta^2\\). Usually, severity is less heterogeneous than frequency, and many available covariates have little impact on the prediction. Beyond the Gamma distribution, the inverse Gaussian distribution is also a possibility to consider. However, with a cubic variance, the possibility of having statistically significant estimators is even lower."
  },
  {
    "objectID": "Theory.html#goodness-of-fit",
    "href": "Theory.html#goodness-of-fit",
    "title": "1  Theoretical concepts",
    "section": "1.2 Goodness of fit",
    "text": "1.2 Goodness of fit\nThe idea of the prediction score is to obtain a numerical value to assess the quality of a model’s prediction on new data. By convention, we assume the objective is to minimize the prediction score. More specifically, to evaluate the model \\(P\\), we calculate a penalty \\(s(P, x)\\) to determine the prediction error.\nFor a model \\(P\\), we get the model’s prediction score, \\(S(P)\\), by taking the average (or the sum) penalty over \\(x\\) observations in a database (which has ever been used to estimate any parameters or properties of the model \\(P\\)):\n\\[S(P) = \\frac{1}{n} \\sum_{i=1}^n s(P, x_i).\\]\nWe can list some relevant penalties:\n\nLogarithmic penalty: \\[\\text{logs}(P, x) = -\\log(\\Pr(N = x)),\\] where \\(\\Pr()\\) is the probability mass function under model \\(P\\), or \\[\\text{logs}(P, x) = -\\log(f_Z(x)),\\] where \\(f_Z()\\) is the probability density function under model \\(P\\);\nQuadratic penalty: \\[\\text{quad}(P, x) = -2 \\Pr(N = x) + \\left(\\sum_{j=0}^{\\infty} \\Pr(N = j)^2\\right)^2,\\] where \\(\\Pr()\\) is the probability mass function under model \\(P\\);\nSquared error penalty: \\[\\text{sq.err}(P, x) = (x - \\lambda_P)^2,\\] where \\(\\lambda_P\\) is the predicted value under model \\(P\\);\nSpherical penalty: \\[\\text{sph}(P, x) = - \\frac{\\Pr(N =x )}{\\sum_{j=0}^{\\infty} \\Pr(N = j)^2},\\] where \\(\\Pr()\\) is the probability mass function under model \\(P\\);\n\nPoisson Deviance penalty: \\[\\text{dev.poi}(P, x) = 2\\left( x \\ln\\left(\\frac{x}{\\lambda_P}\\right) + (x - \\lambda_P) \\right),\\] where \\(\\lambda_P\\) is the predicted value under model \\(P\\).\n\nFor more details on the properties of scores and the assessment of counting models, one can consult (Czado, Gneiting, and Held 2009)."
  },
  {
    "objectID": "Theory.html#modelling-approaches",
    "href": "Theory.html#modelling-approaches",
    "title": "1  Theoretical concepts",
    "section": "1.3 Modelling approaches",
    "text": "1.3 Modelling approaches\nThis paper separately models the frequency and severity components using traditional, telematics, and sensitive explanatory variables. In a regression context, we have a database of size \\(n\\): \\(\\{z_i; \\mathbf{x}_i\\}_{i = 1, 2, \\ldots, n}\\), where \\(\\mathbf{x}_i\\) is a vector on size \\(q\\) of covariates (continuous and categorical) and \\(z_i\\) is the response variable (e.g., the severity of a claim). The objective is to predict the value of the response variable \\(y^*\\) for a new observation whose covariates are \\(\\mathbf{x}^*\\).\nWe have selected two families of models:\n\nGLM-net: a parametric model for which the interpretation of the parameters is simple. We define a generalized linear model (GLM) with the following structure: \\[g(E[Y]) = \\beta_0  + \\sum_{j=1}^q \\beta_jx_{j},\\] where \\(g()\\) is the link function. In this report, we always assume a logarithmic link, i.e., \\[E[Y] = e^{\\beta_0  + \\sum_{j=1}^q \\beta_jx_{j}}.\\] We add an Elastic-net regularization to this model to select the covariates and estimate parameters. This method is seen as a combination of Lasso and Ridge regressions, and we refer to (Hastie et al. 2009) for more details about this approach. One of the advantages of this approach is that it solves the redundancy of variables and the multicollinearity of risk factors. The idea of the procedure is to impose constraints on the coefficients of the model. Excluding the intercept from the procedure, the constraint to be added to the log-likelihood score to be maximized is expressed as: \\[\\left( \\alpha \\sum_{j=1}^{q+1}|\\beta_j| + \\frac{1-\\alpha}{2}\\sum_{j=1}^{q+1}\\beta_j^{2}\\right) \\leq \\lambda, \\, \\lambda &gt; 0, \\, 0 \\leq \\alpha \\leq 1.\\] This penalty constraint depends on the values chosen for the parameters \\(\\alpha\\) and \\(\\lambda\\). If \\(\\alpha = 0\\), the Elastic-net method is equivalent to a Lasso regression. In contrast, if \\(\\alpha = 1\\), it is equivalent to a Ridge regression. In our document, for each model, the optimal values of \\(\\lambda\\) and \\(\\alpha\\) were obtained by cross-validation using deviance as a selection criterion.\nXGBoost model: a non parametric model from the machine learning field and widely used in the actuarial industry. However, the interpretation of the parameters is complex and it often works like a black box. Boosting is a meta-algorithm that improves the predictive power of a simpler model (weak learner). It aims to build \\(B\\) models sequentially: the model \\(B\\) depends on the model \\((B-1)\\), which depends on the model \\((B-2)\\), etc. Each new model is built specifically to improve the predictions made by the previous model. In particular, XGBoost, for eXtreme Gradient Boosting, is an open-source software library that provides a regularizing gradient boosting framework. In this report, we use this mete-algorithm with trees as wear learner.\n\n\n\n\n\nCzado, Claudia, Tilmann Gneiting, and Leonhard Held. 2009. “Predictive Model Assessment for Count Data.” Biometrics 65 (4): 1254–61.\n\n\nHastie, Trevor, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Vol. 2. Springer."
  },
  {
    "objectID": "DataSummary.html#preamble",
    "href": "DataSummary.html#preamble",
    "title": "2  Data Summary",
    "section": "2.1 Preamble",
    "text": "2.1 Preamble\n\nChapter ObjectivePackagesData\n\n\nBefore delving into the development of more advanced statistical models for variable selection and parameter estimation, it is pertinent to analyze all the covariates available in the dataset used for this study. An individual and more in-depth analysis will thus be conducted for each of these segmentation variables. This approach will notably allow:\n\nBetter understanding of how these covariates could explain the risk of accidents;\n\nProposing transformations or groupings of modalities.\n\nWe will cover all segmentation variables available in the database. We have divided the variables into different categories:\n\nSection 5.2: Traditional covariates:\n- Section 2.2.1: Contract duration,\n- Section 5.2.1: Sensitive information,\n- Section 5.2.2: Others.\n\nSection 5.3: Telematic covariates:\n- Section 5.3.1: Vehicle usage level,\n- Section 5.3.2: Type of vehicle usage,\n- Section 5.3.3: Quality of driving.\n\nIt is crucial to emphasize that the covariates under study will all exhibit strong correlations among themselves. For instance, younger insured individuals will likely have different credit score distributions compared to older insured individuals, male driving behaviors may differ from female driving behaviors, or the total distance traveled by an insured individual will be linked to the duration of his contract. Thus, the marginal impact of each segmentation variable may partly be explained by the effects induced by other covariates.\n\n\nHere is the list of packages that will be used:\n\n\nCode\nlibrary(tidyverse)\nlibrary(vtable)\nlibrary(rpart)\nlibrary(repr)\nlibrary(rpart.plot)\nlibrary(rfCountData)\nlibrary(gam)\n\n\n\n\nThe following R script loads the available data. The database has been divided into a train portion containing 80% of the original synthetic data, and the remaining 20% of the synthetic data has been placed in a test database.\n\n\nCode\ndataS &lt;- read.csv('Data/Synthetic.csv')\n\n# Modifications \ndataS &lt;- dataS %&gt;%\n  mutate(Territory = as.factor(Territory)) %&gt;%\n  select(-c('Annual.pct.driven', 'Annual.miles.drive'))\n\ndata.select &lt;- dataS\n\n# Train-test et folds\nset.seed(123)\ntrain &lt;- data.select %&gt;% sample_frac(0.8, replace = FALSE)\ntest &lt;- data.select %&gt;% anti_join(train)"
  },
  {
    "objectID": "DataSummary.html#sec-vartrad",
    "href": "DataSummary.html#sec-vartrad",
    "title": "2  Data Summary",
    "section": "2.2 Traditional covariates",
    "text": "2.2 Traditional covariates\n\n2.2.1 Contract duration\nSpecial attention should be given to the variable representing the duration of the contract. In traditional ratemaking models based on count distributions, such as the Poisson distribution, it is usually assumed that the duration of the contract is an offset variable rather than a covariate for which a parameter needs to be estimated. For example, when using an offset variable, if an insured individual is covered for only half of the year, the insurer will offer him a premium that is half the size of what it would be for full-year coverage. Some papers question this approach and introduce contract duration as a covariate ((Boucher and Denuit 2007; Duval, Boucher, and Pigeon 2022, 2023)), suggesting that an insured individual covered for half the year should have a premium that is either larger or smaller than half of that for a full year.\nTo stimulate further thought, the numerical analysis below visualizes the average number of claims observed based on groupings of the duration of the contract. Although an increase in the number of claims based on the duration of the contract may be observed, it can be noted that the relationship is not perfectly linear compared to the dashed line. To remain consistent with traditional pricing approaches, we will however keep the duration of the contract as a measure of exposure to risk for now. However, with the telematics information available, the distance driven is also available and could be an alternative to consider for representing exposure to risk in automobile insurance. We will discuss this situation in Section Section 5.3.1.\n\n\nCode\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/366, \n                Duration.group = ceiling(Duration.y/0.05) * 0.05) %&gt;%\n  group_by(Duration.group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            nb=n()) %&gt;% \n  mutate(freq = NB_Claim/nb) %&gt;%\n  ggplot(aes(x=Duration.group, y=freq)) + \n  geom_point(aes(size=nb), color='black') + \n  #geom_smooth(aes(weight = nb),se=F) + \n  labs(x = 'Contract Duration',\n       y = 'Average Nnumber of Claims') +\n  geom_segment(aes(x = 0, y = 0, xend = 1, yend = max(freq)), linetype='dashed')+\n  guides(size = guide_legend(title = \"Number of contracts\")) +\n  xlim(0,1)+ylim(0, 0.06)+\n  theme_bw()\n\n\n\n\n\nFigure 2.1: Average number of claims vs. contract duration\n\n\n\n\n\n\n2.2.2 Sensitive information\n\nCredit scoreAge of the insuredSex of the insuredMarital StatusInsured’s territoryCorrelation\n\n\nWe conduct the same exercise as for the duration of the contract, but this time with the credit score. In the graph below, we can observe the distribution of the credit score in the insurance portfolio: the points indicate the observed claims frequency, the size of the points measures the total exposures for each group, and a trend curve (in red) for the frequency has been added. Thus, we can see that the relationship between the number of claims and the credit score is not linear, but generally, a better credit score implies a lower Claims frequency.\nThe link between the frequency of auto insurance claims and credit scores has been previously examined. Referring to (Wu and Guszcza 2003), it was concluded that this relationship persists even after controlling for numerous other variables. Despite the statistical association between claims experience and credit score, establishing causality remains elusive. Some argue that a correlation exists between responsible financial behavior and safe driving, or that individuals with higher credit scores may have easier access to newer vehicles equipped with better safety features, potentially reducing accident risk. However, these explanations lack conviction: the use of credit scoring appears to target young drivers lacking established credit histories, new immigrants, or generally economically disadvantaged populations. Coupled with the opacity of credit score calculation by private entities, it is reasonable to view credit score as a sensitive variable warranting scrutiny in automobile pricing.\n\n\nCode\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(Credit.score/25) * 25) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + \n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Credit Score',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n#  xlim(0,1)+ylim(0, 0.06)+\n  theme_bw()\n\n\n\n\n\nFigure 2.2: Observed claims frequency vs. credit score\n\n\n\n\n\n\nJust like the credit score, the age of the insured is a sensitive variable in ratemaking. The graph below illustrates the relationship between the age of the insured individual and their Claims frequency. A fairly strong relationship between the age of the insured individual and the Claims frequency can be observed.\nAs discussed earlier in this project, age is a controversial segmentation variable. Indeed, one might argue that there is no causal relationship between age and the likelihood of having an accident, and that age is rather used to identify drivers who may lack driving experience, who could be more reckless, and who might be less mature. We can expect that driving behavior measured by telematics devices could better identify such drivers.\n\n\nCode\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(Insured.age/3) * 3) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) + \n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Age of the insured',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n#  xlim(0,1)+ylim(0, 0.06)+\n  theme_bw()\n\n\n\n\n\nFigure 2.3: Observed claims frequency vs. age of the insured\n\n\n\n\n\n\nThe graph below shows the distribution of the insured individual’s gender in the portfolio (in bars), as well as the observed Claims frequency for each of the two groups (in red). It can be seen that there are more men in the portfolio. It can also be observed that the Claims frequency for both men and women is highly similar. Using the Welch two sample t-test, we obtain a \\(p\\)-value of \\(0.2705\\) meaning that the null hypothesis, the true difference in means is equal to \\(0\\), cannot be rejected.\n\n\nCode\ntemp &lt;- train %&gt;%\n  mutate(Var_ = Insured.sex) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(nbclaim = sum(NB_Claim),\n            expo = sum(Duration/365.25)) %&gt;%\n  mutate(freq = nbclaim/expo)\n\ndiv &lt;- min(temp$expo/temp$freq)/1.5\n\nggplot(data = temp, aes(x = Var_, y = expo, linetype = \"Total Exposures\")) + #start plot by by plotting bars\n  geom_bar(stat = \"identity\", alpha=0.5) + \n  geom_point(data = temp, aes(x = Var_, y = (freq)*div, group = 1), color='red', size=3,\n            inherit.aes = FALSE) +\n  geom_line(data = temp, aes(x = Var_, y = (freq)*div, group = 1, linetype = \"Claims Frequency\"), color='red', size=0.7,\n            inherit.aes = FALSE) +\n    labs(x = 'Sex of the insured',\n       y = 'Total exposures') +\n  scale_y_continuous(sec.axis = sec_axis(~./div, name = \"Claims frequency\")) +\n  guides(linetype=guide_legend(title=\"\")) +\n  theme_classic()+\n  theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n\n\nFigure 2.4: Observed claims frequency vs. sex of the insured\n\n\n\n\n\n\nThe marital status of the insured individual is not one of the most important segmentation variables. However, a statistical test rejects the null hypothesis (the true difference in means is equal to \\(0\\)), so we keep this variable in our analysis. The graph below shows the number of insured individuals in each category and the observed claim frequency.\n\n\nCode\ntemp &lt;- train %&gt;%\n  mutate(Var_ = Marital) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(nbclaim = sum(NB_Claim),\n            expo = sum(Duration/365.25)) %&gt;%\n  mutate(freq = nbclaim/expo)\n\ndiv &lt;- min(temp$expo/temp$freq)/0.8\n\nggplot(data = temp, aes(x = Var_, y = expo, linetype = \"Total Exposures\")) + #start plot by by plotting bars\n  geom_bar(stat = \"identity\", alpha=0.5) + \n  geom_point(data = temp, aes(x = Var_, y = (freq)*div, group = 1), color='red', size=3,\n            inherit.aes = FALSE) +\n  geom_line(data = temp, aes(x = Var_, y = (freq)*div, group = 1, linetype = \"Claims Frequency\"), color='red', size=0.7,\n            inherit.aes = FALSE) +\n    labs(x = 'Marital status of the insured',\n       y = 'Total exposures') +\n  scale_y_continuous(sec.axis = sec_axis(~./div, name = \"Claims frequency\")) +\n  guides(linetype=guide_legend(title=\"\")) +\n  theme_classic()+\n  theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n\n\n\n\nFigure 2.5: Observed claims frequency vs. marital status of the insured\n\n\n\n\n\n\n\n\nThe pricing of automobile insurance based on territory usage is common in many insurance companies worldwide. This approach relies on the principle that the risk incurred by a driver may vary depending on the geographical location where they most frequently drive. Thus, insurers may assess the risks associated with different geographical areas, considering traffic density, crime rates, weather conditions, and road quality. In practice, territories in automobile insurance are often chosen based on practical criteria such as blocks of streets, the same postal code, and divisions created by a river or highway.\nTherefore, while using territory in automobile insurance pricing may seem like an objective method for evaluating risks, it can also have significant social implications. Indeed, it has been shown that this approach can contribute to dividing population groups based on criteria such as race, economic level, or even profession. For example, disadvantaged or ethnically predominant urban neighborhoods are often grouped in one territory, while another more affluent neighborhood is associated with a different territory. By segmenting the portfolio according to territory, insurers often price based on socio-economic criteria.\nIn order to protect the privacy of the insured individuals in the database, their specific addresses and postal codes are not available. Instead, a variable known as “territory” is used, which is represented by a numerical value ranging from \\(11\\) to \\(91\\). However, this numerical representation makes it challenging to interpret or correlate with current public data. The graph below demonstrates the influence of territory on the modeling of claim numbers. Upon analysis, we find that territory does not have a significant impact on claim frequency, except for category \\(54\\). In the absence of this category, a statistical test known as the F test fails to reject the null hypothesis, indicating that territory has no effect on the response variable (with a \\(p\\)-value of \\(0.064\\)). However, due to the abstract nature of the Territory variable, it is challenging to interpret its influence or relate it to current public data.\n\n\nCode\ntemp &lt;- train %&gt;%\n  mutate(Var_ = Territory) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(nbclaim = sum(NB_Claim),\n            expo = sum(Duration/365.25)) %&gt;%\n  mutate(freq = nbclaim/expo)\n\ndiv &lt;- min(temp$expo/temp$freq)/0.4\n\nggplot(data = temp, aes(x = Var_, y = expo, linetype = \"Total Exposures\")) + #start plot by by plotting bars\n  geom_bar(stat = \"identity\", alpha=0.5) + \n  geom_point(data = temp, aes(x = Var_, y = (freq)*div, group = 1), color='red', size=1.5,\n            inherit.aes = FALSE) +\n  geom_line(data = temp, aes(x = Var_, y = (freq)*div, group = 1, linetype = \"Claims Frequency\"), color='red', size=0.7,\n            inherit.aes = FALSE) +\n  labs(x = 'Territory',\n       y = 'Total exposures') +\n  scale_x_discrete(labels = NULL, breaks = NULL)+\n  scale_y_continuous(sec.axis = sec_axis(~./div, name = \"Claims frequency\")) +\n  guides(linetype=guide_legend(title=\"\")) +\n  theme_classic()+\n  theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n\n\nFigure 2.6: Observed claims frequency vs. territory\n\n\n\n\n\n\nThe correlation matrix below indicates the level of dependence between each of the sensitive variables. For instance, it can be observed that the insured’s age is correlated with the credit score and marital status.\n\n\nCode\ntrad.vars  &lt;- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\")\n\ndata = data.matrix(train[, paste(trad.vars)])\ncorrelation_matrix &lt;- abs(cor(data))\n\nplt &lt;- ggplot(data = as.data.frame(as.table(correlation_matrix)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=4) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=10, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 10),\n          plot.title = element_text(size = 10),   # Adjust plot title font size\n          legend.text = element_text(size = 10),\n          legend.title = element_text(size=10))\nprint(plt)\n\n\n\n\n\nFigure 2.7: Correlation between sensible covariates\n\n\n\n\n\n\n\n\n\n2.2.3 Other covariates\nOther traditional segmentation variables are also available in the database. Unlike the variables we have just covered, these other traditional segmentation variables are not sensitive and are socially accepted in automobile pricing.\n\nAge of the carUse of the carRegionYears without ClaimCorrelation\n\n\nThe graph below highlights the relationship between claim frequency and the age of the vehicle. A fairly clear link is observed: the newer a vehicle is, the higher its risk of having an insurance claim.\n\n\nCode\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(Car.age/1) * 1) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Age of the car',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n#  xlim(0,1)+ylim(0, 0.06)+\n  theme_bw()\n\n\n\n\n\nFigure 2.8: Observed claims frequency vs. age of the car\n\n\n\n\n\n\nIn the data used, vehicle usage has been categorized into 4 levels: commercial, commute, farm, and private. We can observe some difference in the average number of claims between each of the categories, as illustrated in the graph below. It is also important to note a significant difference in the distribution of this variable: the vast majority of insured individuals are in the commute and private groups. Thus, even if the claim frequency of insured individuals in the commercial and farm groups differs, the small number of contracts in these groups likely limits the predictive capacity of this variable.\n\n\nCode\ntemp &lt;- train %&gt;%\n  mutate(Var_ = Car.use) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(nbclaim = sum(NB_Claim),\n            expo = sum(Duration/365.25)) %&gt;%\n  mutate(freq = nbclaim/expo)\n\ndiv &lt;- min(temp$expo/temp$freq)/0.07\n\nggplot(data = temp, aes(x = Var_, y = expo, linetype='Total Exposures')) + #start plot by by plotting bars\n  geom_bar(stat = \"identity\", alpha=0.5) + \n  geom_point(data = temp, aes(x = Var_, y = (freq)*div, group = 1), color='red', size=3,\n            inherit.aes = FALSE) +\n  geom_line(data = temp, aes(x = Var_, y = (freq)*div, group = 1, linetype = 'Claims Frequency'), color='red', size=0.7,\n            inherit.aes = FALSE) +\n    labs(x = 'Use of the car',\n       y = 'Total exposures') +\n  #scale_x_discrete(labels = NULL, breaks = NULL)+\n  scale_y_continuous(sec.axis = sec_axis(~./div, name = \"Claims frequency\"))  + \n  guides(linetype=guide_legend(title=\"\")) +\n  theme_classic()+\n  theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n\n\nFigure 2.9: Observed claims frequency vs. use of the car\n\n\n\n\n\n\nIn addition to territory, a slightly coarser grouping of the insured individual’s residence has been made for insurance contracts. Indeed, there are two main regions: urban and rural. The difference in claims between these two types of insured individuals is illustrated in the graph below. According to the Welch Two Sample t-test, the difference in means is significant (\\(p\\)-value very close to \\(0\\)).\n\n\nCode\ntemp &lt;- train %&gt;%\n  mutate(Var_ = Region) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(nbclaim = sum(NB_Claim),\n            expo = sum(Duration/365.25)) %&gt;%\n  mutate(freq = nbclaim/expo)\n\ndiv &lt;- min(temp$expo/temp$freq)/0.5\n\nggplot(data = temp, aes(x = Var_, y = expo, linetype='Total Exposures')) + #start plot by by plotting bars\n  geom_bar(stat = \"identity\", alpha=0.5) + \n  geom_point(data = temp, aes(x = Var_, y = (freq)*div, group = 1), color='red', size=3,\n            inherit.aes = FALSE) +\n  geom_line(data = temp, aes(x = Var_, y = (freq)*div, group = 1, linetype='Claims Frequency'), color='red', size=0.7,\n            inherit.aes = FALSE) +\n    labs(x = 'Region',\n       y = 'Total exposures') +\n  #scale_x_discrete(labels = NULL, breaks = NULL)+\n  scale_y_continuous(sec.axis = sec_axis(~./div, name = \"Claims frequency\")) + \n  guides(linetype=guide_legend(title=\"\")) +\n  theme_classic()+\n  theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n\n\n\n\nFigure 2.10: Observed claims frequency vs. region\n\n\n\n\n\n\n\n\nThe variable tracking the number of claim-free years is also crucial in modeling risk in automobile insurance. According to various papers, including (Lemaire 1985, ch.7), this variable is considered particularly significant to the extent that if only one segmentation variable were to be used in ratemaking, it would likely be something related to the insured’s experience.\nThe graph below illustrates the relationship between the number of claim-free years and the number of reported automobile claims. As indicated by numerous studies, a decreasing relationship can be observed, suggesting that insured individuals who have had few or no claims in the past are also likely to make fewer claims in the future.\n\n\nCode\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = pmin(Years.noclaims, 60)) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Years without claim',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(0, 0.06)+\n  theme_bw()\n\n\n\n\n\nFigure 2.11: Observed claims frequency vs. years without claim\n\n\n\n\n\n\nBelow, we can observe the correlation that exists between the traditional segmentation variables. On the left, we have a correlation matrix indicating a strong dependency between vehicle usage and the number of claim-free years. On the right, emphasis is placed on the relationship between the covariates studied in this subsection and the sensitive variables. Thus, a strong dependency between the age of the insured and the number of claim-free years can be observed. Vehicle usage is also linked to the age of the insured.\n\n\nCode\ntrad.vars  &lt;- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\",\n                \"Car.use\", \"Region\", \"Car.age\", \"Years.noclaims\")\n\ndata = data.matrix(train[, paste(trad.vars)])\ncorrelation_matrix &lt;- abs(cor(data))\n\ncorr1 &lt;- correlation_matrix[6:9, 6:9]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=6) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\ncorr1 &lt;- correlation_matrix[1:5, 6:9]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=6) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\n\n\n\n\n\n\n\n(a) Other traditionnal covariates\n\n\n\n\n\n\n\n(b) With sensible covariates\n\n\n\n\nFigure 2.12: Correlation between traditional covariates"
  },
  {
    "objectID": "DataSummary.html#sec-vartele",
    "href": "DataSummary.html#sec-vartele",
    "title": "2  Data Summary",
    "section": "2.3 Telematic covariates",
    "text": "2.3 Telematic covariates\n\n2.3.1 Vehicle usage level\nMany scientific articles have shown that what appeared to be most relevant as telematic information for pricing was not the quality of driving, but rather the level of vehicle usage. In this first part of the telematics variable analysis, we will therefore focus on these variables.\n\nAnnual Miles DrivenContract duration revisitedDays per weekCorrelation\n\n\nIn addition to the declared distance, we also have access to the actual distance traveled by the insured through the telematics device installed in the vehicle.It is increasingly recognized that the distance driven in a car can constitute a more precise measure of risk exposure than simply the duration of the insurance contract. Indeed, the frequency and distance of trips are crucial factors in the likelihood of an accident occurring. For example, one might assume that a driver who regularly covers long distances is statistically more likely to be involved in an accident than a driver who uses their vehicle sporadically, even if both have the same duration of insurance contract.\nThe graph below illustrates the claim frequency as a function of the distance driven. Although there appears to be a proportional relationship between the distance driven and the number of claims, there is also a decrease in claim frequency for drivers who have driven extensively. This decreasing frequency relationship for heavy drivers has been observed in other scientific research (see Cote and Boucher or Turcotte).\n\n\nCode\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ifelse(Total.miles.driven &lt; 20000, ceiling(Total.miles.driven/1000) * 1000, ceiling(Total.miles.driven/5000) * 5000)) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Total miles driven',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  xlim(0,40000)+\n  ylim(0, 0.34)+\n  theme_bw()\n\n\n\n\n\n\n\nFigure 2.13: Claims Frequency vs. yotal miles driven\n\n\n\n\n\n\n\n\nA form of competition between the duration of the contract and the distance driven (measured) appears to emerge as the adequate measure of risk exposure. While distance driven should be more predictive, the duration of the contract exhibits a more consistent relationship with claim frequency, resembling what risk exposure should entail. We thus propose creating a new covariate for our analysis: the average number of kilometers traveled per day. This new variable will solely correspond to the ratio of the total number of kilometers traveled divided by the duration of the contract.\nThis new variable can represent a form of driving activity intensity. By replacing the total kilometers traveled with the average daily kilometers, we can revert to the duration of the contract as the classical measure of risk exposure. The graphs below illustrate the relationship between daily miles and claim frequency.\n\n\nCode\ntrain %&gt;%\n  dplyr::mutate(Miles.per.day = Total.miles.driven/Duration, \n                Duration.y = Duration/365.25, \n                Group = ceiling(Miles.per.day/7.5) * 7.5) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average miles driven per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  xlim(0,127.5)+\n  #ylim(0, 0.31)+\n  theme_bw()\n\n\n\n\n\n\n\nFigure 2.14: Claims Frequency vs. average miles driven per day\n\n\n\n\n\n\n\n\nAnother intensity measure that may be close to the usage percentage is the average number of vehicle uses per week. The graph below shows the relationship between claim frequency and this variable. An increase in claim frequency is observed as the number of days used increases. However, it can also be seen that insured individuals who use their car an average of 7 days per week appear to deviate from the general trend.\n\n\nCode\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(Avgdays.week/0.5) * 0.5) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of days per week',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(0, 0.31)+\n  theme_bw()\n\n\n\n\n\n\n\nFigure 2.15: Claims Frequency vs. average number of days per week the car is used\n\n\n\n\n\n\n\n\nOnce again, we can observe the dependency between the covariates by looking at the results below. It’s interesting to note that mean number of days used is linked to the age of the insured, for example.\n\n\nCode\ntrad.vars  &lt;- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\",\n                \"Miles.per.day\", \"Avgdays.week\")\n\ntrain &lt;- train %&gt;%\n  dplyr::mutate(Miles.per.day = Total.miles.driven/Duration)\n                \ndata = data.matrix(train[, paste(trad.vars)])\ncorrelation_matrix &lt;- abs(cor(data))\n\ncorr1 &lt;- correlation_matrix[6:7, 6:7]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=6) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\ncorr1 &lt;- correlation_matrix[1:5, 6:7]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=6) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\n\n\n\n\n\n\n\n(a) Telematic covariates\n\n\n\n\n\n\n\n(b) With sensible covariates\n\n\n\n\nFigure 2.16: Correlation between covariates\n\n\n\n\n\n\n\n\n2.3.2 Type of vehicle usage\nInstead of using the telematics device solely to measure vehicle usage, it is also possible to see if certain types of vehicle usage are indicators of a higher risk of claims. In this section, we will analyze certain telematics information that we classify as types of usage.\n\nDaysDays (2)Days (3)Week-endTrip DurationRush hoursCorrelation\n\n\nOne pertinent piece of information is to investigate whether vehicle usage on certain days of the week predicts a higher claim frequency. Seven covariates are available in the database, each indicating the percentage of vehicle usage on a particular day of the week. It is worth noting that the sum of the 7 percentages for each contract equals 1. Thus, high vehicle usage on a Saturday corresponds to a high percentage of usage for that day, necessarily implying that the other days will have smaller percentages.\nThe 7 graphs below illustrate the claim frequency as a function of vehicle usage for each day of the week. The results obtained for each day are similar and seem to indicate that uniform vehicle usage across all 7 days (i.e., 1/7 = 14.2%) is the riskiest situation. Thus, vehicle usage for each day appears to signify something, but the information provided by these covariates likely needs transformation.\n\n\nCode\ndiv &lt;- 1/15 \n\nvar &lt;- 'Pct.drive.mon'\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=F, color='red', size=1) + \n  labs(x = 'Usage percentage per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  ylim(-0.01, 0.06)+\n  theme_bw()\n\n\nvar &lt;- 'Pct.drive.tue'\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=F, color='red', size=1) + \n  labs(x = 'Usage percentage per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  ylim(-0.01, 0.06)+\n  theme_bw()\n\n\nvar &lt;- 'Pct.drive.wed'\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=F, color='red', size=1) + \n  labs(x = 'Usage percentage per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  ylim(-0.01, 0.06)+\n  theme_bw()\n\n\nvar &lt;- 'Pct.drive.thr'\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=F, color='red', size=1) + \n  labs(x = 'Usage percentage per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  ylim(-0.01, 0.06)+\n  theme_bw()\n\n\nvar &lt;- 'Pct.drive.fri'\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=F, color='red', size=1) + \n  labs(x = 'Usage percentage per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  ylim(-0.01, 0.06)+\n  theme_bw()\n\n\nvar &lt;- 'Pct.drive.sat'\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=F, color='red', size=1) + \n  labs(x = 'Usage percentage per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  ylim(-0.01, 0.06)+\n  theme_bw()\n\n\nvar &lt;- 'Pct.drive.sun'\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=F, color='red', size=1) + \n  labs(x = 'Usage percentage per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  ylim(-0.01, 0.06)+\n  theme_bw()\n\n\n\n\n\n\n\n\n(a) Monday\n\n\n\n\n\n\n\n(b) Tuesday\n\n\n\n\n\n\n\n(c) Wednesday\n\n\n\n\n\n\n\n\n\n(d) Thursday\n\n\n\n\n\n\n\n(e) Friday\n\n\n\n\n\n\n\n(f) Saturday\n\n\n\n\n\n\n\n\n\n(g) Sunday\n\n\n\n\nFigure 2.17: Claims Frequency vs. percentage of use for each day\n\n\n\n\n\nIn light of the results obtained from the analysis of vehicle usage for each day of the week, it is appropriate to create new variables that may better represent the risk. We thus create the following variables:\n\nA variable identifying the maximum value of vehicle usage for each day;\n\nA variable identifying the minimum value of vehicle usage for each day;\n\nA variable measuring the difference between the maximum and minimum values, which have just been calculated. This variable can thus identify insured individuals who use their vehicle more on specific days, or conversely, insured individuals who typically refrain from using their vehicle on certain days of the week.\n\nFor each of these variables, the graph representing the relationship between claim frequency is illustrated below. While the graph for the maximum use does not seem to point to a significant result in explaining claim frequency, it appears that the two other graphs are more interesting: insured individuals who use their vehicle equally on all days of the week display a higher claim frequency than those who use their vehicle more on certain days.\n\n\nCode\ndf2 &lt;- train %&gt;%\nmutate(max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n       min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n       max.min = max.day - min.day,\n       Dayformax = 'Monday', \n       Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n       Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n       Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n       Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n       Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n       Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n       Dayformin = 'Monday', \n       Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n       Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n       Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n       Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n       Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n       Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin))\n\ndiv &lt;- 1/25 \nvar &lt;- 'max.day'\ndf2 %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Maximum usage percentage per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\ndiv &lt;- 1/75 \nvar &lt;- 'min.day'\ndf2 %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Minimum usage percentage per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\ndiv &lt;- 1/25 \nvar &lt;- 'max.min'\ndf2 %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Difference between percentages',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\n\n\n\n\n\n\n(a) Maximum Use\n\n\n\n\n\n\n\n(b) Minimum Use\n\n\n\n\n\n\n\n\n\n(c) Difference between maximum and minimum use\n\n\n\n\nFigure 2.18: Claims Frequency vs. use for each day\n\n\n\n\n\nContinuing with the analysis related to the days of vehicle usage, two additional variables can also be explored:\n1) A variable identifying the day of the week when the vehicle is most used; 2) A variable identifying the day of the week when the vehicle is least used.\nThe graphs below attempt to verify if the claim frequency differs for those who use their vehicle more or less on specific days of the week. It is evident that Friday is the day when insured individuals tend to use their car more frequently. Conversely, Sunday is the day when the car appears to be used the least.\nFor the claims frequency, two days appear to be slightly more significant than the others:\n\nThursday: Insured individuals who use their vehicle most frequently on Thursdays have a higher claim frequency, while those who use their vehicle less on Thursdays have a lower claim frequency.\nSunday: Insured individuals who use their vehicle most frequently on Sundays have a lower claim frequency, while those who use their vehicle less on Sundays have a higher claim frequency.\n\n\n\nCode\ndf2 &lt;- train %&gt;%\nmutate(max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n       min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n       max.min = max.day - min.day,\n       Dayformax = 'Monday', \n       Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n       Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n       Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n       Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n       Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n       Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n       Dayformin = 'Monday', \n       Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n       Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n       Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n       Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n       Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n       Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin))\n\ndf2$Dayformax &lt;- factor(df2$Dayformax , levels=c(\"Monday\",\"Tuesday\",\"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"))\ndf2$Dayformin &lt;- factor(df2$Dayformin , levels=c(\"Monday\",\"Tuesday\",\"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"))\n\n\ntemp &lt;- df2 %&gt;%\n  mutate(Var_ = Dayformax) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(nbclaim = sum(NB_Claim),\n            expo = sum(Duration/365.25)) %&gt;%\n  mutate(freq = nbclaim/expo)\n\ndiv &lt;- min(temp$expo/temp$freq)/0.5\n\nggplot(data = temp, aes(x = Var_, y = expo, linetype='Total Exposures')) + #start plot by by plotting bars\n  geom_bar(stat = \"identity\", alpha=0.5) + \n  geom_point(data = temp, aes(x = Var_, y = (freq)*div, group = 1), color='red', size=3,\n            inherit.aes = FALSE) +\n  geom_line(data = temp, aes(x = Var_, y = (freq)*div, group = 1, linetype='Claims Frequency'), color='red', size=0.7,\n            inherit.aes = FALSE) +\n    labs(x = 'Day of the week',\n       y = 'Total exposures') +\n  #scale_x_discrete(labels = NULL, breaks = NULL)+\n  scale_y_continuous(sec.axis = sec_axis(~./div, name = \"Claims frequency\")) + \n  guides(linetype=guide_legend(title=\"\")) +\n  theme_classic()+\n  theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\ntemp &lt;- df2 %&gt;%\n  mutate(Var_ = Dayformin) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(nbclaim = sum(NB_Claim),\n            expo = sum(Duration/365.25)) %&gt;%\n  mutate(freq = nbclaim/expo)\n\ndiv &lt;- min(temp$expo/temp$freq)/0.2\n\nggplot(data = temp, aes(x = Var_, y = expo, linetype='Total Exposures')) + #start plot by by plotting bars\n  geom_bar(stat = \"identity\", alpha=0.5) + \n  geom_point(data = temp, aes(x = Var_, y = (freq)*div, group = 1), color='red', size=3,\n            inherit.aes = FALSE) +\n  geom_line(data = temp, aes(x = Var_, y = (freq)*div, group = 1, linetype='Claims Frequency'), color='red', size=0.7,\n            inherit.aes = FALSE) +\n    labs(x = 'Day of the week',\n       y = 'Total exposures') +\n  #scale_x_discrete(labels = NULL, breaks = NULL)+\n  scale_y_continuous(sec.axis = sec_axis(~./div, name = \"Claims frequency\")) + \n  guides(linetype=guide_legend(title=\"\")) +\n  theme_classic()+\n  theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n\n\n\n\n\n(a) Day with the maximum use\n\n\n\n\n\n\n\n(b) Day with the minimum use\n\n\n\n\nFigure 2.19: Claims Frequency vs. sse for each day\n\n\n\n\n\nA grouping of the vehicle usage variables for the days of the week has been performed directly in the database. Thus, the usage percentages for the days from Monday to Friday have been summed, and the usage for Saturday and Sunday has been summed into another variable. Knowing that the two covariates are complementary (since the sum of both equals 100%), only one of the two variables needs to be kept. The two graphs below indeed show the same behavior, but in opposite directions.\nThe final result obtained is similar to what we observed for the individual days of the week, and it is unclear whether these covariates will remain important in the final analysis.\n\n\nCode\nvar &lt;- 'Pct.drive.wkday'\ndiv &lt;- 1/25\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Usage percentage per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Pct.drive.wkend'\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Usage percentage per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\n\n\n\n\n\n\n(a) Week day\n\n\n\n\n\n\n\n(b) Weekend day\n\n\n\n\nFigure 2.20: Claims Frequency vs. percentage of use week day and week-end day\n\n\n\n\n\nThe common explanation for claim probability highlights the use of highways. According to several studies, the risk of an accident per kilometer traveled is much lower on highways than in urban areas. Thus, the duration of each trip, or the percentage of trips exceeding a certain predetermined duration, could be relevant to analyze. The graphs of claim frequency as a function of the percentage of trips exceeding 2, 3, or 4 hours are illustrated below.\nDespite the intuition that this kind of information could be relevant, the graphs do not seem to show a particularly strong link between the proportion of long trips and claim frequency.\n\n\nCode\nvar &lt;- 'Pct.drive.2hrs'\n\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Percent of driving',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Pct.drive.3hrs'\n\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Percent of driving',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Pct.drive.4hrs'\n\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Percent of driving',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\n\n\n\n\n\n\n(a) 2 hours\n\n\n\n\n\n\n\n(b) 3 hours\n\n\n\n\n\n\n\n\n\n(c) 4 hours\n\n\n\n\nFigure 2.21: Claims Frequency vs. percentage of vehicule driven by XX hours\n\n\n\n\n\nAnother common hypothesis links the frequency of automobile insurance claims to traffic congestion. Thus, the proportion of trips made in traffic jams, whether in the morning or evening, is also available in the database. The graphs of frequency linked to these two variables are illustrated below.\nSimilar to the duration of trips, the hypothesis does not seem to be validated by statistical analysis, and it is not clear whether these variables are relevant for explaining the risk of accidents.\n\n\nCode\nvar &lt;- 'Pct.drive.rush.am'\n\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Percent of driving',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\nvar &lt;- 'Pct.drive.rush.pm'\n\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Percent of driving',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\n\n\n\n\n\n\n(a) AM Rush\n\n\n\n\n\n\n\n(b) PM Rush\n\n\n\n\nFigure 2.22: Claims Frequency vs. Percentage of vehicule driven during rush hours\n\n\n\n\n\nThe dependency between each studied covariate is illustrated below. It’s interesting to note the apparent link between vehicle usage and the age of the insured.\n\n\nCode\ntrain2 &lt;- train %&gt;%\n  mutate(Miles.per.day = Total.miles.driven/Duration,\n         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),\n         expo = Duration/365.25)\n\n\ntrad.vars  &lt;- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\",\n                \"Pct.drive.mon\", \"Pct.drive.tue\", \"Pct.drive.wed\", \"Pct.drive.thr\", \"Pct.drive.fri\", \"Pct.drive.sat\", \"Pct.drive.sun\")\n\ndata = data.matrix(train2[, paste(trad.vars)])\ncorrelation_matrix &lt;- abs(cor(data))\n\ncorr1 &lt;- correlation_matrix[6:12, 6:12]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=4) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\ncorr1 &lt;- correlation_matrix[1:5, 6:12]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=6) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\n\ntrad.vars  &lt;- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\",\n                \"max.day\", \"min.day\", \"Dayformax\", \"Dayformin\", \"Pct.drive.wkend\", \n                \"Pct.drive.2hrs\", \"Pct.drive.3hrs\", \"Pct.drive.4hrs\",\n                \"Pct.drive.rush.am\", \"Pct.drive.rush.pm\")\n\ndata = data.matrix(train2[, paste(trad.vars)])\ncorrelation_matrix &lt;- abs(cor(data))\n\ncorr1 &lt;- correlation_matrix[6:15, 6:15]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=4) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\ncorr1 &lt;- correlation_matrix[1:5, 6:15]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=6) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\n\n\n\n\n\n\n\n(a) Telematic covariates\n\n\n\n\n\n\n\n(b) With sensible covariates\n\n\n\n\n\n\n\n\n\n(c) Telematic covariates\n\n\n\n\n\n\n\n(d) With sensible covariates\n\n\n\n\nFigure 2.23: Correlation between covariates\n\n\n\n\n\n\n\n\n2.3.3 Driving behavior\nBeyond the intensity of vehicle usage, telematics devices also allow for the compilation of various statistics on driving behavior. This primarily includes sudden braking, rapid acceleration, or high-speed turns (both left and right). In this final part of the analysis of segmentation variables available in the database, we will therefore work on analyzing and transforming these variables.\n\nBrakesAccelerationsRight turnsLeft turnsCorrelation\n\n\nIn the database, we have access to a series of variables counting the number of abrupt braking events, with decelerations of 6mph, 8mph, 9mph, 11mph, 12mph, and 14mph per 1000 miles traveled. As indicated in the description, the number of abrupt braking events is normalized by the distance traveled and not by the number of insured days. Since we choose to use the number of insured days as a measure of exposure to risk, a transformation of these variables is necessary.\nBy multiplying the number of abrupt braking events by 1000 miles per thousand miles traveled, we will obtain the total number of abrupt braking events. We then divide by the number of insured days to create a new measure of driving quality: the number of daily abrupt braking events. We will perform this exercise for the 5 measures of abrupt braking events. Just like for the average daily distance traveled, we end up with a new variable measuring an intensity, this time the average daily intensity of abrupt braking events.\nFurthermore, some control will also be added to reduce the number of outliers. To do this, for each count of abrupt braking events, we will limit the obtained value by the 99th percentile.\nThe 6 graphs below illustrate the relationship between the number of daily abrupt braking events and claim frequency. For all 6 scenarios, a clear relationship can be observed between an increase in the number of abrupt braking events and the average claim frequency.\n\n\nCode\nvar &lt;- 'Brake.06miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of brakes per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Brake.08miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of brakes per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Brake.09miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of brakes per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\nvar &lt;- 'Brake.11miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of brakes per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Brake.12miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of brakes per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Brake.14miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of brakes per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\n\n\n\n\n\n\n(a) Brakes of 6 mph\n\n\n\n\n\n\n\n(b) Brakes of 8 mph\n\n\n\n\n\n\n\n(c) Brakes of 9 mph\n\n\n\n\n\n\n\n\n\n(d) Brakes of 11 mph\n\n\n\n\n\n\n\n(e) Brakes of 12 mph\n\n\n\n\n\n\n\n(f) Brakes of 14 mph\n\n\n\n\nFigure 2.24: Claims Frequency vs. Average number of brakes\n\n\n\n\n\nSimilarly to braking events, we need to convert accelerations, which are normalized by miles driven, into average daily accelerations. We will again control the possible values to not exceed the 99th percentile. The 6 graphs below illustrate the relationship between the average acceleration and claim frequency.\nOnce again, a clear link between the increase in the number of accelerations and the number of claims can be observed, except for a slight decrease with accelerations of 14mph. However, given the very small number of such extreme accelerations, this does not change our conclusion.\n\n\nCode\nvar &lt;- 'Accel.06miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of accelerations per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Accel.08miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of accelerations per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Accel.09miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of accelerations per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Accel.11miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of accelerations per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Accel.12miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of accelerations per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Accel.14miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of accelerations per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\n\n\n\n\n\n\n(a) Accelerations of 6 mph\n\n\n\n\n\n\n\n(b) Accelerations of 8 mph\n\n\n\n\n\n\n\n(c) Accelerations of 9 mph\n\n\n\n\n\n\n\n\n\n(d) Accelerations of 11 mph\n\n\n\n\n\n\n\n(e) Accelerations of 12 mph\n\n\n\n\n\n\n\n(f) Accelerations of 14 mph\n\n\n\n\nFigure 2.25: Claims Frequency vs. Average number of accelerations\n\n\n\n\n\n\n\nCode\nvar &lt;- 'Right.turn.intensity08'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of right turns per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Right.turn.intensity09'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of right turns per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Right.turn.intensity10'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of right turns per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Right.turn.intensity11'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of right turns per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Right.turn.intensity12'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of right turns per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\n\n\n\n\n\n\n(a) Right turns of 8 mph\n\n\n\n\n\n\n\n(b) Right turns of 9 mph\n\n\n\n\n\n\n\n(c) Right turns of 10 mph\n\n\n\n\n\n\n\n\n\n(d) Right turns of 11 mph\n\n\n\n\n\n\n\n(e) Right turns of 12 mph\n\n\n\n\nFigure 2.26: Claims Frequency vs. Average number of right turns\n\n\n\n\n\n\n\nCode\nvar &lt;- 'Left.turn.intensity08'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of left turns per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\nvar &lt;- 'Left.turn.intensity09'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of left turns per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\nvar &lt;- 'Left.turn.intensity10'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of left turns per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\nvar &lt;- 'Left.turn.intensity11'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of left turns per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\nvar &lt;- 'Left.turn.intensity12'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(freq = NB_Claim/expo) %&gt;%\n  ggplot(aes(x=Group, y=freq)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of left turns per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Total exposures\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\n\n\n\n\n\n\n(a) Left turns of 8 mph\n\n\n\n\n\n\n\n(b) Left turns of 9 mph\n\n\n\n\n\n\n\n(c) Left turns of 10 mph\n\n\n\n\n\n\n\n\n\n(d) Left turns of 11 mph\n\n\n\n\n\n\n\n(e) Left turns of 12 mph\n\n\n\n\nFigure 2.27: Claims Frequency vs. Average number of left turns\n\n\n\n\n\nThe correlation between all variables measuring driving quality can be analyzed by looking at the tables below. Unsurprisingly, we can observe that different accelerations and different braking events are strongly correlated. It’s even apparent that insured individuals with high accelerations likely also exhibit strong braking events. The dependency between accelerations, braking events, and sensitive variables is very weak. Thus, the driving quality as measured in the studied database may not be able to replace the predictive capacity of sensitive covariates. The intensity of turns to the left or right also does not seem to explain the sensitive variables.\n\n\nCode\ntrain2 &lt;- train %&gt;%\n  mutate(Miles.per.day = Total.miles.driven/Duration,\n         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),\n         expo = Duration/365.25)\n\n\ntrad.vars  &lt;- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\",\n                \"Accel.06miles\", \"Accel.08miles\", \"Accel.09miles\", \"Accel.11miles\", \"Accel.12miles\", \"Accel.14miles\", \n               \"Brake.06miles\", \"Brake.08miles\", \"Brake.09miles\", \"Brake.11miles\", \"Brake.12miles\", \"Brake.14miles\", \n               \"Left.turn.intensity08\", \"Left.turn.intensity09\", \"Left.turn.intensity10\", \"Left.turn.intensity11\", \"Left.turn.intensity12\",\n               \"Right.turn.intensity08\", \"Right.turn.intensity09\", \"Right.turn.intensity10\", \"Right.turn.intensity11\", \"Right.turn.intensity12\")\n\ndata = data.matrix(train2[, paste(trad.vars)])\ncorrelation_matrix &lt;- abs(cor(data))\n\ncorr1 &lt;- correlation_matrix[6:17, 6:17]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=4) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\ncorr1 &lt;- correlation_matrix[1:5, 6:17]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=6) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\n\ncorr1 &lt;- correlation_matrix[18:27, 18:27]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=4) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\ncorr1 &lt;- correlation_matrix[1:5, 18:27]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=6) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\n\n\n\n\n\n\n\n(a) Telematic covariates\n\n\n\n\n\n\n\n(b) With sensible covariates\n\n\n\n\n\n\n\n\n\n(c) Telematic covariates\n\n\n\n\n\n\n\n(d) With sensible covariates\n\n\n\n\nFigure 2.28: Correlation between covariates\n\n\n\n\n\n\n\n\n\n\nBoucher, Jean-Philippe, and Michel Denuit. 2007. “Duration Dependence Models for Claim Counts.” Blätter Der DGVFM 28 (1): 29–45.\n\n\nDuval, Francis, Jean-Philippe Boucher, and Mathieu Pigeon. 2022. “How Much Telematics Information Do Insurers Need for Claim Classification?” North American Actuarial Journal 26 (4): 570–90.\n\n\n———. 2023. “Enhancing Claim Classification with Feature Extraction from Anomaly-Detection-Derived Routine and Peculiarity Profiles.” Journal of Risk and Insurance 90 (2): 421–58.\n\n\nLemaire, Jean. 1985. Automobile Insurance: Actuarial Models. Boston: Kluwer.\n\n\nWu, Cheng-Sheng Peter, and JC Guszcza. 2003. “Does Credit Score Really Explain Insurance Losses? Multivariate Analysis from a Data Mining Point of View.” In Proceedings of the Casualty Actuarial Society, 113–38."
  },
  {
    "objectID": "VarTraditionelles.html#preamble",
    "href": "VarTraditionelles.html#preamble",
    "title": "3  Traditional Covariates",
    "section": "3.1 Preamble",
    "text": "3.1 Preamble\n\nChapter ObjectivePackagesData\n\n\nUsing only traditional covariates, the objective of this chapter is to propose various statistical models for estimation and variable selection to predict the number of claims. More specifically, the following model families will be examined:\n\nBasic GLM,\n\nGLM family, including elastic-net,\n\nXGBoost.\n\nAs mentioned in the theory review chapter, to compare models and strike a balance between bias and variance while avoiding overfitting, an interesting approach is to assess the prediction quality of models when applied to new data. The following R script presents a function for calculating various scores:\n\n\nCode\nScore.pred &lt;- function(mu, x) {\n  Sc.log  &lt;- -sum(dpois(x, mu, log=TRUE))\n  Sc.MSE  &lt;- sum((x - mu)^2)\n  Sc.quad &lt;- sum(-2*dpois(x,lambda=mu) + sapply(mu, function(x){ sum(dpois(0:10,lambda=x)^2) }))\n  Sc.sph &lt;- sum(- dpois(x,mu) / sqrt(sapply(mu, function(x){ sum(dpois(0:10,lambda=x)^2) })))\n  Sc.DSS &lt;- sum(dss_pois(x, mu))\n  Sc.CRPS &lt;- sum(crps_pois(x, mu))\n    \n  return(c(Sc.log, Sc.MSE, Sc.quad, Sc.sph, Sc.DSS, Sc.CRPS))\n}\n\n\n\n\nHere is the list of packages that will be used:\n\n\nCode\nlibrary(tidyverse)\nlibrary(vtable)\nlibrary(rpart)\nlibrary(repr)\nlibrary(rpart.plot)\nlibrary(rfCountData)\nlibrary(gam)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(glmnet)\nlibrary(scoringRules)\nlibrary(sjPlot)\n\n\n\n\nThe analyses in this chapter will be conducted using the same data as in the previous chapter. However, as we concluded at the end of our overview of the data, a transformation of certain variables is also necessary.\n\n\nCode\ndataS &lt;- read.csv('Data/Synthetic.csv')\n\n# Modifications \ndataS &lt;- dataS %&gt;%\n  mutate(Territory = as.factor(Territory)) %&gt;%\n  select(-c('Annual.pct.driven', 'Annual.miles.drive'))\ndata.select &lt;- dataS\n\n# Train-test \nset.seed(123)\ntrain &lt;- data.select %&gt;% sample_frac(0.8, replace = FALSE)\ntest &lt;- data.select %&gt;% anti_join(train)\n\n# Modif data\ntrain2 &lt;- train %&gt;%\n  mutate(Miles.per.day = Total.miles.driven/Duration,\n         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),\n         expo = Duration/365.25)\n\ntransform.fct &lt;- function(var){\n  df &lt;- train2 %&gt;% mutate(var_ = get(var)*Total.miles.driven/(1000*Duration))\n  q99 &lt;- quantile(df$var_, 0.99)\n  df &lt;- df %&gt;% mutate(var_ = ifelse(var_ &gt; q99, q99, var_))\n  #colnames(df)[ncol(df)] &lt;- paste0(var, '_')\n  return(df)\n}\n\ntrain2 &lt;- transform.fct(\"Brake.06miles\")\ntrain2 &lt;- transform.fct(\"Brake.08miles\")\ntrain2 &lt;- transform.fct(\"Brake.09miles\")\ntrain2 &lt;- transform.fct(\"Brake.11miles\")\ntrain2 &lt;- transform.fct(\"Brake.14miles\")\ntrain2 &lt;- transform.fct(\"Accel.06miles\")\ntrain2 &lt;- transform.fct(\"Accel.08miles\")\ntrain2 &lt;- transform.fct(\"Accel.09miles\")\ntrain2 &lt;- transform.fct(\"Accel.11miles\")\ntrain2 &lt;- transform.fct(\"Accel.12miles\")\ntrain2 &lt;- transform.fct(\"Accel.14miles\")\ntrain2 &lt;- transform.fct(\"Left.turn.intensity08\")\ntrain2 &lt;- transform.fct(\"Left.turn.intensity09\")\ntrain2 &lt;- transform.fct(\"Left.turn.intensity10\")\ntrain2 &lt;- transform.fct(\"Left.turn.intensity11\")\ntrain2 &lt;- transform.fct(\"Left.turn.intensity12\")\ntrain2 &lt;- transform.fct(\"Right.turn.intensity08\")\ntrain2 &lt;- transform.fct(\"Right.turn.intensity09\")\ntrain2 &lt;- transform.fct(\"Right.turn.intensity10\")\ntrain2 &lt;- transform.fct(\"Right.turn.intensity11\")\ntrain2 &lt;- transform.fct(\"Right.turn.intensity12\")\n\n# Create folds\nnb.fold &lt;- 5\nfold &lt;- sample(1:nb.fold, nrow(train2), replace = TRUE)\ntrain2$fold &lt;- fold\n\n##\n\ntest2 &lt;- test %&gt;%\n  mutate(Miles.per.day = Total.miles.driven/Duration,\n         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),\n         expo = Duration/365.25)\n\ntransform.fct &lt;- function(var){\n  df &lt;- test2 %&gt;% mutate(var_ = get(var)*Total.miles.driven/(1000*Duration))\n  q99 &lt;- quantile(df$var_, 0.99)\n  df &lt;- df %&gt;% mutate(var_ = ifelse(var_ &gt; q99, q99, var_))\n  #colnames(df)[ncol(df)] &lt;- paste0(var, '_')\n  return(df)\n}\n\ntest2 &lt;- transform.fct(\"Brake.06miles\")\ntest2 &lt;- transform.fct(\"Brake.08miles\")\ntest2 &lt;- transform.fct(\"Brake.09miles\")\ntest2 &lt;- transform.fct(\"Brake.11miles\")\ntest2 &lt;- transform.fct(\"Brake.14miles\")\ntest2 &lt;- transform.fct(\"Accel.06miles\")\ntest2 &lt;- transform.fct(\"Accel.08miles\")\ntest2 &lt;- transform.fct(\"Accel.09miles\")\ntest2 &lt;- transform.fct(\"Accel.11miles\")\ntest2 &lt;- transform.fct(\"Accel.12miles\")\ntest2 &lt;- transform.fct(\"Accel.14miles\")\ntest2 &lt;- transform.fct(\"Left.turn.intensity08\")\ntest2 &lt;- transform.fct(\"Left.turn.intensity09\")\ntest2 &lt;- transform.fct(\"Left.turn.intensity10\")\ntest2 &lt;- transform.fct(\"Left.turn.intensity11\")\ntest2 &lt;- transform.fct(\"Left.turn.intensity12\")\ntest2 &lt;- transform.fct(\"Right.turn.intensity08\")\ntest2 &lt;- transform.fct(\"Right.turn.intensity09\")\ntest2 &lt;- transform.fct(\"Right.turn.intensity10\")\ntest2 &lt;- transform.fct(\"Right.turn.intensity11\")\ntest2 &lt;- transform.fct(\"Right.turn.intensity12\")"
  },
  {
    "objectID": "VarTraditionelles.html#basic-glm-models",
    "href": "VarTraditionelles.html#basic-glm-models",
    "title": "3  Traditional Covariates",
    "section": "3.2 Basic GLM Models",
    "text": "3.2 Basic GLM Models\n\nSingle interceptCategorical covariatesEstimated Parameters\n\n\nA baseline model corresponding to a Generalized Linear Model (GLM) with intercept and predicting for each contract only the observed mean multiplied by the exposure is used as a point of comparison.\n\n\nCode\n## Model on each fold\nResult_  &lt;- data.frame()\nResult2_  &lt;- data.frame()\nfor(i in 1:nb.fold) {\n    learn &lt;- train2[train2$fold != i,]\n    valid &lt;- train2[train2$fold == i,]\n\n    mean &lt;- sum(learn$NB_Claim)/sum(learn$expo) \n    learn$pred.base &lt;- mean*learn$expo\n    valid$pred.base &lt;- mean*valid$expo\n\n    Result_ &lt;- rbind(Result_, c(i, Score.pred(valid$pred.base, valid$NB_Claim)/nrow(valid)))\n    Result2_ &lt;- rbind(Result2_, c(i, Score.pred(valid$pred.base, valid$NB_Claim)))\n}\n\n## Show results\ncolnames(Result_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ncolnames(Result2_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ntot &lt;- colSums(Result2_)/nrow(train2)\ntot$Fold &lt;- 'Total'\nResult_ &lt;- rbind(Result_ , tot)\n\nResult.base &lt;- Result_  \nBase &lt;- Result.base[nb.fold+1,]\n\nknitr::kable(Result_, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n\nTable 3.1: Prediction scores for the base model\n\n\nFold\nSc.log\nSc.MSE\nSc.quad\nSc.sph\nSc.DSS\nSc.CRPS\n\n\n\n\n1\n0.18212\n0.04666\n-0.91794\n-0.95797\n-2.16359\n0.04283\n\n\n2\n0.18185\n0.04681\n-0.91869\n-0.95838\n-2.14758\n0.04263\n\n\n3\n0.17509\n0.04555\n-0.92361\n-0.96096\n-2.17549\n0.04057\n\n\n4\n0.19451\n0.05149\n-0.91176\n-0.95472\n-2.06650\n0.04650\n\n\n5\n0.18543\n0.04832\n-0.91673\n-0.95733\n-2.13131\n0.04382\n\n\nTotal\n0.18379\n0.04777\n-0.91775\n-0.95787\n-2.13689\n0.04327\n\n\n\n\n\n\n\n\nThe model is then estimated on the entire training set and predicted on the test set, which was not used in parameter calibration.\n\n\nCode\nmean &lt;- sum(train2$NB_Claim)/sum(train2$expo) \ntest2$pred.base &lt;- mean*test2$expo\n\nResult_ &lt;- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ &lt;- cbind('Base', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all &lt;- Result_\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n\nTable 3.2: Prediction scores for the base model (testing set)\n\n\nModel\nSc.log\nSc.MSE\nSc.quad\nSc.sph\nSc.DSS\nSc.CRPS\n\n\n\n\nBase\n0.17674\n0.04545\n-0.92147\n-0.95981\n-2.19876\n0.04127\n\n\n\n\n\n\n\n\n\n\nA first regression approach is attempted using only the traditional categorical variables, namely:\n\nInsured’s gender,\n\nMarital status,\n\nVehicle usage,\n\nRegion.\n\nEven though territory should also be considered since it consists of more than fifty different factors, it will not be integrated into the model immediately. As we saw in the overview of variables in a previous section, the insured’s gender did not appear to be an important variable for predicting the number of claims. This GLM approach confirms this observation. Therefore, this variable is excluded from the model. In the table below, we can see the impact of adding traditional variables on the prediction quality.\nBelow are the prediction scores of the model with all categorical covariates. As expected, the addition of segmentation variables improves the prediction scores compared to the simple baseline model with only an intercept.\n\n\nCode\n## Model \nscore.base &lt;- as.formula(NB_Claim ~ 1 + offset(log(expo)))\nscore.glm &lt;- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo)))\n\n## Model on each fold\nResult_  &lt;- data.frame()\nResult2_  &lt;- data.frame()\nfor(i in 1:nb.fold) {\n    learn &lt;- train2[train2$fold != i,]\n    valid &lt;- train2[train2$fold == i,]\n    glm.fit &lt;- glm(score.glm, family = poisson(), data = learn)\n\n    learn$pred.base &lt;- predict(glm.fit, newdata=learn, type='response')\n    valid$pred.base &lt;- predict(glm.fit, newdata=valid, type='response')\n\n    Result_ &lt;- rbind(Result_, c(i, Score.pred(valid$pred.base, valid$NB_Claim)/nrow(valid)))\n    Result2_ &lt;- rbind(Result2_, c(i, Score.pred(valid$pred.base, valid$NB_Claim)))\n}\n\n## Model on all data from train\nglm.base &lt;- glm(score.base, family = poisson(), data = train2)\nglm.fit &lt;- glm(score.glm, family = poisson(), data = train2)\ntrain2$pred.glm1 &lt;- predict(glm.fit, newdata=train2, type='response')\nResult.glm1 &lt;- Result_  \n\n## Show results\ncolnames(Result_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ncolnames(Result2_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ntot &lt;- colSums(Result2_)/nrow(train2)\ntot$Fold &lt;- 'Total'\nResult_ &lt;- rbind(Result_ , tot)\nResult_ &lt;- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] &lt;- 'Improvement'\n\nfor(i in 2:7){\n  Result_[nb.fold+2,i] &lt;-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) &lt;- NULL\nknitr::kable(Result_, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n\nTable 3.3: Prediction scores for the GLM1 model\n\n\nFold\nSc.log\nSc.MSE\nSc.quad\nSc.sph\nSc.DSS\nSc.CRPS\n\n\n\n\n1\n0.18098\n0.04656\n-0.91814\n-0.95803\n-2.19092\n0.04272\n\n\n2\n0.18118\n0.04673\n-0.91879\n-0.95841\n-2.16470\n0.04257\n\n\n3\n0.17414\n0.04549\n-0.92368\n-0.96097\n-2.21238\n0.04052\n\n\n4\n0.19332\n0.05137\n-0.91193\n-0.95477\n-2.10039\n0.04640\n\n\n5\n0.18495\n0.04829\n-0.91679\n-0.95734\n-2.15256\n0.04379\n\n\nTotal\n0.18291\n0.04769\n-0.91787\n-0.95791\n-2.16420\n0.04320\n\n\nImprovement\n-0.00088\n-0.00008\n-0.00012\n-0.00003\n-0.02731\n-0.00007\n\n\n\n\n\n\n\n\nThe comparison with the test dataset is also indicated in the table below. It shows that adding traditional variables does not substantially enhance prediction on the test dataset.\n\n\nCode\nscore.glm &lt;- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo)))\n\nglm.fit &lt;- glm(score.glm, family = poisson(), data = train2)\ntest2$pred.base &lt;- predict(glm.fit, newdata=test2, type='response')\n\nResult_ &lt;- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ &lt;- cbind('GLM (trad.)', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all &lt;- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n\nTable 3.4: Prediction scores for the GLM model with traditional covariates (testing set)\n\n\nModel\nSc.log\nSc.MSE\nSc.quad\nSc.sph\nSc.DSS\nSc.CRPS\n\n\n\n\nBase\n0.17674\n0.04545\n-0.92147\n-0.95981\n-2.19876\n0.04127\n\n\nGLM (trad.)\n0.17587\n0.04538\n-0.92157\n-0.95984\n-2.22852\n0.04121\n\n\n\n\n\n\n\n\n\n\nThe table below shows the estimators obtained for the GLM-Poisson approach and compares them with the baseline model having only an intercept.\n\n\nCode\n## Model \nscore.base &lt;- as.formula(NB_Claim ~ 1 + offset(log(expo)))\nscore.glm &lt;- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo)))\n\n## Model on all data from train\nglm.base &lt;- glm(score.base, family = poisson(), data = train2)\nglm.fit &lt;- glm(score.glm, family = poisson(), data = train2)\n\ntab_model(glm.base, glm.fit, transform = NULL)\n\n\n\n\nTable 3.5: Estimated parameters for the GLM1 model\n\n\n \nNB Claim\nNB Claim\n\n\nPredictors\nLog-Mean\nCI\np\nLog-Mean\nCI\np\n\n\n(Intercept)\n-2.94\n-2.98 – -2.91\n&lt;0.001\n-2.66\n-2.84 – -2.49\n&lt;0.001\n\n\nInsured sex [Male]\n\n\n\n-0.02\n-0.08 – 0.05\n0.626\n\n\nMarital [Single]\n\n\n\n0.12\n0.05 – 0.19\n0.001\n\n\nCar use [Commute]\n\n\n\n-0.35\n-0.50 – -0.18\n&lt;0.001\n\n\nCar use [Farmer]\n\n\n\n-1.10\n-1.62 – -0.64\n&lt;0.001\n\n\nCar use [Private]\n\n\n\n-0.65\n-0.81 – -0.48\n&lt;0.001\n\n\nRegion [Urban]\n\n\n\n0.18\n0.10 – 0.27\n&lt;0.001\n\n\nObservations\n80000\n80000\n\n\nR2 Nagelkerke\n0.000\n0.008"
  },
  {
    "objectID": "VarTraditionelles.html#glm-net",
    "href": "VarTraditionelles.html#glm-net",
    "title": "3  Traditional Covariates",
    "section": "3.3 GLM-Net",
    "text": "3.3 GLM-Net\nAs we saw in the previous chapter, a series of traditional continuous segmentation variables is also available:\n\nCredit score,\n\nAge of the insured,\n\nAge of the vehicle,\n\nNumber of claim-free years,\nFurthermore, as we will explain later, the territory will also be treated as a continuous variable.\n\nDirectly using a continuous variable in a GLM is usually ineffective as it assumes a linear relationship. To avoid overfitting the data, an approach using splines, utilizing the Generalized Additive Models (GAM) theory, is interesting. This approach allows visualizing the general form of the covariate to explain the number of claims. A parametric form can then be proposed to achieve the best possible correspondence with the spline obtained by the GAM.\nSubsequently, instead of attempting to fit a basic GLM model with all variables, we will work with a GLM-net model that allows for variable selection.\n\n3.3.1 Parametric transformation of continuous covariates\n\nCredit ScoreAge of the insuredAge of the carYears without claimsTerritory\n\n\nThe first covariate studied is the credit score. We include all categorical variables in the analysis and apply a spline approach with a GAM. The spline analysis indicates that the following parametric form appears to be appropriate for capturing the relationship between the number of claims:\n\\[s(Credit.Score) \\approx Credit.Score + Credit.Score^2\\]\n\n\n\n\n\nFigure 3.1: Smoothing of the credit score\n\n\n\n\n\n\nA spline to examine the relationship between the age of the insured and the claim frequency has also been produced. The most appropriate parametric form is as follows:\n\\[s(Insured.age) \\approx Insured.age + \\log(Insured.age) + Insured.age^2\\]\n\n\n\n\n\nFigure 3.2: Smoothing of the age of the insured\n\n\n\n\n\n\nThe most appropriate parametric form is as follows:\n\\[s(Car.age) \\approx Car.age + Car.age^2\\]\n\n\n\n\n\nFigure 3.3: Smoothing of the age of the car\n\n\n\n\n\n\nFinally, the proposed parametric form for the covariate indicating the years without claim is:\n\\[s(Years.noclaims) \\approx Years.noclaims + Years.noclaims^2 + Years.noclaims^3\\]\n\n\n\n\n\nFigure 3.4: Smoothing of years without claim\n\n\n\n\n\n\nAs we saw in the previous chapter, the insured’s territory code corresponds to a categorical variable with a large cardinality. In such a situation, creating a binary variable for each possible territory is not appropriate. Instead, we propose using target encoding based on the territory’s rank.\nThis means that we first calculate the observed frequency for each territory. Then, we rank the frequencies for the 53 territories in the database. Next, the rank divided by 53 corresponds to the numerical value of the territory. This form is called rank-encoding. We believe that this transformation is justified, considering that ranking territories according to risk is an approach that could be taken in insurance companies.\nWith the encoded form of the territory, as has been done with the other continuous variables, we propose a parametric form for the spline obtained:\n\\[s(terr.code) \\approx terr.code + terr.code^2 + terr.code^3\\]\n\n\n\n\n\nFigure 3.5: Smoothing of the territories (encoded)\n\n\n\n\n\n\n\n\n\n3.3.2 Fitting the GLM-Net model\n\nOptimal valueParsimonious modelCategorical covariatesContinuous covariates\n\n\nThe parameters of the GLM-net were calibrated using cross-validation to obtain the model’s hyperparameters. Using these values, we can calculate the prediction scores of the model based on all covariates.\n\n\nCode\nglm.score &lt;- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo))\n                                  + Credit.score +  I(Credit.score^2) \n                                  + Insured.age +  log(Insured.age) + I(Insured.age^2) \n                                  + Car.age + I(Car.age^2) \n                                  + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nResult_  &lt;- data.frame()\nResult2_  &lt;- data.frame()\nfor(i in 1:nb.fold) {\n    learn &lt;- train2[train2$fold != i,]\n    valid &lt;- train2[train2$fold == i,]\n    \n    matrix.x &lt;- model.matrix(glm.score, data=learn)[,-1]\n    y &lt;- learn$NB_Claim\n    offset &lt;- log(learn$expo)\n\n    lambda.min &lt;- 0\n    lambda.1se &lt;- 0.006309573\n    \n    lambda.select &lt;- lambda.min\n    fit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\n    #fit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n  \n    matrix.x &lt;- model.matrix(glm.score, data=valid)[,-1]\n    y &lt;- valid$NB_Claim\n    offset &lt;- log(valid$expo)\n\n    valid$pred &lt;- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)\n    \n    Result_ &lt;- rbind(Result_, c(i, Score.pred(valid$pred, valid$NB_Claim)/nrow(valid)))\n    Result2_ &lt;- rbind(Result2_, c(i, Score.pred(valid$pred, valid$NB_Claim)))\n}\n\n\n## Show results\ncolnames(Result_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ncolnames(Result2_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ntot &lt;- colSums(Result2_)/nrow(train2)\ntot$Fold &lt;- 'Total'\nResult_ &lt;- rbind(Result_ , tot)\nResult_ &lt;- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] &lt;- 'Improvement'\n\nfor(i in 2:7){\n  Result_[nb.fold+2,i] &lt;-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) &lt;- NULL\nknitr::kable(Result_, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n\nTable 3.6: Prediction scores for the GLM-net model (alpha=1)\n\n\nFold\nSc.log\nSc.MSE\nSc.quad\nSc.sph\nSc.DSS\nSc.CRPS\n\n\n\n\n1\n0.17641\n0.04602\n-0.91898\n-0.95827\n-2.31270\n0.04227\n\n\n2\n0.17750\n0.04627\n-0.91946\n-0.95859\n-2.19831\n0.04220\n\n\n3\n0.16929\n0.04499\n-0.92431\n-0.96113\n-2.36647\n0.04015\n\n\n4\n0.18745\n0.05067\n-0.91299\n-0.95506\n-2.23560\n0.04582\n\n\n5\n0.18152\n0.04789\n-0.91728\n-0.95747\n-2.25605\n0.04350\n\n\nTotal\n0.17843\n0.04717\n-0.91861\n-0.95810\n-2.27389\n0.04278\n\n\nImprovement\n-0.00536\n-0.00060\n-0.00086\n-0.00023\n-0.13700\n-0.00048\n\n\n\n\n\n\n\n\nOn the test dataset, we obtain:\n\n\nCode\nglm.score &lt;- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo))\n                                  + Credit.score +  I(Credit.score^2) \n                                  + Insured.age +  log(Insured.age) + I(Insured.age^2) \n                                  + Car.age + I(Car.age^2) \n                                  + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x &lt;- model.matrix(glm.score, data=train2)[,-1]\ny &lt;- train2$NB_Claim\noffset &lt;- log(train2$expo)\n\nlambda.min &lt;- 0\nlambda.1se &lt;- 0.006309573\n    \nlambda.select &lt;- lambda.min\nfit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\n#fit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n  \nmatrix.x &lt;- model.matrix(glm.score, data=test2)[,-1]\ny &lt;- test2$NB_Claim\noffset &lt;- log(test2$expo)\n\ntest2$pred.base &lt;- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)\n\nResult_ &lt;- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ &lt;- cbind('LASSO (optimal)', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all &lt;- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n\nTable 3.7: Prediction scores for the GLM-net model (testing set)\n\n\nModel\nSc.log\nSc.MSE\nSc.quad\nSc.sph\nSc.DSS\nSc.CRPS\n\n\n\n\nBase\n0.17674\n0.04545\n-0.92147\n-0.95981\n-2.19876\n0.04127\n\n\nGLM (trad.)\n0.17587\n0.04538\n-0.92157\n-0.95984\n-2.22852\n0.04121\n\n\nLASSO (optimal)\n0.17169\n0.04492\n-0.92229\n-0.96004\n-2.34060\n0.04081\n\n\n\n\n\n\n\n\n\n\nInstead of using the optimal value of the penalty \\(\\lambda\\) in the elastic-net approach, it is often advised to use a penalty value located at one standard error (\\(\\lambda_{1se}\\)). This helps to obtain a more parsimonious model. The prediction scores of such a model are displayed below.\n\n\nCode\nResult_  &lt;- data.frame()\nResult2_  &lt;- data.frame()\nfor(i in 1:nb.fold) {\n    learn &lt;- train2[train2$fold != i,]\n    valid &lt;- train2[train2$fold == i,]\n    \n    matrix.x &lt;- model.matrix(glm.score, data=learn)[,-1]\n    y &lt;- learn$NB_Claim\n    offset &lt;- log(learn$expo)\n\n    lambda.min &lt;- 0\n    lambda.1se &lt;- 0.006309573\n    \n    lambda.select &lt;- lambda.1se\n    #fit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\n    fit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n  \n    matrix.x &lt;- model.matrix(glm.score, data=valid)[,-1]\n    y &lt;- valid$NB_Claim\n    offset &lt;- log(valid$expo)\n\n    valid$pred &lt;- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)\n    \n    Result_ &lt;- rbind(Result_, c(i, Score.pred(valid$pred, valid$NB_Claim)/nrow(valid)))\n    Result2_ &lt;- rbind(Result2_, c(i, Score.pred(valid$pred, valid$NB_Claim)))\n}\n\n\n## Show results\ncolnames(Result_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ncolnames(Result2_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ntot &lt;- colSums(Result2_)/nrow(train2)\ntot$Fold &lt;- 'Total'\nResult_ &lt;- rbind(Result_ , tot)\nResult_ &lt;- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] &lt;- 'Improvement'\n\nfor(i in 2:7){\n  Result_[nb.fold+2,i] &lt;-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) &lt;- NULL\nknitr::kable(Result_, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n\nTable 3.8: Prediction scores for the GLM-net model (alpha=1)\n\n\nFold\nSc.log\nSc.MSE\nSc.quad\nSc.sph\nSc.DSS\nSc.CRPS\n\n\n\n\n1\n0.17859\n0.04629\n-0.91854\n-0.95814\n-2.25068\n0.04251\n\n\n2\n0.17859\n0.04646\n-0.91924\n-0.95853\n-2.22599\n0.04233\n\n\n3\n0.17172\n0.04521\n-0.92411\n-0.96109\n-2.26708\n0.04029\n\n\n4\n0.19035\n0.05105\n-0.91244\n-0.95491\n-2.16923\n0.04612\n\n\n5\n0.18272\n0.04804\n-0.91713\n-0.95743\n-2.20639\n0.04360\n\n\nTotal\n0.18039\n0.04741\n-0.91830\n-0.95802\n-2.22387\n0.04297\n\n\nImprovement\n-0.00340\n-0.00035\n-0.00055\n-0.00015\n-0.08698\n-0.00030\n\n\n\n\n\n\n\n\nOn the test dataset, we obtain:\n\n\nCode\nglm.score &lt;- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo))\n                                  + Credit.score +  I(Credit.score^2) \n                                  + Insured.age +  log(Insured.age) + I(Insured.age^2) \n                                  + Car.age + I(Car.age^2) \n                                  + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x &lt;- model.matrix(glm.score, data=train2)[,-1]\ny &lt;- train2$NB_Claim\noffset &lt;- log(train2$expo)\n\nlambda.min &lt;- 0\nlambda.1se &lt;- 0.006309573\n    \nlambda.select &lt;- lambda.1se\n#fit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\nfit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n  \nmatrix.x &lt;- model.matrix(glm.score, data=test2)[,-1]\ny &lt;- test2$NB_Claim\noffset &lt;- log(test2$expo)\n\ntest2$pred.base &lt;- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)\n\nResult_ &lt;- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ &lt;- cbind('LASSO (parsimonious)', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all &lt;- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n\nTable 3.9: Prediction scores for the GLM-net model (testing set)\n\n\nModel\nSc.log\nSc.MSE\nSc.quad\nSc.sph\nSc.DSS\nSc.CRPS\n\n\n\n\nBase\n0.17674\n0.04545\n-0.92147\n-0.95981\n-2.19876\n0.04127\n\n\nGLM (trad.)\n0.17587\n0.04538\n-0.92157\n-0.95984\n-2.22852\n0.04121\n\n\nLASSO (optimal)\n0.17169\n0.04492\n-0.92229\n-0.96004\n-2.34060\n0.04081\n\n\nLASSO (parsimonious)\n0.17332\n0.04511\n-0.92202\n-0.95997\n-2.28596\n0.04097\n\n\n\n\n\n\n\n\n\n\nFor categorical variables, the relativity values obtained for both GLM-net approaches are displayed below.\n\n\n\n\n\n\n\n(a) Sex of the insured\n\n\n\n\n\n\n\n(b) Marital status of the insured\n\n\n\n\n\n\n\n\n\n(c) Car use\n\n\n\n\n\n\n\n(d) Region\n\n\n\n\nFigure 3.6: Interpretation of the categorical variables from the GLM-net model\n\n\n\n\nAs with categorical variables, the relativities obtained are illustrated below for continuous variables. It can be observed that the parsimonious approach tends to reduce the impact of segmentation variables on the premium.\n\n\nCode\nglm.score &lt;- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo))\n                                  + Credit.score +  I(Credit.score^2) \n                                  + Insured.age +  log(Insured.age) + I(Insured.age^2) \n                                  + Car.age + I(Car.age^2) \n                                  + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x &lt;- model.matrix(glm.score, data=train2)[,-1]\ny &lt;- train2$NB_Claim\noffset &lt;- log(train2$expo)\n\nlambda.min &lt;- 0\nlambda.1se &lt;- 0.006309573\n\nlasso.min &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.min)\nlasso.1se &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.1se)\n#cbind(coef(lasso.min), coef(lasso.1se))\n\n### Credit Score ###\nCredit.score &lt;- seq(from=min(train2$Credit.score), to=max(train2$Credit.score), by=1)\n\nbeta &lt;- coef(lasso.1se)[8:9]\ncurve1 &lt;- exp(beta[1]*Credit.score + beta[2]*Credit.score^2) \nbase1 &lt;- exp(beta[1]*mean(train2$Credit.score) + beta[2]*mean(train2$Credit.score)^2) \n\nbeta &lt;- coef(lasso.min)[8:9]\ncurve2 &lt;- exp(beta[1]*Credit.score + beta[2]*Credit.score^2) \nbase2 &lt;- exp(beta[1]*mean(train2$Credit.score) + beta[2]*mean(train2$Credit.score)^2) \n\ncurve1 &lt;- curve1/base1\ncurve2 &lt;- curve2/base2\ndb &lt;- data.frame(cbind(Credit.score, curve1, curve2))\n\nggplot()+\n  geom_line(aes(x=Credit.score, y=curve1, color = 'lambda.1se' ), data=db)+\n  geom_line(aes(x=Credit.score, y=curve2, color = 'lambda.min' ), data=db)+\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Credit Score',\n       y = 'Relativity') +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Insured.age \nInsured.age &lt;- seq(from=min(train2$Insured.age ), to=max(train2$Insured.age ), by=1)\nbeta &lt;- coef(lasso.1se)[10:12]\ncurve1 &lt;- exp(beta[1]*Insured.age  + beta[2]*log(Insured.age ) + beta[3]*Insured.age^2)       \nbase1  &lt;- exp(beta[1]*mean(train2$Insured.age) + beta[2]*log(mean(train2$Insured.age)) + beta[3]*mean(train2$Insured.age)^2) \n\nbeta &lt;- coef(lasso.min)[10:12]\ncurve2 &lt;- exp(beta[1]*Insured.age  + beta[2]*log(Insured.age ) + beta[3]*Insured.age^2)       \nbase2  &lt;- exp(beta[1]*mean(train2$Insured.age) + beta[2]*log(mean(train2$Insured.age)) + beta[3]*mean(train2$Insured.age)^2) \n\ncurve1 &lt;- curve1/base1\ncurve2 &lt;- curve2/base2\ndb &lt;- data.frame(cbind(Insured.age, curve1, curve2))\n\nggplot()+\n  geom_line(aes(x=Insured.age, y=curve1, color = 'lambda.1se' ), data=db)+\n  geom_line(aes(x=Insured.age, y=curve2, color = 'lambda.min' ), data=db)+\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Age of the insured',\n       y = 'Relativity') +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Car Age ###\nCar.age &lt;- seq(from=min(train2$Car.age), to=max(train2$Car.age), by=1)\nbeta &lt;- coef(lasso.1se)[13:14]\ncurve1 &lt;- exp(beta[1]*Car.age + beta[2]*Car.age^2)\nbase1  &lt;- exp(beta[1]*mean(train2$Car.age) + beta[2]*mean(train2$Car.age)^2) \n\nbeta &lt;- coef(lasso.min)[13:14]\ncurve2 &lt;- exp(beta[1]*Car.age + beta[2]*Car.age^2)\nbase2  &lt;- exp(beta[1]*mean(train2$Car.age) + beta[2]*mean(train2$Car.age)^2) \n\ncurve1 &lt;- curve1/base1\ncurve2 &lt;- curve2/base2\ndb &lt;- data.frame(cbind(Car.age, curve1, curve2))\n\nggplot()+\n  geom_line(aes(x=Car.age, y=curve1, color = 'lambda.1se' ), data=db)+\n  geom_line(aes(x=Car.age, y=curve2, color = 'lambda.min' ), data=db)+\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Age of the car',\n       y = 'Relativity') +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Years.noclaims \nYears.noclaims &lt;- seq(from=min(train2$Years.noclaims ), to=max(train2$Years.noclaims ), by=1)\nbeta &lt;- coef(lasso.1se)[15:17]\ncurve1 &lt;- exp(beta[1]*Years.noclaims  + beta[2]*Years.noclaims^2 + beta[3]*Years.noclaims ^3)        \nbase1  &lt;- exp(beta[1]*mean(train2$Years.noclaims) + beta[2]*mean(train2$Years.noclaims)^2 + beta[3]*mean(train2$Years.noclaims)^3) \n\nbeta &lt;- coef(lasso.min)[15:17]\ncurve2 &lt;- exp(beta[1]*Years.noclaims  + beta[2]*Years.noclaims^2 + beta[3]*Years.noclaims ^3)        \nbase2  &lt;- exp(beta[1]*mean(train2$Years.noclaims) + beta[2]*mean(train2$Years.noclaims)^2 + beta[3]*mean(train2$Years.noclaims)^3) \n\ncurve1 &lt;- curve1/base1\ncurve2 &lt;- curve2/base2\ndb &lt;- data.frame(cbind(Years.noclaims, curve1, curve2))\n\nggplot()+\n  geom_line(aes(x=Years.noclaims, y=curve1, color = 'lambda.1se' ), data=db)+\n  geom_line(aes(x=Years.noclaims, y=curve2, color = 'lambda.min' ), data=db)+\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Years without claim',\n       y = 'Relativity') +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### terr.code  \nterr.code  &lt;- seq(from=min(train2$terr.code  ), to=max(train2$terr.code  ), by=0.01)\nbeta &lt;- coef(lasso.1se)[18:20]\ncurve1 &lt;- exp(beta[1]*terr.code + beta[2]*terr.code^2 + beta[3]*terr.code^3)\nbase1  &lt;- exp(beta[1]*mean(train2$terr.code) + beta[2]*mean(train2$terr.code)^2 + beta[3]*mean(train2$terr.code)^3)\n\nbeta &lt;- coef(lasso.min)[18:20]\ncurve2 &lt;- exp(beta[1]*terr.code + beta[2]*terr.code^2 + beta[3]*terr.code^3)\nbase2  &lt;- exp(beta[1]*mean(train2$terr.code) + beta[2]*mean(train2$terr.code)^2 + beta[3]*mean(train2$terr.code)^3)\n\ncurve1 &lt;- curve1/base1\ncurve2 &lt;- curve2/base2\ndb &lt;- data.frame(cbind(terr.code, curve1, curve2))\n\nggplot()+\n  geom_line(aes(x=terr.code, y=curve1, color = 'lambda.1se' ), data=db)+\n  geom_line(aes(x=terr.code, y=curve2, color = 'lambda.min' ), data=db)+\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Territory (encoded)',\n       y = 'Relativity') +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n\n\n\n\n\n(a) Credit Score\n\n\n\n\n\n\n\n(b) Age of the insured\n\n\n\n\n\n\n\n\n\n(c) Age of the car\n\n\n\n\n\n\n\n(d) Years without claim\n\n\n\n\n\n\n\n\n\n(e) Territory Code\n\n\n\n\nFigure 3.7: Interpretation of the continuous variables from the GLM-net model:"
  },
  {
    "objectID": "VarTraditionelles.html#xgboost",
    "href": "VarTraditionelles.html#xgboost",
    "title": "3  Traditional Covariates",
    "section": "3.4 XGBoost",
    "text": "3.4 XGBoost\nAnother approach to consider is XGBoost. However, through cross-validation, this method requires finely tuning its hyperparameters to be effective. We utilized a grid search approach coupled with Bayesian optimization for the dataset used in the project.\n\nPrediction ScoresVariables Importance\n\n\nWith the hyperparameters we discovered, we can compute the model’s prediction scores. The scores obtained show a significant improvement compared to other tested approaches.\n\n\nCode\nlibrary(xgboost)\nlibrary(Ckmeans.1d.dp)\nlibrary(SHAPforxgboost)\n\ntrad.vars &lt;- c(\"Marital\", \"Car.use\", \"Region\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Car.age\", \"Years.noclaims\", \"Territory\") \n\ndtrain &lt;- xgb.DMatrix(data = data.matrix(train2[, paste(trad.vars)]), label = train2$NB_Claim)\nsetinfo(dtrain,\"base_margin\",log(train2$expo))\nfolds &lt;-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n\n\n\n\nCode\nparam &lt;- list(\n  eta = 0.08846194,\n  max_depth = 43,\n  subsample = 0.8389601,\n  min_child_weight = 1,\n  booster = \"gbtree\",\n  objective = \"count:poisson\",\n  eval_metric = \"poisson-nloglik\")\n\nset.seed(333)\nxgbcv &lt;- xgb.cv(params = param,\n                nrounds = 96,\n                data = dtrain,\n                folds = folds,\n                prediction = TRUE,\n                early_stopping_rounds = 10,\n                verbose = 0,\n                maximize = F)\n  \nSc.log &lt;- sapply(xgbcv$folds, function(x){-dpois(train2$NB_Claim[x], unlist(xgbcv$pred[x]), log=TRUE)})\nSc.MSE &lt;- sapply(xgbcv$folds, function(x){(train2$NB_Claim[x]-unlist(xgbcv$pred[x]))^2})\nSc.quad &lt;- sapply(xgbcv$folds, function(x){\n  nb &lt;- train2$NB_Claim[x]\n  mu &lt;- unlist(xgbcv$pred[x])\n  -2*dpois(nb,lambda=mu) + dpois(0,lambda=mu)^2 + dpois(1,lambda=mu)^2 + dpois(2,lambda=mu)^2+ dpois(3,lambda=mu)^2 + dpois(4,lambda=mu)^2 + dpois(5,lambda=mu)^2 \n})  \nSc.sph &lt;- sapply(xgbcv$folds, function(x){\n  nb &lt;- train2$NB_Claim[x]\n  mu &lt;- unlist(xgbcv$pred[x])\n  -dpois(nb,lambda=mu) / sqrt(dpois(0,lambda=mu)^2 + dpois(1,lambda=mu)^2 + dpois(2,lambda=mu)^2+ dpois(3,lambda=mu)^2 + dpois(4,lambda=mu)^2 + dpois(5,lambda=mu)^2 ) \n})\nSc.DSS &lt;- sapply(xgbcv$folds, function(x){dss_pois(train2$NB_Claim[x], unlist(xgbcv$pred[x]))})\nSc.CRPS &lt;- sapply(xgbcv$folds, function(x){crps_pois(train2$NB_Claim[x], unlist(xgbcv$pred[x]))})\n\nResult_  &lt;- rbind(\nc(1,mean(Sc.log[1]$fold1), mean(Sc.MSE[1]$fold1), mean(Sc.quad[1]$fold1), mean(Sc.sph[1]$fold1), mean(Sc.DSS[1]$fold1), mean(Sc.CRPS[1]$fold1)),\nc(2,mean(Sc.log[2]$fold2), mean(Sc.MSE[2]$fold2), mean(Sc.quad[2]$fold2), mean(Sc.sph[2]$fold2), mean(Sc.DSS[2]$fold2), mean(Sc.CRPS[2]$fold2)),\nc(3,mean(Sc.log[3]$fold3), mean(Sc.MSE[3]$fold3), mean(Sc.quad[3]$fold3), mean(Sc.sph[3]$fold3), mean(Sc.DSS[3]$fold3), mean(Sc.CRPS[3]$fold3)),\nc(4,mean(Sc.log[4]$fold4), mean(Sc.MSE[4]$fold4), mean(Sc.quad[4]$fold4), mean(Sc.sph[4]$fold4), mean(Sc.DSS[4]$fold4), mean(Sc.CRPS[4]$fold4)),\nc(5,mean(Sc.log[5]$fold5), mean(Sc.MSE[5]$fold5), mean(Sc.quad[5]$fold5), mean(Sc.sph[5]$fold5), mean(Sc.DSS[5]$fold5), mean(Sc.CRPS[5]$fold5))\n)\n\nRes.sum  &lt;- rbind(\nc(sum(Sc.log[1]$fold1), sum(Sc.MSE[1]$fold1), sum(Sc.quad[1]$fold1), sum(Sc.sph[1]$fold1), sum(Sc.DSS[1]$fold1), sum(Sc.CRPS[1]$fold1)),\nc(sum(Sc.log[2]$fold2), sum(Sc.MSE[2]$fold2), sum(Sc.quad[2]$fold2), sum(Sc.sph[2]$fold2), sum(Sc.DSS[2]$fold2), sum(Sc.CRPS[2]$fold2)),\nc(sum(Sc.log[3]$fold3), sum(Sc.MSE[3]$fold3), sum(Sc.quad[3]$fold3), sum(Sc.sph[3]$fold3), sum(Sc.DSS[3]$fold3), sum(Sc.CRPS[3]$fold3)),\nc(sum(Sc.log[4]$fold4), sum(Sc.MSE[4]$fold4), sum(Sc.quad[4]$fold4), sum(Sc.sph[4]$fold4), sum(Sc.DSS[4]$fold4), sum(Sc.CRPS[4]$fold4)),\nc(sum(Sc.log[5]$fold5), sum(Sc.MSE[5]$fold5), sum(Sc.quad[5]$fold5), sum(Sc.sph[5]$fold5), sum(Sc.DSS[5]$fold5), sum(Sc.CRPS[5]$fold5))\n)\nsum &lt;- c('Total', colSums(Res.sum)/nrow(train2))\n\nResult_  &lt;- data.frame(rbind(Result_, sum)) \n\n## Show results\ncolnames(Result_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\nResult_ &lt;- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] &lt;- 'Improvement'\n\nfor(i in 2:7){\n  Result_[,i] &lt;- as.numeric(Result_[,i])  \n  Result_[nb.fold+2,i] &lt;-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\nrownames(Result_) &lt;- NULL\nknitr::kable(Result_, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n\nTable 3.10: Prediction scores for the XGBoost model\n\n\nFold\nSc.log\nSc.MSE\nSc.quad\nSc.sph\nSc.DSS\nSc.CRPS\n\n\n\n\n1\n0.15641\n0.04057\n-0.92589\n-0.96093\n-2.62584\n0.03824\n\n\n2\n0.15288\n0.03912\n-0.92772\n-0.96181\n-2.65957\n0.03720\n\n\n3\n0.14901\n0.03948\n-0.93151\n-0.96388\n-2.64415\n0.03599\n\n\n4\n0.16611\n0.04377\n-0.92036\n-0.95790\n-2.55143\n0.04115\n\n\n5\n0.16121\n0.04232\n-0.92452\n-0.96026\n-2.54601\n0.03928\n\n\nTotal\n0.15712\n0.04105\n-0.92600\n-0.96096\n-2.60534\n0.03837\n\n\nImprovement\n-0.02667\n-0.00671\n-0.00825\n-0.00308\n-0.46845\n-0.00490\n\n\n\n\n\n\n\n\nWe can use the same model to compute scores on the test set. Furthermore, it’s evident that the XGBoost approach is the most effective.\n\n\nCode\ndtrain &lt;- xgb.DMatrix(data = data.matrix(train2[, paste(trad.vars)]), label = train2$NB_Claim)\nsetinfo(dtrain,\"base_margin\",log(train2$expo))\nfolds &lt;-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\ndtest &lt;- xgb.DMatrix(data = data.matrix(test2[, paste(trad.vars)]), label = test2$NB_Claim)\nsetinfo(dtest,\"base_margin\",log(test2$expo))\n\n\n\n\nCode\nparam &lt;- list(\n  eta = 0.08846194,\n  max_depth = 43,\n  subsample = 0.8389601,\n  min_child_weight = 1,\n  booster = \"gbtree\",\n  objective = \"count:poisson\",\n  eval_metric = \"poisson-nloglik\")\n\nset.seed(333)\nfit.xgb &lt;- xgb.train(params = param,\n                     nrounds = 96,\n                     data = dtrain)\n\ntrain2$pred.xgb &lt;- predict(fit.xgb, dtrain, type='response')\ntest2$pred.xgb &lt;- predict(fit.xgb, dtest, type='response')\n\ntest2$pred.base &lt;- test2$pred.xgb\nResult_ &lt;- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ &lt;- cbind('XGBoost', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all &lt;- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n\nTable 3.11: Prediction scores for the XGBoost model with traditional covariates\n\n\nModel\nSc.log\nSc.MSE\nSc.quad\nSc.sph\nSc.DSS\nSc.CRPS\n\n\n\n\nBase\n0.17674\n0.04545\n-0.92147\n-0.95981\n-2.19876\n0.04127\n\n\nGLM (trad.)\n0.17587\n0.04538\n-0.92157\n-0.95984\n-2.22852\n0.04121\n\n\nLASSO (optimal)\n0.17169\n0.04492\n-0.92229\n-0.96004\n-2.34060\n0.04081\n\n\nLASSO (parsimonious)\n0.17332\n0.04511\n-0.92202\n-0.95997\n-2.28596\n0.04097\n\n\nXGBoost\n0.14560\n0.03648\n-0.93145\n-0.96369\n-2.72777\n0.03509\n\n\n\n\n\n\n\n\n\n\nA challenge associated with the XGBoost approach is comprehending the full impact of each segmentation variable. The following graph depicts the most crucial variables in the XGBoost model. We observe that the credit score, the territory, the age of the insured, and the years without claim are the most significant covariates in the XGBoost model.\n\n\nCode\nparam &lt;- list(\n  eta = 0.08846194,\n  max_depth = 43,\n  subsample = 0.8389601,\n  min_child_weight = 1,\n  booster = \"gbtree\",\n  objective = \"count:poisson\",\n  eval_metric = \"poisson-nloglik\")\n\nset.seed(333)\nfit.xgb &lt;- xgb.train(params = param,\n                     nrounds = 96,\n                     data = dtrain)\n\nimportance_matrix &lt;- xgb.importance(dimnames(dtrain)[[2]], model = fit.xgb)\nxgb.ggplot.importance(importance_matrix,top_n=10) + theme(text = element_text(size=15))"
  },
  {
    "objectID": "VarTelematiques.html#preamble",
    "href": "VarTelematiques.html#preamble",
    "title": "4  Telematic Covariates",
    "section": "4.1 Preamble",
    "text": "4.1 Preamble\n\nChapter ObjectivePackagesDataData Transformation\n\n\nWe continue the analysis of claim frequency by adding telematic variables to the same three approaches of the last chapter. However, we will remove protected traditional variables from our analysis. Specifically, we will not use the following five covariates in our models:\n\nCredit.score,\n\nInsured.age,\n\nInsured.sex,\n\nMarital,\n\nTerritory.\n\nThe objective is to assess the performance of ratemaking approaches incorporating telematics information without protected covariates. By analyzing the residuals of the approach, we will gauge the relevance of those protected covariates.\nTo compare models, we will employ the following functions that calculate predictive scores:\n\n\nCode\nScore.pred &lt;- function(mu, x) {\n  Sc.log  &lt;- -sum(dpois(x, mu, log=TRUE))\n  Sc.MSE  &lt;- sum((x - mu)^2)\n  Sc.quad &lt;- sum(-2*dpois(x,lambda=mu) + sapply(mu, function(x){ sum(dpois(0:10,lambda=x)^2) }))\n  Sc.sph &lt;- sum(- dpois(x,mu) / sqrt(sapply(mu, function(x){ sum(dpois(0:10,lambda=x)^2) })))\n  Sc.DSS &lt;- sum(dss_pois(x, mu))\n  Sc.CRPS &lt;- sum(crps_pois(x, mu))\n    \n  return(c(Sc.log, Sc.MSE, Sc.quad, Sc.sph, Sc.DSS, Sc.CRPS))\n}\n\n\n\n\nHere is the list of packages that will be used:\n\n\nCode\nlibrary(tidyverse)\nlibrary(vtable)\nlibrary(rpart)\nlibrary(repr)\nlibrary(rpart.plot)\nlibrary(gam)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(glmnet)\nlibrary(scoringRules)\nlibrary(sjPlot)\n\n\n\n\nThe same data is used.\n\n\nCode\ndataS &lt;- read.csv('Data/Synthetic.csv')\n\n# Modifications \ndataS &lt;- dataS %&gt;%\n  mutate(Territory = as.factor(Territory)) %&gt;%\n  select(-c('Annual.pct.driven', 'Annual.miles.drive'))\ndata.select &lt;- dataS\n\n# Train-test \nset.seed(123)\ntrain &lt;- data.select %&gt;% sample_frac(0.8, replace = FALSE)\ntest &lt;- data.select %&gt;% anti_join(train)\n\n\n\n\nAs we concluded at the end of our overview of the data, a transformation of certain variables is also necessary.\n\n\nCode\n# Modif data\ntrain2 &lt;- train %&gt;%\n  mutate(Miles.per.day = Total.miles.driven/Duration,\n         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),\n         expo = Duration/365.25)\n\ntransform.fct &lt;- function(var){\n  df &lt;- train2 %&gt;% mutate(var_ = get(var)*Total.miles.driven/(1000*Duration))\n  q99 &lt;- quantile(df$var_, 0.99)\n  df &lt;- df %&gt;% mutate(var_ = ifelse(var_ &gt; q99, q99, var_))\n  #colnames(df)[ncol(df)] &lt;- paste0(var, '_')\n  return(df)\n}\n\ntrain2 &lt;- transform.fct(\"Brake.06miles\")\ntrain2 &lt;- transform.fct(\"Brake.08miles\")\ntrain2 &lt;- transform.fct(\"Brake.09miles\")\ntrain2 &lt;- transform.fct(\"Brake.11miles\")\ntrain2 &lt;- transform.fct(\"Brake.14miles\")\ntrain2 &lt;- transform.fct(\"Accel.06miles\")\ntrain2 &lt;- transform.fct(\"Accel.08miles\")\ntrain2 &lt;- transform.fct(\"Accel.09miles\")\ntrain2 &lt;- transform.fct(\"Accel.11miles\")\ntrain2 &lt;- transform.fct(\"Accel.12miles\")\ntrain2 &lt;- transform.fct(\"Accel.14miles\")\ntrain2 &lt;- transform.fct(\"Left.turn.intensity08\")\ntrain2 &lt;- transform.fct(\"Left.turn.intensity09\")\ntrain2 &lt;- transform.fct(\"Left.turn.intensity10\")\ntrain2 &lt;- transform.fct(\"Left.turn.intensity11\")\ntrain2 &lt;- transform.fct(\"Left.turn.intensity12\")\ntrain2 &lt;- transform.fct(\"Right.turn.intensity08\")\ntrain2 &lt;- transform.fct(\"Right.turn.intensity09\")\ntrain2 &lt;- transform.fct(\"Right.turn.intensity10\")\ntrain2 &lt;- transform.fct(\"Right.turn.intensity11\")\ntrain2 &lt;- transform.fct(\"Right.turn.intensity12\")\n\n# Create folds\nnb.fold &lt;- 5\nfold &lt;- sample(1:nb.fold, nrow(train2), replace = TRUE)\ntrain2$fold &lt;- fold\n\n##\n\ntest2 &lt;- test %&gt;%\n  mutate(Miles.per.day = Total.miles.driven/Duration,\n         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),\n         expo = Duration/365.25)\n\ntransform.fct &lt;- function(var){\n  df &lt;- test2 %&gt;% mutate(var_ = get(var)*Total.miles.driven/(1000*Duration))\n  q99 &lt;- quantile(df$var_, 0.99)\n  df &lt;- df %&gt;% mutate(var_ = ifelse(var_ &gt; q99, q99, var_))\n  #colnames(df)[ncol(df)] &lt;- paste0(var, '_')\n  return(df)\n}\n\ntest2 &lt;- transform.fct(\"Brake.06miles\")\ntest2 &lt;- transform.fct(\"Brake.08miles\")\ntest2 &lt;- transform.fct(\"Brake.09miles\")\ntest2 &lt;- transform.fct(\"Brake.11miles\")\ntest2 &lt;- transform.fct(\"Brake.14miles\")\ntest2 &lt;- transform.fct(\"Accel.06miles\")\ntest2 &lt;- transform.fct(\"Accel.08miles\")\ntest2 &lt;- transform.fct(\"Accel.09miles\")\ntest2 &lt;- transform.fct(\"Accel.11miles\")\ntest2 &lt;- transform.fct(\"Accel.12miles\")\ntest2 &lt;- transform.fct(\"Accel.14miles\")\ntest2 &lt;- transform.fct(\"Left.turn.intensity08\")\ntest2 &lt;- transform.fct(\"Left.turn.intensity09\")\ntest2 &lt;- transform.fct(\"Left.turn.intensity10\")\ntest2 &lt;- transform.fct(\"Left.turn.intensity11\")\ntest2 &lt;- transform.fct(\"Left.turn.intensity12\")\ntest2 &lt;- transform.fct(\"Right.turn.intensity08\")\ntest2 &lt;- transform.fct(\"Right.turn.intensity09\")\ntest2 &lt;- transform.fct(\"Right.turn.intensity10\")\ntest2 &lt;- transform.fct(\"Right.turn.intensity11\")\ntest2 &lt;- transform.fct(\"Right.turn.intensity12\")\n\n# Mean Encoding with White Noise pour les territoires\ncardi &lt;- length(unique(train$Territory))\n\nenc.terr &lt;- train2 %&gt;%\n  group_by(Territory) %&gt;%\n  summarize(freq = sum(NB_Claim)/sum(expo)) %&gt;%\n  arrange(freq) %&gt;%\n  mutate(terr.code= row_number()/(cardi+1)) %&gt;%\n  select(Territory, terr.code)\n\ntrain2 &lt;- train2 %&gt;%\n  group_by(Territory) %&gt;%\n  left_join(enc.terr, by='Territory') %&gt;%\n  ungroup()\n\ntest2 &lt;- test2 %&gt;%\n  group_by(Territory) %&gt;%\n  left_join(enc.terr, by='Territory') %&gt;%\n  ungroup()"
  },
  {
    "objectID": "VarTelematiques.html#basic-glm-models",
    "href": "VarTelematiques.html#basic-glm-models",
    "title": "4  Telematic Covariates",
    "section": "4.2 Basic GLM Models",
    "text": "4.2 Basic GLM Models\n\nSingle interceptTraditional covariates already used (without protected variables)Estimated Parameters\n\n\nA baseline model corresponding to a Generalized Linear Model (GLM) with intercept and predicting for each contract only the observed mean multiplied by the exposure is used as a point of comparison.\n\n\nCode\n## Model on each fold\nResult_  &lt;- data.frame()\nResult2_  &lt;- data.frame()\nfor(i in 1:nb.fold) {\n    learn &lt;- train2[train2$fold != i,]\n    valid &lt;- train2[train2$fold == i,]\n\n    mean &lt;- sum(learn$NB_Claim)/sum(learn$expo) \n    learn$pred.base &lt;- mean*learn$expo\n    valid$pred.base &lt;- mean*valid$expo\n\n    Result_ &lt;- rbind(Result_, c(i, Score.pred(valid$pred.base, valid$NB_Claim)/nrow(valid)))\n    Result2_ &lt;- rbind(Result2_, c(i, Score.pred(valid$pred.base, valid$NB_Claim)))\n}\n\n## Show results\ncolnames(Result_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ncolnames(Result2_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ntot &lt;- colSums(Result2_)/nrow(train2)\ntot$Fold &lt;- 'Total'\nResult_ &lt;- rbind(Result_ , tot)\n\nResult.base &lt;- Result_  \nBase &lt;- Result.base[nb.fold+1,]\n\nknitr::kable(Result_, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n\nTable 4.1: Prediction scores for the base model\n\n\nFold\nSc.log\nSc.MSE\nSc.quad\nSc.sph\nSc.DSS\nSc.CRPS\n\n\n\n\n1\n0.18212\n0.04666\n-0.91794\n-0.95797\n-2.16359\n0.04283\n\n\n2\n0.18185\n0.04681\n-0.91869\n-0.95838\n-2.14758\n0.04263\n\n\n3\n0.17509\n0.04555\n-0.92361\n-0.96096\n-2.17549\n0.04057\n\n\n4\n0.19451\n0.05149\n-0.91176\n-0.95472\n-2.06650\n0.04650\n\n\n5\n0.18543\n0.04832\n-0.91673\n-0.95733\n-2.13131\n0.04382\n\n\nTotal\n0.18379\n0.04777\n-0.91775\n-0.95787\n-2.13689\n0.04327\n\n\n\n\n\n\n\n\nThe model is trained on the entire training dataset and subsequently tested on the untouched test dataset, ensuring that the parameter calibration process remains independent from the test data.\n\n\nCode\nmean &lt;- sum(train2$NB_Claim)/sum(train2$expo) \ntest2$pred.base &lt;- mean*test2$expo\n\nResult_ &lt;- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ &lt;- cbind('Base', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all &lt;- Result_\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n\nTable 4.2: Prediction scores for the base model (testing set)\n\n\nModel\nSc.log\nSc.MSE\nSc.quad\nSc.sph\nSc.DSS\nSc.CRPS\n\n\n\n\nBase\n0.17674\n0.04545\n-0.92147\n-0.95981\n-2.19876\n0.04127\n\n\n\n\n\n\n\n\n\n\nFor comparison, we start with a simple GLM model considering only the traditional covariates (excluding the protected variables). Therefore, we include:\n\nCar.use,\n\nRegion,\n\nCar.age,\nYears.noclaims.\n\nThe model’s prediction scores are displayed below. As expected, adding segmentation variables improves the prediction scores compared to the simple baseline model with only an intercept.\n\n\nCode\n## Model \nscore.base &lt;- as.formula(NB_Claim ~ 1 + offset(log(expo)))\nscore.glm &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + offset(log(expo)))\n## Model on each fold\nResult_  &lt;- data.frame()\nResult2_  &lt;- data.frame()\nfor(i in 1:nb.fold) {\n    learn &lt;- train2[train2$fold != i,]\n    valid &lt;- train2[train2$fold == i,]\n    glm.fit &lt;- glm(score.glm, family = poisson(), data = learn)\n\n    learn$pred.base &lt;- predict(glm.fit, newdata=learn, type='response')\n    valid$pred.base &lt;- predict(glm.fit, newdata=valid, type='response')\n\n    Result_ &lt;- rbind(Result_, c(i, Score.pred(valid$pred.base, valid$NB_Claim)/nrow(valid)))\n    Result2_ &lt;- rbind(Result2_, c(i, Score.pred(valid$pred.base, valid$NB_Claim)))\n}\n\n## Model on all data from train\nglm.base &lt;- glm(score.base, family = poisson(), data = train2)\nglm.fit &lt;- glm(score.glm, family = poisson(), data = train2)\ntrain2$pred.glm1 &lt;- predict(glm.fit, newdata=train2, type='response')\nResult.glm1 &lt;- Result_  \n\n## Show results\ncolnames(Result_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ncolnames(Result2_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ntot &lt;- colSums(Result2_)/nrow(train2)\ntot$Fold &lt;- 'Total'\nResult_ &lt;- rbind(Result_ , tot)\nResult_ &lt;- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] &lt;- 'Improvement'\n\nfor(i in 2:7){\n  Result_[nb.fold+2,i] &lt;-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) &lt;- NULL\nknitr::kable(Result_, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n\nTable 4.3: Prediction scores for the GLM model with traditional covariates (without protected)\n\n\nFold\nSc.log\nSc.MSE\nSc.quad\nSc.sph\nSc.DSS\nSc.CRPS\n\n\n\n\n1\n0.17859\n0.04632\n-0.91850\n-0.95813\n-2.26779\n0.04253\n\n\n2\n0.17983\n0.04658\n-0.91905\n-0.95848\n-2.12885\n0.04243\n\n\n3\n0.17133\n0.04523\n-0.92405\n-0.96107\n-2.30038\n0.04031\n\n\n4\n0.19041\n0.05106\n-0.91249\n-0.95493\n-2.15895\n0.04611\n\n\n5\n0.18255\n0.04803\n-0.91717\n-0.95744\n-2.22301\n0.04358\n\n\nTotal\n0.18054\n0.04744\n-0.91826\n-0.95801\n-2.21586\n0.04299\n\n\nImprovement\n-0.00326\n-0.00032\n-0.00051\n-0.00014\n-0.07896\n-0.00028\n\n\n\n\n\n\n\n\nThe comparison with the test dataset is also depicted in the table below.\n\n\nCode\nscore.glm &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + offset(log(expo)))\n\nglm.fit &lt;- glm(score.glm, family = poisson(), data = train2)\ntest2$pred.base &lt;- predict(glm.fit, newdata=test2, type='response')\n\nResult_ &lt;- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ &lt;- cbind('GLM (trad.)', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all &lt;- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n\nTable 4.4: Prediction scores for the GLM model with traditional covariates (testing set)\n\n\nModel\nSc.log\nSc.MSE\nSc.quad\nSc.sph\nSc.DSS\nSc.CRPS\n\n\n\n\nBase\n0.17674\n0.04545\n-0.92147\n-0.95981\n-2.19876\n0.04127\n\n\nGLM (trad.)\n0.17359\n0.04514\n-0.92197\n-0.95995\n-2.27716\n0.04099\n\n\n\n\n\n\n\n\n\n\nThe table below shows the estimators obtained for the GLM-Poisson approach and compares them with the baseline model, which has only an intercept.\n\n\nCode\n## Model \nscore.base &lt;- as.formula(NB_Claim ~ 1 + offset(log(expo)))\n\nscore.glm &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + offset(log(expo)))\n\n## Model on all data from train\nglm.base &lt;- glm(score.base, family = poisson(), data = train2)\nglm.fit &lt;- glm(score.glm, family = poisson(), data = train2)\n\ntab_model(glm.base, glm.fit, transform = NULL)\n\n\n\n\nTable 4.5: Estimated parameters for the GLM model with traditional covariates (without protected)\n\n\n \nNB Claim\nNB Claim\n\n\nPredictors\nLog-Mean\nCI\np\nLog-Mean\nCI\np\n\n\n(Intercept)\n-2.94\n-2.98 – -2.91\n&lt;0.001\n-1.78\n-2.00 – -1.56\n&lt;0.001\n\n\nCar use [Commute]\n\n\n\n-0.32\n-0.47 – -0.15\n&lt;0.001\n\n\nCar use [Farmer]\n\n\n\n-0.84\n-1.37 – -0.38\n0.001\n\n\nCar use [Private]\n\n\n\n-0.46\n-0.62 – -0.29\n&lt;0.001\n\n\nRegion [Urban]\n\n\n\n0.14\n0.05 – 0.22\n0.002\n\n\nCar age\n\n\n\n-0.03\n-0.05 – -0.00\n0.028\n\n\nCar age^2\n\n\n\n-0.00\n-0.01 – -0.00\n0.001\n\n\nYears noclaims\n\n\n\n-0.07\n-0.09 – -0.05\n&lt;0.001\n\n\nYears noclaims^2\n\n\n\n0.00\n0.00 – 0.00\n&lt;0.001\n\n\nYears noclaims^3\n\n\n\n-0.00\n-0.00 – -0.00\n&lt;0.001\n\n\nObservations\n80000\n80000\n\n\nR2 Nagelkerke\n0.000\n0.028"
  },
  {
    "objectID": "VarTelematiques.html#glm-net",
    "href": "VarTelematiques.html#glm-net",
    "title": "4  Telematic Covariates",
    "section": "4.3 GLM-Net",
    "text": "4.3 GLM-Net\n\n4.3.1 Parametric transformation of continuous covariates\nNow, we add all available telematic variables to the model. Similar to the approach taken in the previous chapter, we will introduce a method utilizing the Generalized Additive Models (GAM) theory for all these continuous variables. This will enable us to observe the general form of the covariate to explain the number of claims. Subsequently, a parametric form will be proposed to achieve the best possible correspondence with the spline obtained by the GAM.\n\nVEHICLE USAGE LEVELType of Vehicle UsageDriving Behavior\n\n\nFor the two covariates related to usage level, the proposed parametric forms are as follows:\n\\[\\begin{align*}\ns(Miles.per.day) &\\approx Miles.per.day + log(Miles.per.day)\\\\\ns(Avgdays.week) &\\approx Avgdays.week + Avgdays.week^2\n\\end{align*}\\]\nThe graphs below compare the fit of the parametric approach with that of the GAM model.\n\n\nCode\nmin_ &lt;- min(train2$Miles.per.day) \nmax_ &lt;- max(train2$Miles.per.day) \nby_ &lt;-  (max_ - min_)/(nrow(train2)-1) \nadd &lt;- data.frame(seq(min_, max_, by_)) \ncolnames(add) &lt;- 'Miles.per.day'\n\nq99 &lt;- quantile(train2$Miles.per.day, 0.99)\n\ndb &lt;- train2 %&gt;%\n  select(-'Miles.per.day') %&gt;%\n  slice(1) \ndb &lt;- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb &lt;- cbind(db, add)\n\n##\n\nscore.gam &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + s(Miles.per.day))\n\nscore.glm &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + Miles.per.day + log(Miles.per.day) )\n\ngam.fit &lt;- gam(score.gam, family = poisson(), data = train2)\nglm.fit &lt;- glm(score.glm, family = poisson(), data = train2)\n\ndb$pred.gam &lt;- predict(gam.fit, newdata=db, type='response')\ndb$pred.glm &lt;- predict(glm.fit, newdata=db, type='response')\nbase &lt;- db %&gt;%\n  mutate(diff = abs(Miles.per.day - mean(train2$Miles.per.day))) %&gt;%\n  filter(diff == min(diff))\ndb$pred.gam &lt;- db$pred.gam/base$pred.gam[1]\ndb$pred.glm &lt;- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=Miles.per.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=Miles.per.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Miles per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n### Avgdays.week\n\nmin_ &lt;- min(train2$Avgdays.week) \nmax_ &lt;- max(train2$Avgdays.week) \nby_ &lt;-  (max_ - min_)/(nrow(train2)-1) \nadd &lt;- data.frame(seq(min_, max_, by_)) \ncolnames(add) &lt;- 'Avgdays.week'\n\nq99 &lt;- quantile(train2$Avgdays.week, 0.99)\n\ndb &lt;- train2 %&gt;%\n  select(-'Avgdays.week') %&gt;%\n  slice(1) \ndb &lt;- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb &lt;- cbind(db, add)\n\n##\n\nscore.gam &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + s(Avgdays.week))\n\nscore.glm &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + Avgdays.week + I(Avgdays.week^2) )\n\ngam.fit &lt;- gam(score.gam, family = poisson(), data = train2)\nglm.fit &lt;- glm(score.glm, family = poisson(), data = train2)\n\ndb$pred.gam &lt;- predict(gam.fit, newdata=db, type='response')\ndb$pred.glm &lt;- predict(glm.fit, newdata=db, type='response')\nbase &lt;- db %&gt;%\n  mutate(diff = abs(Avgdays.week - mean(train2$Avgdays.week))) %&gt;%\n  filter(diff == min(diff))\ndb$pred.gam &lt;- db$pred.gam/base$pred.gam[1]\ndb$pred.glm &lt;- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=Avgdays.week, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=Avgdays.week, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Avgdays.week',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n\n\n\n\n\n(a) Miles.per.day\n\n\n\n\n\n\n\n(b) Avgdays.week\n\n\n\n\nFigure 4.1: Smoothing of Usage level covariates\n\n\n\n\n\nSeveral covariates are available in the category Type of vehicle usage:\n\nWe propose the same parametric form for all variants of the variable Pct.drive.day (Monday to Sunday);\n\nThe same parametric form will also be proposed for Pct.drive.rush.am, Pct.drive.rush.pm, Pct.drive.2hrs, Pct.drive.3hrs, and Pct.drive.4hrs;\n\nThe other three covariates have their own parametric form.\n\nWe then have:\n\\[\\begin{align*}\ns(Pct.drive.day) &\\approx Pct.drive.day + Pct.drive.day^2 \\\\\ns(Pct.drive) &\\approx Pct.drive + \\sqrt{Pct.drive} \\\\\ns(max.day) &\\approx max.day + \\log(max.day) \\\\\ns(min.day) &\\approx min.day + min.day^2 \\\\\ns(max.min) &\\approx max.min + max.min^2\n\\end{align*}\\]\n\n\nCode\ntrain2$Pct.drive &lt;- train2$Pct.drive.sun\n\nmin_ &lt;- min(train2$Pct.drive) \nmax_ &lt;- max(train2$Pct.drive) \nby_ &lt;-  (max_ - min_)/(nrow(train2)-1) \nadd &lt;- data.frame(seq(min_, max_, by_)) \ncolnames(add) &lt;- 'Pct.drive'\n\nq99 &lt;- quantile(train2$Pct.drive, 0.99)\n\ndb &lt;- train2 %&gt;%\n  select(-'Pct.drive') %&gt;%\n  slice(1) \ndb &lt;- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb &lt;- cbind(db, add)\n\n##\n\nscore.gam &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + s(Pct.drive))\n\nscore.glm &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + Pct.drive + I(Pct.drive^2) )\n\ngam.fit &lt;- gam(score.gam, family = poisson(), data = train2)\nglm.fit &lt;- glm(score.glm, family = poisson(), data = train2)\n\ndb$pred.gam &lt;- predict(gam.fit, newdata=db, type='response')\ndb$pred.glm &lt;- predict(glm.fit, newdata=db, type='response')\nbase &lt;- db %&gt;%\n  mutate(diff = abs(Pct.drive - mean(train2$Pct.drive))) %&gt;%\n  filter(diff == min(diff))\ndb$pred.gam &lt;- db$pred.gam/base$pred.gam[1]\ndb$pred.glm &lt;- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=Pct.drive, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=Pct.drive, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Pct.drive',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n### Pct.drive \n\n\ntrain2$use.day &lt;- train2$Pct.drive.rush.am\n\nmin_ &lt;- min(train2$use.day) \nmax_ &lt;- max(train2$use.day) \nby_ &lt;-  (max_ - min_)/(nrow(train2)-1) \nadd &lt;- data.frame(seq(min_, max_, by_)) \ncolnames(add) &lt;- 'use.day'\n\nq99 &lt;- quantile(train2$use.day, 0.99)\n\ndb &lt;- train2 %&gt;%\n  select(-'use.day') %&gt;%\n  slice(1) \ndb &lt;- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb &lt;- cbind(db, add)\n\n##\n\nscore.gam &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + s(use.day))\n\nscore.glm &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + use.day + I(use.day^0.5))\n\ngam.fit &lt;- gam(score.gam, family = poisson(), data = train2)\nglm.fit &lt;- glm(score.glm, family = poisson(), data = train2)\n\ndb$pred.gam &lt;- predict(gam.fit, newdata=db, type='response')\ndb$pred.glm &lt;- predict(glm.fit, newdata=db, type='response')\nbase &lt;- db %&gt;%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %&gt;%\n  filter(diff == min(diff))\ndb$pred.gam &lt;- db$pred.gam/base$pred.gam[1]\ndb$pred.glm &lt;- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n### Max day\n\ntrain2$use.day &lt;- train2$max.day\n\nmin_ &lt;- min(train2$use.day) \nmax_ &lt;- max(train2$use.day) \nby_ &lt;-  (max_ - min_)/(nrow(train2)-1) \nadd &lt;- data.frame(seq(min_, max_, by_)) \ncolnames(add) &lt;- 'use.day'\n\nq99 &lt;- quantile(train2$use.day, 0.99)\n\ndb &lt;- train2 %&gt;%\n  select(-'use.day') %&gt;%\n  slice(1) \ndb &lt;- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb &lt;- cbind(db, add)\n\n##\n\nscore.gam &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + s(use.day))\n\nscore.glm &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + use.day + log(use.day) )\n\ngam.fit &lt;- gam(score.gam, family = poisson(), data = train2)\nglm.fit &lt;- glm(score.glm, family = poisson(), data = train2)\n\ndb$pred.gam &lt;- predict(gam.fit, newdata=db, type='response')\ndb$pred.glm &lt;- predict(glm.fit, newdata=db, type='response')\nbase &lt;- db %&gt;%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %&gt;%\n  filter(diff == min(diff))\ndb$pred.gam &lt;- db$pred.gam/base$pred.gam[1]\ndb$pred.glm &lt;- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Min day\n\n\ntrain2$use.day &lt;- train2$min.day\n\nmin_ &lt;- min(train2$use.day) \nmax_ &lt;- max(train2$use.day) \nby_ &lt;-  (max_ - min_)/(nrow(train2)-1) \nadd &lt;- data.frame(seq(min_, max_, by_)) \ncolnames(add) &lt;- 'use.day'\n\nq99 &lt;- quantile(train2$use.day, 0.99)\n\ndb &lt;- train2 %&gt;%\n  select(-'use.day') %&gt;%\n  slice(1) \ndb &lt;- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb &lt;- cbind(db, add)\n\n##\n\nscore.gam &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + s(use.day))\n\nscore.glm &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + use.day + I(use.day^2) )\n\ngam.fit &lt;- gam(score.gam, family = poisson(), data = train2)\nglm.fit &lt;- glm(score.glm, family = poisson(), data = train2)\n\ndb$pred.gam &lt;- predict(gam.fit, newdata=db, type='response')\ndb$pred.glm &lt;- predict(glm.fit, newdata=db, type='response')\nbase &lt;- db %&gt;%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %&gt;%\n  filter(diff == min(diff))\ndb$pred.gam &lt;- db$pred.gam/base$pred.gam[1]\ndb$pred.glm &lt;- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Max min\n\ntrain2$use.day &lt;- train2$max.min\n\nmin_ &lt;- min(train2$use.day) \nmax_ &lt;- max(train2$use.day) \nby_ &lt;-  (max_ - min_)/(nrow(train2)-1) \nadd &lt;- data.frame(seq(min_, max_, by_)) \ncolnames(add) &lt;- 'use.day'\n\nq99 &lt;- quantile(train2$use.day, 0.99)\n\ndb &lt;- train2 %&gt;%\n  select(-'use.day') %&gt;%\n  slice(1) \ndb &lt;- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb &lt;- cbind(db, add)\n\n##\n\nscore.gam &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + s(use.day))\n\nscore.glm &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + use.day + I(use.day^2) )\n\ngam.fit &lt;- gam(score.gam, family = poisson(), data = train2)\nglm.fit &lt;- glm(score.glm, family = poisson(), data = train2)\n\ndb$pred.gam &lt;- predict(gam.fit, newdata=db, type='response')\ndb$pred.glm &lt;- predict(glm.fit, newdata=db, type='response')\nbase &lt;- db %&gt;%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %&gt;%\n  filter(diff == min(diff))\ndb$pred.gam &lt;- db$pred.gam/base$pred.gam[1]\ndb$pred.glm &lt;- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n\n\n\n\n\n(a) Pct.drive.mon\n\n\n\n\n\n\n\n(b) Pct.drive.rush.am\n\n\n\n\n\n\n\n\n\n(c) max.day\n\n\n\n\n\n\n\n(d) min.day\n\n\n\n\n\n\n\n\n\n(e) max.min\n\n\n\n\nFigure 4.2: Smoothing of Type of vehicle usage covariates (severity)\n\n\n\n\n\nThe same parametric form is proposed for the different variants of the Accel and Brake variables, i.e., Accel.06miles to Accel.14miles, and Brake.06miles to Brake.14miles. For the different variants of the turn variable, a single parametric form is also used:\n\\[\\begin{align*}\ns(Brake.Accel) &\\approx Brake.Accel + Brake.Accel^2 + Brake.Accel^3\\\\\ns(Turn) &\\approx Turn + log(Turn)\n\\end{align*}\\]\nThe graphs below compare the fit of the parametric approach for Accel.06miles and Right.turn.intensity08 with that of the GAM model.\n\n\nCode\ntrain2$use.day &lt;- train2$Accel.06miles\n\nmin_ &lt;- min(train2$use.day) \nmax_ &lt;- max(train2$use.day) \nby_ &lt;-  (max_ - min_)/(nrow(train2)-1) \nadd &lt;- data.frame(seq(min_, max_, by_)) \ncolnames(add) &lt;- 'use.day'\n\nq99 &lt;- quantile(train2$use.day, 0.99)\n\ndb &lt;- train2 %&gt;%\n  select(-'use.day') %&gt;%\n  slice(1) \ndb &lt;- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb &lt;- cbind(db, add)\n\n##\n\nscore.gam &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + s(use.day))\n\nscore.glm &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + use.day + I(use.day^2) + I(use.day^3) )\n\ngam.fit &lt;- gam(score.gam, family = poisson(), data = train2)\nglm.fit &lt;- glm(score.glm, family = poisson(), data = train2)\n\ndb$pred.gam &lt;- predict(gam.fit, newdata=db, type='response')\ndb$pred.glm &lt;- predict(glm.fit, newdata=db, type='response')\nbase &lt;- db %&gt;%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %&gt;%\n  filter(diff == min(diff))\ndb$pred.gam &lt;- db$pred.gam/base$pred.gam[1]\ndb$pred.glm &lt;- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n### Right and left turns\n\n\ntrain2$use.day &lt;- train2$Right.turn.intensity08\n\nq99 &lt;- quantile(train2$use.day, 0.99)\n\nmin_ &lt;- min(train2$use.day) \nmax_ &lt;- q99\nby_ &lt;-  (max_ - min_)/(nrow(train2)-1) \nadd &lt;- data.frame(seq(min_, max_, by_)) \ncolnames(add) &lt;- 'use.day'\n\ndb &lt;- train2 %&gt;%\n  select(-'use.day') %&gt;%\n  slice(1) \ndb &lt;- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb &lt;- cbind(db, add)\n\n##\n\ntemp &lt;- train2 %&gt;%\n  mutate(use.day = pmin(q99, use.day))\n\nscore.gam &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + s(use.day))\n\nscore.glm &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                         + use.day + log1p(use.day))\n\ngam.fit &lt;- gam(score.gam, family = poisson(), data = temp)\nglm.fit &lt;- glm(score.glm, family = poisson(), data = temp)\n\ndb$pred.gam &lt;- predict(gam.fit, newdata=db, type='response')\ndb$pred.glm &lt;- predict(glm.fit, newdata=db, type='response')\nbase &lt;- db %&gt;%\n  mutate(diff = abs(use.day - mean(temp$use.day))) %&gt;%\n  filter(diff == min(diff))\ndb$pred.gam &lt;- db$pred.gam/base$pred.gam[1]\ndb$pred.glm &lt;- db$pred.glm/base$pred.glm[1]\n  \nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n # xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n\n\n\n\n\n(a) Accel.06miles\n\n\n\n\n\n\n\n(b) Right.turn.intensity08\n\n\n\n\nFigure 4.3: Smoothing of Driving behavior covariates (severity)\n\n\n\n\n\n\n\n\n4.3.2 Fitting the GLM-Net model\n\nOptimal valueParsimonious modelRESIDUALS AND PROTECTED VARIABLESGLM-NET ON RESIDUALS\n\n\nThe parameters of the GLM-net were calibrated using cross-validation to obtain the model’s hyperparameters. Using these values, we can calculate the prediction scores of the model based on all covariates.\n\n\nCode\nglm.score &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3)\n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + log(max.day) \n                        + min.day + I(min.day^2)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Accel.06miles + I(Accel.06miles^2) + I(Accel.06miles^3)\n                        + Accel.08miles + I(Accel.08miles^2) + I(Accel.08miles^3)\n                        + Accel.09miles + I(Accel.09miles^2) + I(Accel.09miles^3)\n                        + Accel.11miles + I(Accel.11miles^2) + I(Accel.11miles^3)\n                        + Accel.12miles + I(Accel.12miles^2) + I(Accel.12miles^3)\n                        + Accel.14miles + I(Accel.14miles^2) + I(Accel.14miles^3)\n                        + Brake.06miles + I(Brake.06miles^2) + I(Brake.06miles^3)\n                        + Brake.08miles + I(Brake.08miles^2) + I(Brake.08miles^3)\n                        + Brake.09miles + I(Brake.09miles^2) + I(Brake.09miles^3)\n                        + Brake.11miles + I(Brake.11miles^2) + I(Brake.11miles^3)\n                        + Brake.12miles + I(Brake.12miles^2) + I(Brake.12miles^3)\n                        + Brake.14miles + I(Brake.14miles^2) + I(Brake.14miles^3)\n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\nResult_  &lt;- data.frame()\nResult2_  &lt;- data.frame()\nfor(i in 1:nb.fold) {\n    learn &lt;- train2[train2$fold != i,]\n    valid &lt;- train2[train2$fold == i,]\n    \n    matrix.x &lt;- model.matrix(glm.score, data=learn)[,-1]\n    y &lt;- learn$NB_Claim\n    offset &lt;- log(learn$expo)\n\n    lambda.min &lt;- 3.981072e-05\n    lambda.1se &lt;- 0.001258925\n    \n    lambda.select &lt;- lambda.min\n    fit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\n    #fit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n  \n    matrix.x &lt;- model.matrix(glm.score, data=valid)[,-1]\n    y &lt;- valid$NB_Claim\n    offset &lt;- log(valid$expo)\n\n    valid$pred &lt;- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)\n    \n    Result_ &lt;- rbind(Result_, c(i, Score.pred(valid$pred, valid$NB_Claim)/nrow(valid)))\n    Result2_ &lt;- rbind(Result2_, c(i, Score.pred(valid$pred, valid$NB_Claim)))\n}\n\n\n## Show results\ncolnames(Result_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ncolnames(Result2_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ntot &lt;- colSums(Result2_)/nrow(train2)\ntot$Fold &lt;- 'Total'\nResult_ &lt;- rbind(Result_ , tot)\nResult_ &lt;- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] &lt;- 'Improvement'\n\nfor(i in 2:7){\n  Result_[nb.fold+2,i] &lt;-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) &lt;- NULL\nknitr::kable(Result_, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n\nTable 4.6: Prediction scores for the GLM-net model (alpha=1)\n\n\nFold\nSc.log\nSc.MSE\nSc.quad\nSc.sph\nSc.DSS\nSc.CRPS\n\n\n\n\n1\n0.16065\n0.04360\n-0.92280\n-0.95956\n-2.84021\n0.04018\n\n\n2\n0.15958\n0.04302\n-0.92380\n-0.96009\n512.20739\n0.03970\n\n\n3\n0.15391\n0.04248\n-0.92795\n-0.96228\n5.94017\n0.03807\n\n\n4\n0.16786\n0.04706\n-0.91797\n-0.95674\n-2.67292\n0.04298\n\n\n5\n0.16300\n0.04500\n-0.92166\n-0.95890\n-2.76580\n0.04106\n\n\nTotal\n0.16099\n0.04424\n-0.92284\n-0.95952\n101.77054\n0.04039\n\n\nImprovement\n-0.02280\n-0.00353\n-0.00509\n-0.00164\n103.90744\n-0.00287\n\n\n\n\n\n\n\n\nOn the test set, we obtain:\n\n\nCode\nglm.score &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3)\n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + log(max.day) \n                        + min.day + I(min.day^2)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Accel.06miles + I(Accel.06miles^2) + I(Accel.06miles^3)\n                        + Accel.08miles + I(Accel.08miles^2) + I(Accel.08miles^3)\n                        + Accel.09miles + I(Accel.09miles^2) + I(Accel.09miles^3)\n                        + Accel.11miles + I(Accel.11miles^2) + I(Accel.11miles^3)\n                        + Accel.12miles + I(Accel.12miles^2) + I(Accel.12miles^3)\n                        + Accel.14miles + I(Accel.14miles^2) + I(Accel.14miles^3)\n                        + Brake.06miles + I(Brake.06miles^2) + I(Brake.06miles^3)\n                        + Brake.08miles + I(Brake.08miles^2) + I(Brake.08miles^3)\n                        + Brake.09miles + I(Brake.09miles^2) + I(Brake.09miles^3)\n                        + Brake.11miles + I(Brake.11miles^2) + I(Brake.11miles^3)\n                        + Brake.12miles + I(Brake.12miles^2) + I(Brake.12miles^3)\n                        + Brake.14miles + I(Brake.14miles^2) + I(Brake.14miles^3)\n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\n\nmatrix.x &lt;- model.matrix(glm.score, data=train2)[,-1]\ny &lt;- train2$NB_Claim\noffset &lt;- log(train2$expo)\n\nlambda.min &lt;- 3.981072e-05\nlambda.1se &lt;- 0.001258925\n    \nlambda.select &lt;- lambda.min\nfit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\n#fit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n\ntrain2$pred.tele &lt;- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)\n\nmatrix.x &lt;- model.matrix(glm.score, data=test2)[,-1]\ny &lt;- test2$NB_Claim\noffset &lt;- log(test2$expo)\n\ntest2$pred.base &lt;- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)\n\nResult_ &lt;- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ &lt;- cbind('LASSO (optimal)', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all &lt;- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n\nTable 4.7: Prediction scores for the GLM-net model (testing set)\n\n\nModel\nSc.log\nSc.MSE\nSc.quad\nSc.sph\nSc.DSS\nSc.CRPS\n\n\n\n\nBase\n0.17674\n0.04545\n-0.92147\n-0.95981\n-2.19876\n0.04127\n\n\nGLM (trad.)\n0.17359\n0.04514\n-0.92197\n-0.95995\n-2.27716\n0.04099\n\n\nLASSO (optimal)\n0.15536\n0.04239\n-0.92624\n-0.96135\n-2.69596\n0.03866\n\n\n\n\n\n\n\n\n\n\nInstead of using the optimal value of the penalty \\(\\lambda\\) in the elastic-net approach, it is often advised to use a penalty value located at one standard error (\\(\\lambda_{1se}\\)). This helps to obtain a more parsimonious model. The prediction scores of such a model are displayed below.\n\n\nCode\nResult_  &lt;- data.frame()\nResult2_  &lt;- data.frame()\nfor(i in 1:nb.fold) {\n    learn &lt;- train2[train2$fold != i,]\n    valid &lt;- train2[train2$fold == i,]\n    \n    matrix.x &lt;- model.matrix(glm.score, data=learn)[,-1]\n    y &lt;- learn$NB_Claim\n    offset &lt;- log(learn$expo)\n\n    lambda.min &lt;- 3.981072e-05\n    lambda.1se &lt;- 0.001258925\n    \n    lambda.select &lt;- lambda.1se\n    #fit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\n    fit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n  \n    matrix.x &lt;- model.matrix(glm.score, data=valid)[,-1]\n    y &lt;- valid$NB_Claim\n    offset &lt;- log(valid$expo)\n\n    valid$pred &lt;- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)\n    \n    Result_ &lt;- rbind(Result_, c(i, Score.pred(valid$pred, valid$NB_Claim)/nrow(valid)))\n    Result2_ &lt;- rbind(Result2_, c(i, Score.pred(valid$pred, valid$NB_Claim)))\n}\n\n\n## Show results\ncolnames(Result_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ncolnames(Result2_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ntot &lt;- colSums(Result2_)/nrow(train2)\ntot$Fold &lt;- 'Total'\nResult_ &lt;- rbind(Result_ , tot)\nResult_ &lt;- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] &lt;- 'Improvement'\n\nfor(i in 2:7){\n  Result_[nb.fold+2,i] &lt;-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) &lt;- NULL\nknitr::kable(Result_, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n\nTable 4.8: Prediction scores for the GLM-net model (alpha=1)\n\n\nFold\nSc.log\nSc.MSE\nSc.quad\nSc.sph\nSc.DSS\nSc.CRPS\n\n\n\n\n1\n0.16141\n0.04348\n-0.92233\n-0.95936\n-2.76915\n0.04033\n\n\n2\n0.16045\n0.04370\n-0.92307\n-0.95975\n-2.72663\n0.04015\n\n\n3\n0.15505\n0.04271\n-0.92730\n-0.96203\n-2.73338\n0.03840\n\n\n4\n0.16975\n0.04763\n-0.91721\n-0.95642\n-2.65287\n0.04342\n\n\n5\n0.16418\n0.04518\n-0.92116\n-0.95868\n-2.69140\n0.04131\n\n\nTotal\n0.16216\n0.04454\n-0.92222\n-0.95925\n-2.71461\n0.04072\n\n\nImprovement\n-0.02163\n-0.00323\n-0.00447\n-0.00137\n-0.57772\n-0.00255\n\n\n\n\n\n\n\n\nOn the test set, we obtain:\n\n\nCode\nglm.score &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3)\n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + log(max.day) \n                        + min.day + I(min.day^2)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Accel.06miles + I(Accel.06miles^2) + I(Accel.06miles^3)\n                        + Accel.08miles + I(Accel.08miles^2) + I(Accel.08miles^3)\n                        + Accel.09miles + I(Accel.09miles^2) + I(Accel.09miles^3)\n                        + Accel.11miles + I(Accel.11miles^2) + I(Accel.11miles^3)\n                        + Accel.12miles + I(Accel.12miles^2) + I(Accel.12miles^3)\n                        + Accel.14miles + I(Accel.14miles^2) + I(Accel.14miles^3)\n                        + Brake.06miles + I(Brake.06miles^2) + I(Brake.06miles^3)\n                        + Brake.08miles + I(Brake.08miles^2) + I(Brake.08miles^3)\n                        + Brake.09miles + I(Brake.09miles^2) + I(Brake.09miles^3)\n                        + Brake.11miles + I(Brake.11miles^2) + I(Brake.11miles^3)\n                        + Brake.12miles + I(Brake.12miles^2) + I(Brake.12miles^3)\n                        + Brake.14miles + I(Brake.14miles^2) + I(Brake.14miles^3)\n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\n\nmatrix.x &lt;- model.matrix(glm.score, data=train2)[,-1]\ny &lt;- train2$NB_Claim\noffset &lt;- log(train2$expo)\n\nlambda.min &lt;- 3.981072e-05\nlambda.1se &lt;- 0.001258925\n    \nlambda.select &lt;- lambda.1se\n#fit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\nfit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n  \nmatrix.x &lt;- model.matrix(glm.score, data=test2)[,-1]\ny &lt;- test2$NB_Claim\noffset &lt;- log(test2$expo)\n\ntest2$pred.base &lt;- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)\n\nResult_ &lt;- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ &lt;- cbind('LASSO (parsimonious)', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all &lt;- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n\nTable 4.9: Prediction scores for the GLM-net model (testing set)\n\n\nModel\nSc.log\nSc.MSE\nSc.quad\nSc.sph\nSc.DSS\nSc.CRPS\n\n\n\n\nBase\n0.17674\n0.04545\n-0.92147\n-0.95981\n-2.19876\n0.04127\n\n\nGLM (trad.)\n0.17359\n0.04514\n-0.92197\n-0.95995\n-2.27716\n0.04099\n\n\nLASSO (optimal)\n0.15536\n0.04239\n-0.92624\n-0.96135\n-2.69596\n0.03866\n\n\nLASSO (parsimonious)\n0.15704\n0.04261\n-0.92579\n-0.96120\n-2.70016\n0.03889\n\n\n\n\n\n\n\n\n\n\nComparing the results obtained with the pricing model based solely on traditional variables, we observe that the addition of telematic variables improves prediction quality. However, we can verify whether the protected variables we excluded from the analysis still retain predictive capability. Thus, we fit a GLM-net model on telematic data and predict the expected frequency of the model on the training dataset.\nUsing the prediction as an offset variable, we can effectively assess whether the protected variables still capture a trend. The reliability of this approach is further bolstered by the comparison of the results obtained with the graphs we had in Chapter 2. If a curve is horizontal and close to the value of 1 for all possible values of a covariate, it indicates that telematic variables have indeed captured the predictive capability of the variable to predict claim frequency, as demonstrated in the graphs below.\nIn the graphs below, we observe that the addition of telematic variables:\n\ndiminishes the effect of credit score, but it still appears to be predictive;\n\nalmost eliminates the effect of driver age;\n\nthe addition of telematic variables has led to a significant shift in the effect of driver gender. Originally, a premium reduction for males seemed necessary, but adding telematic variables suggest that males should now have a surcharge;\nsignificantly reduces the effect of marital status;\n\nslightly diminishes the effect of territory without eliminating it.\n\n\n\nCode\nmeaninv  &lt;- sum(train2$expo)/sum(train2$NB_Claim)\n\ntemp2 &lt;- train2 %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Insured.age = pmin(Insured.age, 80),\n                Group = ceiling(Credit.score/25) * 25) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(expo),\n            expo2=sum(pred.tele)) %&gt;% \n  mutate(freq = meaninv*NB_Claim/expo, \n         freq2 =  NB_Claim/expo2)\n\nggplot() + \n  geom_smooth(aes(x=Group, y=freq, weight = expo, color='Observed'),se=F, size=1, data=temp2) + \n  geom_smooth(aes(x=Group, y=freq2, weight = expo, color='Residuals'),se=F, size=1, data=temp2) + \n  labs(x = 'Credit Score',\n       y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  guides(color = guide_legend(title = \"\")) +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n### Age of the insured\n\ntemp2 &lt;- train2 %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Insured.age = pmin(Insured.age, 80),\n                Group = ceiling(Insured.age/5) * 5) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(expo),\n            expo2=sum(pred.tele)) %&gt;% \n  mutate(freq = meaninv*NB_Claim/expo, \n         freq2 =  NB_Claim/expo2)\n\nggplot() + \n  geom_smooth(aes(x=Group, y=freq, weight = expo, color='Observed'),se=F, size=1, data=temp2) + \n  geom_smooth(aes(x=Group, y=freq2, weight = expo, color='Residuals'),se=F, size=1, data=temp2) + \n  labs(x = 'Age of the insured',\n       y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  guides(color = guide_legend(title = \"\")) +\n        theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n### Sex of the insured\n\ntemp &lt;- train2 %&gt;%\n  mutate(Var_ = Insured.sex) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(nbclaim = sum(NB_Claim),\n            expo = sum(expo),\n            expo2 = sum(pred.tele)) %&gt;%\n  mutate(freq = nbclaim/expo,\n         freq2 = nbclaim/expo2)\n\ntemp$freq &lt;- temp$freq/temp$freq[1]\ntemp$freq2 &lt;- temp$freq2/temp$freq2[1]\n\nggplot() + #start plot by by plotting bars\n  geom_point(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=0.7) +\n  geom_point(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=0.7) +\n  labs(x = 'Sex of the insured', y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+\n  guides(color=guide_legend(title=\"\")) +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Marital Status\n\ntemp &lt;- train2 %&gt;%\n  mutate(Var_ = Marital) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(nbclaim = sum(NB_Claim),\n            expo = sum(expo),\n            expo2 = sum(pred.tele)) %&gt;%\n  mutate(freq = nbclaim/expo, \n         freq2 = nbclaim/expo2)\n\ntemp$freq &lt;- temp$freq/temp$freq[1]\ntemp$freq2 &lt;- temp$freq2/temp$freq2[1]\n\nggplot() + #start plot by by plotting bars\n  geom_point(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=0.7) +\n  geom_point(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=0.7) +\n  labs(x = 'Marital status of the insured', y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  ylim(0.9, max(temp$freq, temp$freq2)*1.2)+\n  guides(color=guide_legend(title=\"\")) +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Insured's territory\n\ntemp &lt;- train2 %&gt;%\n  mutate(Var_ = Territory) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(nbclaim = sum(NB_Claim),\n            expo = sum(expo),\n            expo2 = sum(pred.tele)) %&gt;%\n  mutate(freq1 = nbclaim/expo2, \n         freq2 = meaninv*nbclaim/expo)\n\nggplot() + \n  geom_line(data = temp, aes(x = Var_, y = freq1, group = 1, color='Residuals'), size=0.7) +\n  geom_point(data = temp, aes(x = Var_, y = freq1, group = 1, color='Residuals'), size=0.7) +\n  geom_line(data = temp, aes(x = Var_, y = freq2, group = 1, color='Observed'), size=0.7) +\n  geom_point(data = temp, aes(x = Var_, y = freq2, group = 1, color='Observed'), size=0.7) +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  labs(x = 'Territory',\n       y = 'Relativity') +\n  scale_x_discrete(labels = NULL, breaks = NULL)+\n  guides(color=guide_legend(title=\"\")) +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n\n\n\n\n\n(a) Credit Score\n\n\n\n\n\n\n\n(b) Age of the Insured\n\n\n\n\n\n\n\n\n\n(c) Sex of the Insured\n\n\n\n\n\n\n\n(d) Marital Status of the Insured\n\n\n\n\n\n\n\n\n\n(e) Territory\n\n\n\n\nFigure 4.4: Observed Relativity vs. Residuals Relativity\n\n\n\n\n\nOne can directly fit a GLM-Net model by appropriately utilizing the initial predictions. By retaining only the protected variables in the GLM-Net approach while using the prediction as an offset variable, it is possible to verify whether adding protected variables enhances the model’s predictive capacity.\nThe tables below indicate the scores obtained for this new model (with the two approaches marked with an *). We observe a slight improvement in the model, indicating that using telematics data does not eliminate the need to use protected variables.\n\n\nCode\nglm.score &lt;- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3)\n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + log(max.day) \n                        + min.day + I(min.day^2)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Accel.06miles + I(Accel.06miles^2) + I(Accel.06miles^3)\n                        + Accel.08miles + I(Accel.08miles^2) + I(Accel.08miles^3)\n                        + Accel.09miles + I(Accel.09miles^2) + I(Accel.09miles^3)\n                        + Accel.11miles + I(Accel.11miles^2) + I(Accel.11miles^3)\n                        + Accel.12miles + I(Accel.12miles^2) + I(Accel.12miles^3)\n                        + Accel.14miles + I(Accel.14miles^2) + I(Accel.14miles^3)\n                        + Brake.06miles + I(Brake.06miles^2) + I(Brake.06miles^3)\n                        + Brake.08miles + I(Brake.08miles^2) + I(Brake.08miles^3)\n                        + Brake.09miles + I(Brake.09miles^2) + I(Brake.09miles^3)\n                        + Brake.11miles + I(Brake.11miles^2) + I(Brake.11miles^3)\n                        + Brake.12miles + I(Brake.12miles^2) + I(Brake.12miles^3)\n                        + Brake.14miles + I(Brake.14miles^2) + I(Brake.14miles^3)\n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\nmatrix.x &lt;- model.matrix(glm.score, data=train2)[,-1]\ny &lt;- train2$NB_Claim\noffset &lt;- log(train2$expo)\n    \nlambda.min &lt;- 3.981072e-05\nlasso.min &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.min)\ntrain2$pred.tele &lt;- predict(lasso.min, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.min)\n\nmatrix.x &lt;- model.matrix(glm.score, data=test2)[,-1]\ny &lt;- test2$NB_Claim\noffset &lt;- log(test2$expo)\ntest2$pred.tele &lt;- predict(lasso.min, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.min)\n\nglm.score &lt;- as.formula(NB_Claim ~ Insured.sex + Marital \n                                  + Credit.score +  I(Credit.score^2) \n                                  + Insured.age +  log(Insured.age) + I(Insured.age^2) \n                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x &lt;- model.matrix(glm.score, data=train2)[,-1]\ny &lt;- train2$NB_Claim\noffset &lt;- log(train2$pred.tele)\n\nlambda.min &lt;- 1e-08\nlambda.1se &lt;- 0.007943282\n    \nlambda.select &lt;- lambda.min\n#fit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\nfit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n  \nmatrix.x &lt;- model.matrix(glm.score, data=test2)[,-1]\ny &lt;- test2$NB_Claim\noffset &lt;- log(test2$pred.tele)\n\ntest2$pred.base &lt;- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)\n\nResult_ &lt;- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ &lt;- cbind('LASSO* (optimal)', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all &lt;- rbind(Result_all, Result_)\n\n###\n\nglm.score &lt;- as.formula(NB_Claim ~ Insured.sex + Marital \n                                  + Credit.score +  I(Credit.score^2) \n                                  + Insured.age +  log(Insured.age) + I(Insured.age^2) \n                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x &lt;- model.matrix(glm.score, data=train2)[,-1]\ny &lt;- train2$NB_Claim\noffset &lt;- log(train2$pred.tele)\n\nlambda.min &lt;- 1e-08\nlambda.1se &lt;- 0.007943282\n    \nlambda.select &lt;- lambda.1se\nfit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\n#fit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n  \nmatrix.x &lt;- model.matrix(glm.score, data=test2)[,-1]\ny &lt;- test2$NB_Claim\noffset &lt;- log(test2$pred.tele)\n\ntest2$pred.base &lt;- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)\n\nResult_ &lt;- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ &lt;- cbind('LASSO* (parsimonious)', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all &lt;- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n\nTable 4.10: Prediction scores for the GLM-net model (testing set)\n\n\nModel\nSc.log\nSc.MSE\nSc.quad\nSc.sph\nSc.DSS\nSc.CRPS\n\n\n\n\nBase\n0.17674\n0.04545\n-0.92147\n-0.95981\n-2.19876\n0.04127\n\n\nGLM (trad.)\n0.17359\n0.04514\n-0.92197\n-0.95995\n-2.27716\n0.04099\n\n\nLASSO (optimal)\n0.15536\n0.04239\n-0.92624\n-0.96135\n-2.69596\n0.03866\n\n\nLASSO (parsimonious)\n0.15704\n0.04261\n-0.92579\n-0.96120\n-2.70016\n0.03889\n\n\nLASSO* (optimal)\n0.15373\n0.04209\n-0.92677\n-0.96154\n-2.62254\n0.03837\n\n\nLASSO* (parsimonious)\n0.15481\n0.04226\n-0.92642\n-0.96142\n-2.69780\n0.03855"
  },
  {
    "objectID": "VarTelematiques.html#xgboost",
    "href": "VarTelematiques.html#xgboost",
    "title": "4  Telematic Covariates",
    "section": "4.4 XGBoost",
    "text": "4.4 XGBoost\nWe now consider an XGBoost model. As with the frequency model, hyperparameter values are obtained by performing a Bayesian search on a grid of possible values.\n\nPrediction ScoresVariables ImportanceRESIDUALS AND PROTECTED VARIABLESXGBOOST ON RESIDUALS\n\n\nWe can calculate the model’s prediction scores based on all classical and telematics covariates. The XGBoost approach is particularly effective in capturing the effect of all available telematic covariates. Indeed, the scores obtained are significantly improved compared to other tested approaches.\n\n\nCode\nparam &lt;- list(\n  eta = 0.02337437,\n  max_depth = 26,\n  subsample = 0.8097923,\n  min_child_weight = 1,\n  booster = \"gbtree\",\n  objective = \"count:poisson\",\n  eval_metric = \"poisson-nloglik\")\n\nset.seed(133)\nxgbcv &lt;- xgb.cv(params = param,\n                nrounds = 367,\n                data = dtrain,\n                folds = folds,\n                prediction = TRUE,\n                early_stopping_rounds = 10,\n                verbose = 0,\n                maximize = F)\n  \nSc.log &lt;- sapply(xgbcv$folds, function(x){-dpois(train2$NB_Claim[x], unlist(xgbcv$pred[x]), log=TRUE)})\nSc.MSE &lt;- sapply(xgbcv$folds, function(x){(train2$NB_Claim[x]-unlist(xgbcv$pred[x]))^2})\nSc.quad &lt;- sapply(xgbcv$folds, function(x){\n  nb &lt;- train2$NB_Claim[x]\n  mu &lt;- unlist(xgbcv$pred[x])\n  -2*dpois(nb,lambda=mu) + dpois(0,lambda=mu)^2 + dpois(1,lambda=mu)^2 + dpois(2,lambda=mu)^2+ dpois(3,lambda=mu)^2 + dpois(4,lambda=mu)^2 + dpois(5,lambda=mu)^2 \n})  \nSc.sph &lt;- sapply(xgbcv$folds, function(x){\n  nb &lt;- train2$NB_Claim[x]\n  mu &lt;- unlist(xgbcv$pred[x])\n  -dpois(nb,lambda=mu) / sqrt(dpois(0,lambda=mu)^2 + dpois(1,lambda=mu)^2 + dpois(2,lambda=mu)^2+ dpois(3,lambda=mu)^2 + dpois(4,lambda=mu)^2 + dpois(5,lambda=mu)^2 ) \n})\nSc.DSS &lt;- sapply(xgbcv$folds, function(x){dss_pois(train2$NB_Claim[x], unlist(xgbcv$pred[x]))})\nSc.CRPS &lt;- sapply(xgbcv$folds, function(x){crps_pois(train2$NB_Claim[x], unlist(xgbcv$pred[x]))})\n\nResult_  &lt;- rbind(\nc(1,mean(Sc.log[1]$fold1), mean(Sc.MSE[1]$fold1), mean(Sc.quad[1]$fold1), mean(Sc.sph[1]$fold1), mean(Sc.DSS[1]$fold1), mean(Sc.CRPS[1]$fold1)),\nc(2,mean(Sc.log[2]$fold2), mean(Sc.MSE[2]$fold2), mean(Sc.quad[2]$fold2), mean(Sc.sph[2]$fold2), mean(Sc.DSS[2]$fold2), mean(Sc.CRPS[2]$fold2)),\nc(3,mean(Sc.log[3]$fold3), mean(Sc.MSE[3]$fold3), mean(Sc.quad[3]$fold3), mean(Sc.sph[3]$fold3), mean(Sc.DSS[3]$fold3), mean(Sc.CRPS[3]$fold3)),\nc(4,mean(Sc.log[4]$fold4), mean(Sc.MSE[4]$fold4), mean(Sc.quad[4]$fold4), mean(Sc.sph[4]$fold4), mean(Sc.DSS[4]$fold4), mean(Sc.CRPS[4]$fold4)),\nc(5,mean(Sc.log[5]$fold5), mean(Sc.MSE[5]$fold5), mean(Sc.quad[5]$fold5), mean(Sc.sph[5]$fold5), mean(Sc.DSS[5]$fold5), mean(Sc.CRPS[5]$fold5))\n)\n\nRes.sum  &lt;- rbind(\nc(sum(Sc.log[1]$fold1), sum(Sc.MSE[1]$fold1), sum(Sc.quad[1]$fold1), sum(Sc.sph[1]$fold1), sum(Sc.DSS[1]$fold1), sum(Sc.CRPS[1]$fold1)),\nc(sum(Sc.log[2]$fold2), sum(Sc.MSE[2]$fold2), sum(Sc.quad[2]$fold2), sum(Sc.sph[2]$fold2), sum(Sc.DSS[2]$fold2), sum(Sc.CRPS[2]$fold2)),\nc(sum(Sc.log[3]$fold3), sum(Sc.MSE[3]$fold3), sum(Sc.quad[3]$fold3), sum(Sc.sph[3]$fold3), sum(Sc.DSS[3]$fold3), sum(Sc.CRPS[3]$fold3)),\nc(sum(Sc.log[4]$fold4), sum(Sc.MSE[4]$fold4), sum(Sc.quad[4]$fold4), sum(Sc.sph[4]$fold4), sum(Sc.DSS[4]$fold4), sum(Sc.CRPS[4]$fold4)),\nc(sum(Sc.log[5]$fold5), sum(Sc.MSE[5]$fold5), sum(Sc.quad[5]$fold5), sum(Sc.sph[5]$fold5), sum(Sc.DSS[5]$fold5), sum(Sc.CRPS[5]$fold5))\n)\nsum &lt;- c('Total', colSums(Res.sum)/nrow(train2))\n\nResult_  &lt;- data.frame(rbind(Result_, sum)) \n\n## Show results\ncolnames(Result_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\nResult_ &lt;- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] &lt;- 'Improvement'\n\nfor(i in 2:7){\n  Result_[,i] &lt;- as.numeric(Result_[,i])  \n  Result_[nb.fold+2,i] &lt;-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\nrownames(Result_) &lt;- NULL\nknitr::kable(Result_, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n\nTable 4.11: Prediction scores for the XGBoost model with telematics\n\n\nFold\nSc.log\nSc.MSE\nSc.quad\nSc.sph\nSc.DSS\nSc.CRPS\n\n\n\n\n1\n0.13388\n0.03521\n-0.93284\n-0.96351\n-3.06883\n0.03421\n\n\n2\n0.12792\n0.03257\n-0.93583\n-0.96499\n-3.15351\n0.03241\n\n\n3\n0.12411\n0.03254\n-0.93948\n-0.96697\n-3.12854\n0.03113\n\n\n4\n0.14126\n0.03757\n-0.92850\n-0.96108\n-3.03608\n0.03648\n\n\n5\n0.13492\n0.03562\n-0.93262\n-0.96335\n-3.05897\n0.03442\n\n\nTotal\n0.13241\n0.03470\n-0.93386\n-0.96398\n-3.08921\n0.03373\n\n\nImprovement\n-0.05138\n-0.01306\n-0.01611\n-0.00611\n-0.95232\n-0.00954\n\n\n\n\n\n\n\n\nOn the test set, we obtain:\n\n\nCode\ndtrain &lt;- xgb.DMatrix(data = data.matrix(train2[, paste(all.vars2)]), label = train2$NB_Claim)\nsetinfo(dtrain,\"base_margin\",log(train2$expo))\ndtest &lt;- xgb.DMatrix(data = data.matrix(test2[, paste(all.vars2)]), label = test2$NB_Claim)\nsetinfo(dtest,\"base_margin\",log(test2$expo))\nfolds &lt;-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n\n\n\n\nCode\nparam &lt;- list(\n  eta = 0.02337437,\n  max_depth = 26,\n  subsample = 0.8097923,\n  min_child_weight = 1,\n  booster = \"gbtree\",\n  objective = \"count:poisson\",\n  eval_metric = \"poisson-nloglik\")\n\nset.seed(133)\nfit.xgb &lt;- xgb.train(params = param,\n                     nrounds = 367,\n                     data = dtrain)\n\ntrain2$pred.xgb &lt;- predict(fit.xgb, dtrain, type='response')\n\n\ntest2$pred.xgb &lt;- predict(fit.xgb, dtest, type='response')\n\ntest2$pred.base &lt;- test2$pred.xgb\n\nResult_ &lt;- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ &lt;- cbind('XGBoost', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all &lt;- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n\nTable 4.12: Prediction scores for the XGBoost model with telematics\n\n\nModel\nSc.log\nSc.MSE\nSc.quad\nSc.sph\nSc.DSS\nSc.CRPS\n\n\n\n\nBase\n0.17674\n0.04545\n-0.92147\n-0.95981\n-2.19876\n0.04127\n\n\nGLM (trad.)\n0.17359\n0.04514\n-0.92197\n-0.95995\n-2.27716\n0.04099\n\n\nLASSO (optimal)\n0.15536\n0.04239\n-0.92624\n-0.96135\n-2.69596\n0.03866\n\n\nLASSO (parsimonious)\n0.15704\n0.04261\n-0.92579\n-0.96120\n-2.70016\n0.03889\n\n\nLASSO* (optimal)\n0.15373\n0.04209\n-0.92677\n-0.96154\n-2.62254\n0.03837\n\n\nLASSO* (parsimonious)\n0.15481\n0.04226\n-0.92642\n-0.96142\n-2.69780\n0.03855\n\n\nXGBoost\n0.12142\n0.03110\n-0.93907\n-0.96656\n-3.21523\n0.03081\n\n\n\n\n\n\n\n\n\n\nThe graph below illustrates the most critical variables in the XGBoost model for claim frequency. As indicated in the literature, the level of vehicle usage, represented here by the daily mileage, is the most crucial variable in the model. Driving experience, measured by the number of claim-free years, follows. To a lesser extent, a series of telematic variables also has predictive capability.\n\n\nCode\nimportance_matrix &lt;- xgb.importance(dimnames(dtrain)[[2]], model = fit.xgb)\nxgb.ggplot.importance(importance_matrix,top_n=15) + theme(text = element_text(size=15))\n\n\n\n\n\n\n\nAs we did for the GLM-Net approach, we can again analyze the residuals of the approach to see if the addition of telematic data eliminates the need to use protected variables. The graphs below show that XGBoost is even more effective than GLM-Net. Indeed, although the credit score still appears useful in modeling frequency, its effect is greatly diminished. The effects of age, gender, and marital status of the insured are also significantly reduced. Finally, we can even see that the effect of territory is also greatly minimized.\n\n\nCode\nmeaninv  &lt;- sum(train2$expo)/sum(train2$NB_Claim)\nmoy.xgb &lt;- sum(train2$pred.xgb)/sum(train2$NB_Claim)\n\ntemp2 &lt;- train2 %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Insured.age = pmin(Insured.age, 80),\n                Group = ceiling(Credit.score/25) * 25) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(expo),\n            expo2=sum(pred.xgb)) %&gt;% \n  mutate(freq = meaninv*NB_Claim/expo, \n         freq2 =  moy.xgb*NB_Claim/expo2)\n\nGraph_resCS &lt;- ggplot() + \n  geom_smooth(aes(x=Group, y=freq, weight = expo, color='Observed'),se=F, size=1, data=temp2) + \n  geom_smooth(aes(x=Group, y=freq2, weight = expo, color='Residuals'),se=F, size=1, data=temp2) + \n  labs(x = 'Credit Score',\n       y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  guides(color = guide_legend(title = \"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\nprint(Graph_resCS)\nsave(Graph_resCS, file = \"Data/Graph_resCS.rdata\")\n\n### Age of the insured\n\ntemp2 &lt;- train2 %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Insured.age = pmin(Insured.age, 80), \n                Group = ceiling(Insured.age/5) * 5) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(expo),\n            expo2=sum(pred.xgb)) %&gt;% \n  mutate(freq = meaninv*NB_Claim/expo, \n         freq2 =  moy.xgb*NB_Claim/expo2)\n\nGraph_resAge &lt;- ggplot() + \n  geom_smooth(aes(x=Group, y=freq, weight = expo, color='Observed'),se=F, size=1, data=temp2) + \n  geom_smooth(aes(x=Group, y=freq2, weight = expo, color='Residuals'),se=F, size=1, data=temp2) + \n  labs(x = 'Age of the insured',\n       y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  guides(color = guide_legend(title = \"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\nprint(Graph_resAge)\nsave(Graph_resAge, file = \"Data/Graph_resAge.rdata\")\n\n### Sex of the insured\n\n\ntemp &lt;- train2 %&gt;%\n  mutate(Var_ = Insured.sex) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(nbclaim = sum(NB_Claim),\n            expo = sum(expo),\n            expo2 = sum(pred.xgb)) %&gt;%\n  mutate(freq = nbclaim/expo,\n         freq2 = nbclaim/expo2)\n\ntemp$freq &lt;- temp$freq/temp$freq[1]\ntemp$freq2 &lt;- temp$freq2/temp$freq2[1]\n\nggplot() + #start plot by by plotting bars\n  geom_point(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=0.7) +\n  geom_point(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=0.7) +\n  labs(x = 'Sex of the insured', y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+\n  guides(color=guide_legend(title=\"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n### Marital Status\n\n\ntemp &lt;- train2 %&gt;%\n  mutate(Var_ = Marital) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(nbclaim = sum(NB_Claim),\n            expo = sum(expo),\n            expo2 = sum(pred.xgb)) %&gt;%\n  mutate(freq = nbclaim/expo, \n         freq2 = nbclaim/expo2)\n\ntemp$freq &lt;- temp$freq/temp$freq[1]\ntemp$freq2 &lt;- temp$freq2/temp$freq2[1]\n\nggplot() + #start plot by by plotting bars\n  geom_point(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=0.7) +\n  geom_point(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=0.7) +\n  labs(x = 'Marital status of the insured', y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  ylim(0.9, max(temp$freq, temp$freq2)*1.2)+\n  guides(color=guide_legend(title=\"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Insured's territory\n\ntemp &lt;- train2 %&gt;%\n  mutate(Var_ = Territory) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(nbclaim = sum(NB_Claim),\n            expo = sum(expo),\n            expo2 = sum(pred.xgb)) %&gt;%\n  mutate(freq = moy.xgb*nbclaim/expo2)\n\ntemp2 &lt;- train2 %&gt;%\n  mutate(Var_ = Territory) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(nbclaim = sum(NB_Claim),\n            expo = sum(expo)) %&gt;%\n  mutate(freq = meaninv*nbclaim/expo)\n\nGraph_resTerr &lt;- ggplot() + \n  geom_line(data = temp, aes(x = Var_, y = freq, group = 1, color='Residuals'), size=0.7) +\n  geom_point(data = temp, aes(x = Var_, y = freq, group = 1, color='Residuals'), size=0.7) +\n  geom_line(data = temp2, aes(x = Var_, y = freq, group = 1, color='Observed'), size=0.7) +\n  geom_point(data = temp2, aes(x = Var_, y = freq, group = 1, color='Observed'), size=0.7) +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  labs(x = 'Territory',\n       y = 'Relativity') +\n  scale_x_discrete(labels = NULL, breaks = NULL)+\n  guides(color=guide_legend(title=\"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\nprint(Graph_resTerr)\nsave(Graph_resTerr, file = \"Data/Graph_resTerr.rdata\")\n\n\n\n\n\n\n\n\n(a) Credit Score\n\n\n\n\n\n\n\n(b) Age of the Insured\n\n\n\n\n\n\n\n\n\n(c) Sex of the Insured\n\n\n\n\n\n\n\n(d) Marital Status of the Insured\n\n\n\n\n\n\n\n\n\n(e) Territory\n\n\n\n\nFigure 4.5: Observed Relativity vs. Residuals Relativity\n\n\n\n\n\nWe repeat the same exercise we did with the GLM-Net approach: fitting an XGBoost model on the residuals of the first XGBoost model. This will allow us to see if protected variables are capable of capturing trends in the approach’s residuals.\nThe table below shows the different scores for the XGBoost* model. There is only a gain on some of the indicated scores. Hence, telematics data can substitute protected variables based on the studied dataset.\n\n\nCode\nlibrary(xgboost)\nlibrary(Ckmeans.1d.dp)\nlibrary(SHAPforxgboost)\nlibrary(pacman)\n\nvar.sens &lt;- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\")    \n\ndtrain &lt;- xgb.DMatrix(data = data.matrix(train2[, paste(var.sens)]), label = train2$NB_Claim)\nsetinfo(dtrain,\"base_margin\",log(train2$pred.xgb))\nfolds &lt;-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n\nparam &lt;- list(\n  eta = 0.2215803,\n  max_depth = 50,\n  subsample = 0.7623535,\n  min_child_weight = 1,\n  booster = \"gbtree\",\n  objective = \"count:poisson\",\n  eval_metric = \"poisson-nloglik\")\n\nset.seed(533)\nfit.xgb2 &lt;- xgb.train(params = param,\n                      nrounds = 5,\n                      data = dtrain)\n\ndtest &lt;- xgb.DMatrix(data = data.matrix(test2[, paste(var.sens)]), label = test2$NB_Claim)\nsetinfo(dtest,\"base_margin\",log(test2$pred.xgb))\n\n\n\n\nCode\ntest2$pred.base &lt;- predict(fit.xgb2, dtest, type='response')\n\nResult_ &lt;- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ &lt;- cbind('XGBoost*', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all &lt;- rbind(Result_all, Result_)\n\nsave(Result_all, file='Data/ResultsSynth.Rda')\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n\nTable 4.13: Prediction scores for the XGBoost model with telematics\n\n\nModel\nSc.log\nSc.MSE\nSc.quad\nSc.sph\nSc.DSS\nSc.CRPS\n\n\n\n\nBase\n0.17674\n0.04545\n-0.92147\n-0.95981\n-2.19876\n0.04127\n\n\nGLM (trad.)\n0.17359\n0.04514\n-0.92197\n-0.95995\n-2.27716\n0.04099\n\n\nLASSO (optimal)\n0.15536\n0.04239\n-0.92624\n-0.96135\n-2.69596\n0.03866\n\n\nLASSO (parsimonious)\n0.15704\n0.04261\n-0.92579\n-0.96120\n-2.70016\n0.03889\n\n\nLASSO* (optimal)\n0.15373\n0.04209\n-0.92677\n-0.96154\n-2.62254\n0.03837\n\n\nLASSO* (parsimonious)\n0.15481\n0.04226\n-0.92642\n-0.96142\n-2.69780\n0.03855\n\n\nXGBoost\n0.12142\n0.03110\n-0.93907\n-0.96656\n-3.21523\n0.03081\n\n\nXGBoost*\n0.12373\n0.03250\n-0.93737\n-0.96579\n-3.29489\n0.03181\n\n\n\n\n\n\n\n\nAlthough the XGBoost model on residuals does not yield significant gains, the graph below illustrates the most critical protected variables in the XGBoost model. It shows that the insured’s credit score and territory are the most critical variables in this model.\n\n\nCode\nimportance_matrix &lt;- xgb.importance(dimnames(dtrain)[[2]], model = fit.xgb2)\nxgb.ggplot.importance(importance_matrix,top_n=15) + theme(text = element_text(size=15))"
  },
  {
    "objectID": "severity.html#preamble",
    "href": "severity.html#preamble",
    "title": "5  Data Summary",
    "section": "5.1 Preamble",
    "text": "5.1 Preamble\n\nChapter ObjectivePackagesData\n\n\nThe objective of this chapter is the same as that of Chapter 2. However, we will analyze severity.\n\n\nHere is the list of packages that will be used:\n\n\nCode\nlibrary(tidyverse)\nlibrary(vtable)\nlibrary(rpart)\nlibrary(repr)\nlibrary(rpart.plot)\nlibrary(rfCountData)\nlibrary(gam)\n\n\n\n\n\n\nCode\ndataS &lt;- read.csv('Data/Synthetic.csv')\ndata &lt;- dataS[dataS$AMT_Claim &gt; 0,]\ndata$M_Claim &lt;- data$AMT_Claim/data$NB_Claim\n\n# Modifications \ndata &lt;- data %&gt;%\n  mutate(Territory = as.factor(Territory)) %&gt;%\n  select(-c('Annual.pct.driven', 'Annual.miles.drive'))\n\ndata.select &lt;- data\n\n# Train-test et folds\nset.seed(123)\ntrain &lt;- data.select %&gt;% sample_frac(0.8, replace = FALSE)\ntest &lt;- data.select %&gt;% anti_join(train)"
  },
  {
    "objectID": "severity.html#sec-vartrad",
    "href": "severity.html#sec-vartrad",
    "title": "5  Data Summary",
    "section": "5.2 Traditional covariates",
    "text": "5.2 Traditional covariates\nDuration is often not considered in the modeling of severity so we don’t use it for the severity analysis.\n\n5.2.1 Sensitive information\n\nCredit scoreAge of the insuredSex of the insuredMarital StatusInsured’s territoryCorrelation\n\n\nWe conduct the same exercise with the credit score. In the graph below, we can see the relation between the credit score and claim severity in the portfolio: the points indicate the observed claims average severity, the size of the points measures the Number of claims for each group, and a trend curve (in red) for the average severity has been added.\nAs for the frequency, we observe a non-linear relationship, but overall, a better credit score implies a lower claim severity.\n\n\nCode\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(Credit.score/25) * 25) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + \n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Credit score',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #  xlim(0,1)+ylim(0, 0.06)+\n  theme_bw()\n\n\n\n\n\nFigure 5.1: Average claim severity vs. Credit Score\n\n\n\n\n\n\nThe age of the insured is also a sensitive variable in ratemaking. The graph below illustrates the relationship between age and claim severity. There is a negative relationship between the age and the average claim severity.\n\n\nCode\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(Insured.age/10) * 10) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) + \n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Age',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #  xlim(0,1)+ylim(0, 0.06)+\n  theme_bw()\n\n\n\n\n\nFigure 5.2: Average claim severity vs. Age\n\n\n\n\n\n\nThe graph below shows the distribution of gender in the portfolio (in bars), as well as the observed average claim severity (in red). We do not see a significant difference between means which can be confirmed using, e.g., the Welch two sample t-test. The \\(p\\)-value of the test is \\(0.2573\\) meaning that the null hypothesis, the true difference in means is equal to \\(0\\), cannot be rejected.\n\n\nCode\ntemp &lt;- train %&gt;%\n  mutate(Var_ = Insured.sex) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(Msev = mean(M_Claim),\n            expo = sum(Duration/365.25)) %&gt;%\n  mutate(sev = Msev/1)\n\ndiv &lt;- min(temp$expo/temp$sev)/1.5\n\nggplot(data = temp, aes(x = Var_, y = expo, linetype = \"Number of claims\")) + #start plot by by plotting bars\n  geom_bar(stat = \"identity\", alpha=0.5) + \n  geom_point(data = temp, aes(x = Var_, y = (sev)*div, group = 1), color='red', size=3,\n            inherit.aes = FALSE) +\n  geom_line(data = temp, aes(x = Var_, y = (sev)*div, group = 1, linetype = \"Claim severity\"), color='red', size=0.7,\n            inherit.aes = FALSE) +\n    labs(x = 'Sex',\n       y = 'Number of claims') +\n  scale_y_continuous(sec.axis = sec_axis(~./div, name = \"Claim severity\")) +\n  guides(linetype=guide_legend(title=\"\")) +\n  theme_classic()+\n  theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n\n\nFigure 5.3: Average claim severity vs. Sex\n\n\n\n\n\n\nThe marital status of the insured individual has a minimal impact on the average claim severity. A \\(95\\) percent confidence interval for the true difference in means is \\([-797, -12]\\), which is very close to including \\(0\\).\n\n\nCode\ntemp &lt;- train %&gt;%\n  mutate(Var_ = Marital) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(mSev = mean(M_Claim),\n            expo = sum(Duration/365.25)) %&gt;%\n  mutate(sev = mSev/1)\n\ndiv &lt;- min(temp$expo/temp$sev)/0.8\n\nggplot(data = temp, aes(x = Var_, y = expo, linetype = \"Number of claims\")) + #start plot by by plotting bars\n  geom_bar(stat = \"identity\", alpha=0.5) + \n  geom_point(data = temp, aes(x = Var_, y = (sev)*div, group = 1), color='red', size=3,\n             inherit.aes = FALSE) +\n  geom_line(data = temp, aes(x = Var_, y = (sev)*div, group = 1, linetype = \"Claim Severity\"), color='red', size=0.7,\n            inherit.aes = FALSE) +\n  labs(x = 'Marital status',\n       y = 'Number of claims') +\n  scale_y_continuous(sec.axis = sec_axis(~./div, name = \"Average claim Severity\")) +\n  guides(linetype=guide_legend(title=\"\")) +\n  theme_classic()+\n  theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n\n\n\n\nFigure 5.4: Average claim severity vs. Marital Status\n\n\n\n\n\n\n\n\nWe note that territory does not significantly impact a claim severity except for categories 74 and 91. For territory 74, this is mainly explained by a large claim ($104,074.9). Given these categories artificial nature, it is impossible to offer a rational explanation.\n\n\nCode\ntemp &lt;- train %&gt;%\n  mutate(Var_ = Territory) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(mSev = mean(M_Claim),\n            expo = sum(Duration/365.25)) %&gt;%\n  mutate(sev = mSev/1)\n\ndiv &lt;- min(temp$expo/temp$sev)/0.4\n\nggplot(data = temp, aes(x = Var_, y = expo, linetype = \"Number of claims\")) + #start plot by by plotting bars\n  geom_bar(stat = \"identity\", alpha=0.5) + \n  geom_point(data = temp, aes(x = Var_, y = (sev)*div, group = 1), color='red', size=1.5,\n             inherit.aes = FALSE) +\n  geom_line(data = temp, aes(x = Var_, y = (sev)*div, group = 1, linetype = \"Average Claim Severity\"), color='red', size=0.7,\n            inherit.aes = FALSE) +\n  labs(x = 'Territory',\n       y = 'Number of claims') +\n  scale_x_discrete(labels = NULL, breaks = NULL)+\n  scale_y_continuous(sec.axis = sec_axis(~./div, name = \"Average claim severity\")) +\n  guides(linetype=guide_legend(title=\"\")) +\n  theme_classic()+\n  theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n\n\nFigure 5.5: Average claim severity vs. Territory\n\n\n\n\n\n\nThe correlation matrix below indicates the level of dependence between each of the sensitive variables.\n\n\nCode\ntrad.vars  &lt;- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\")\n\ndata = data.matrix(train[, paste(trad.vars)])\ncorrelation_matrix &lt;- abs(cor(data))\n\nplt &lt;- ggplot(data = as.data.frame(as.table(correlation_matrix)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=4) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=10, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 10),\n          plot.title = element_text(size = 10),   # Adjust plot title font size\n          legend.text = element_text(size = 10),\n          legend.title = element_text(size=10))\nprint(plt)\n\n\n\n\n\nFigure 5.6: Correlation between sensible covariates\n\n\n\n\n\n\n\n\n\n5.2.2 Other covariates\n\nCar AgeUse of the carRegionYears without ClaimCorrelation\n\n\nThe graph below highlights the relationship between claim severity and vehicle age. A fairly clear link is observed: the newer a vehicle is, the more it costs.\n\n\nCode\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(Car.age/1) * 1) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Car age',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #  xlim(0,1)+ylim(0, 0.06)+\n  theme_bw()\n\n\n\n\n\nFigure 5.7: Average claim severity vs. Car Age\n\n\n\n\n\n\nIn the graph below, we see that the use of the vehicle has an impact on the average claim severity. We notice a very low exposure for two categories: farmer and commercial. If we focus only on the other two categories, the difference remains significant (the \\(p\\)-value is very close to \\(0\\)). However, this effect seems to disappear if we also take into account the car age.\n\n\nCode\ntemp &lt;- train %&gt;%\n  mutate(Var_ = Car.use) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(mSev = mean(M_Claim),\n            expo = sum(Duration/365.25)) %&gt;%\n  mutate(sev = mSev/1)\n\ndiv &lt;- min(temp$expo/temp$sev)/0.07\n\nggplot(data = temp, aes(x = Var_, y = expo, linetype='Number of claims')) + #start plot by by plotting bars\n  geom_bar(stat = \"identity\", alpha=0.5) + \n  geom_point(data = temp, aes(x = Var_, y = (sev)*div, group = 1), color='red', size=3,\n             inherit.aes = FALSE) +\n  geom_line(data = temp, aes(x = Var_, y = (sev)*div, group = 1, linetype = 'Claim Severity'), color='red', size=0.7,\n            inherit.aes = FALSE) +\n  labs(x = 'Car Use',\n       y = 'Number of claims') +\n  #scale_x_discrete(labels = NULL, breaks = NULL)+\n  scale_y_continuous(sec.axis = sec_axis(~./div, name = \"Claim Severity\"))  + \n  guides(linetype=guide_legend(title=\"\")) +\n  theme_classic()+\n  theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n\n\nFigure 5.8: Average claim severity vs. Car Use\n\n\n\n\n\n\nThe difference in severity between urban and rural claims is illustrated in the graph below. The \\(p\\) value is very close to \\(5\\%\\), a \\(95\\) percent confidence interval for the true difference in means is \\([-892, -1.49]\\), and we conclude there is no significant difference between these two regions.\n\n\nCode\ntemp &lt;- train %&gt;%\n  mutate(Var_ = Region) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(mSev = mean(M_Claim),\n            expo = sum(Duration/365.25)) %&gt;%\n  mutate(sev = mSev/1)\n\ndiv &lt;- min(temp$expo/temp$sev)/0.5\n\nggplot(data = temp, aes(x = Var_, y = expo, linetype='Number of claims')) + #start plot by by plotting bars\n  geom_bar(stat = \"identity\", alpha=0.5) + \n  geom_point(data = temp, aes(x = Var_, y = (sev)*div, group = 1), color='red', size=3,\n            inherit.aes = FALSE) +\n  geom_line(data = temp, aes(x = Var_, y = (sev)*div, group = 1, linetype='Claim Severity'), color='red', size=0.7,\n            inherit.aes = FALSE) +\n    labs(x = 'Region',\n       y = 'Number of claims') +\n  #scale_x_discrete(labels = NULL, breaks = NULL)+\n  scale_y_continuous(sec.axis = sec_axis(~./div, name = \"Claim Severity\")) + \n  guides(linetype=guide_legend(title=\"\")) +\n  theme_classic()+\n  theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n\n\n\n\nFigure 5.9: Average claim severity vs. Region\n\n\n\n\n\n\n\n\nThe graph below illustrates the relationship between the number of claim-free years and the average claim severity. The link between these two variables is unclear.\n\n\nCode\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = pmin(Years.noclaims, 60)) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Years without claim',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(0, 0.06)+\n  theme_bw()\n\n\n\n\n\nFigure 5.10: Average claim severity vs. Years without Claim\n\n\n\n\n\n\nBelow, we can observe the correlation that exists between the traditional segmentation variables.\n\n\nCode\ntrad.vars  &lt;- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\",\n                \"Car.use\", \"Region\", \"Car.age\", \"Years.noclaims\")\n\ndata = data.matrix(train[, paste(trad.vars)])\ncorrelation_matrix &lt;- abs(cor(data))\n\ncorr1 &lt;- correlation_matrix[6:9, 6:9]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=6) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\ncorr1 &lt;- correlation_matrix[1:5, 6:9]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=6) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\n\n\n\n\n\n\n\n(a) Other traditionnal covariates\n\n\n\n\n\n\n\n(b) With sensible covariates\n\n\n\n\nFigure 5.11: Correlation between traditional covariates"
  },
  {
    "objectID": "severity.html#sec-vartele",
    "href": "severity.html#sec-vartele",
    "title": "5  Data Summary",
    "section": "5.3 Telematic covariates",
    "text": "5.3 Telematic covariates\n\n5.3.1 Vehicle usage level\n\nAnnual Miles DrivenContract duration revisitedDays per weekCorrelation\n\n\nIn addition to the declared distance, we also have access to the actual distance traveled by the insured through the telematics device installed in the vehicle. It is increasingly recognized that the distance driven in a car can constitute a more precise measure of risk exposure than simply the duration of the insurance contract. However,\nThe graph below illustrates the claim severity as a function of the distance driven. There appears to be a small negative relationship between these two variables.\n\n\nCode\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ifelse(Total.miles.driven &lt; 20000, ceiling(Total.miles.driven/1000) * 1000, ceiling(Total.miles.driven/5000) * 5000)) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Total miles driven',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  xlim(0,25000)+\n  ylim(0, 7500)+\n  theme_bw()\n\n\n\n\n\n\n\nFigure 5.12: Average claim severity vs. Total Miles Driven\n\n\n\n\n\n\n\n\n\n\nCode\ntrain %&gt;%\n  dplyr::mutate(Miles.per.day = Total.miles.driven/Duration, \n                Duration.y = Duration/365.25, \n                Group = ceiling(Miles.per.day/7.5) * 7.5) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average miles driven per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  xlim(0,127.5)+\n  #ylim(0, 0.31)+\n  theme_bw()\n\n\n\n\n\n\n\nFigure 5.13: Average claim severity vs. average miles driven per day\n\n\n\n\n\n\n\n\nAnother intensity measure that may be close to the usage percentage is the average number of vehicle uses per week. The graph below shows no clear relationship between these two variables.\n\n\nCode\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(Avgdays.week/0.5) * 0.5) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of days per week',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(0, 0.31)+\n  theme_bw()\n\n\n\n\n\n\n\nFigure 5.14: Average claim severity vs. average number of days per week the car is used\n\n\n\n\n\n\n\n\nOnce again, we can observe the dependency between the covariates by looking at the results below.\n\n\nCode\ntrad.vars  &lt;- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\",\n                \"Miles.per.day\", \"Avgdays.week\")\n\ntrain &lt;- train %&gt;%\n  dplyr::mutate(Miles.per.day = Total.miles.driven/Duration)\n                \ndata = data.matrix(train[, paste(trad.vars)])\ncorrelation_matrix &lt;- abs(cor(data))\n\ncorr1 &lt;- correlation_matrix[6:7, 6:7]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=6) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\ncorr1 &lt;- correlation_matrix[1:5, 6:7]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=6) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\n\n\n\n\n\n\n\n(a) Telematic covariates\n\n\n\n\n\n\n\n(b) With sensible covariates\n\n\n\n\nFigure 5.15: Correlation between covariates\n\n\n\n\n\n\n\n\n5.3.2 Type of vehicle usage\nInstead of using the telematics device solely to measure vehicle usage, it is also possible to see if certain types of vehicle usage are indicators of a higher risk of claims. In this section, we will analyze certain telematics information that we classify as types of usage.\n\nDaysDays (2)Days (3)Week-endTrip DurationRush hoursCorrelation\n\n\nOne pertinent piece of information is to investigate whether vehicle usage on certain days of the week predicts a higher claim frequency. Seven covariates are available in the database, each indicating the percentage of vehicle usage on a particular day of the week. It is worth noting that the sum of the 7 percentages for each contract equals 1. Thus, high vehicle usage on a Saturday corresponds to a high percentage of usage for that day, necessarily implying that the other days will have smaller percentages.\nThe 7 graphs below illustrate the claim severity as a function of vehicle usage for each day of the week. The results obtained for each day are similar and seem to indicate that uniform vehicle usage across all 7 days (i.e., 1/7 = 14.2%) is the riskiest situation. Thus, vehicle usage for each day appears to signify something, but the information provided by these covariates likely needs transformation.\n\n\nCode\ndiv &lt;- 1/15 \n\nvar &lt;- 'Pct.drive.mon'\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=F, color='red', size=1) + \n  labs(x = 'Usage percentage per day',\n       y = 'Claims severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  ylim(0, 6000)+\n  theme_bw()\n\n\nvar &lt;- 'Pct.drive.tue'\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=F, color='red', size=1) + \n  labs(x = 'Usage percentage per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  ylim(0, 6000)+\n  theme_bw()\n\n\nvar &lt;- 'Pct.drive.wed'\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=F, color='red', size=1) + \n  labs(x = 'Usage percentage per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  ylim(0, 8000)+\n  theme_bw()\n\n\nvar &lt;- 'Pct.drive.thr'\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=F, color='red', size=1) + \n  labs(x = 'Usage percentage per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  ylim(0, 6000)+\n  theme_bw()\n\n\nvar &lt;- 'Pct.drive.fri'\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=F, color='red', size=1) + \n  labs(x = 'Usage percentage per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  ylim(0, 20000)+\n  theme_bw()\n\n\nvar &lt;- 'Pct.drive.sat'\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=F, color='red', size=1) + \n  labs(x = 'Usage percentage per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  ylim(0, 8000)+\n  theme_bw()\n\n\nvar &lt;- 'Pct.drive.sun'\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=F, color='red', size=1) + \n  labs(x = 'Usage percentage per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  ylim(0, 8000)+\n  theme_bw()\n\n\n\n\n\n\n\n\n(a) Monday\n\n\n\n\n\n\n\n(b) Tuesday\n\n\n\n\n\n\n\n(c) Wednesday\n\n\n\n\n\n\n\n\n\n(d) Thursday\n\n\n\n\n\n\n\n(e) Friday\n\n\n\n\n\n\n\n(f) Saturday\n\n\n\n\n\n\n\n\n\n(g) Sunday\n\n\n\n\nFigure 5.16: Average claim severity vs. percentage of use for each day\n\n\n\n\n\nIn light of the results obtained from the analysis of vehicle usage for each day of the week, it is appropriate to create new variables that may better represent the risk. We thus create the following variables:\n\nA variable identifying the maximum value of vehicle usage for each day;\n\nA variable identifying the minimum value of vehicle usage for each day;\n\nA variable measuring the difference between the maximum and minimum values, which have just been calculated. This variable can thus identify insured individuals who use their vehicle more on specific days, or conversely, insured individuals who typically refrain from using their vehicle on certain days of the week.\n\nFor each of these variables, the graph representing the relationship between claim severity is illustrated below. All graphs do not seem to point to a significant result.\n\n\nCode\ndf2 &lt;- train %&gt;%\n  mutate(max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin))\n\ndiv &lt;- 1/25 \nvar &lt;- 'max.day'\ndf2 %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Maximum usage percentage per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\ndiv &lt;- 1/75 \nvar &lt;- 'min.day'\ndf2 %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Minimum usage percentage per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\ndiv &lt;- 1/25 \nvar &lt;- 'max.min'\ndf2 %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Difference between percentages',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\n\n\n\n\n\n\n(a) Maximum Use\n\n\n\n\n\n\n\n(b) Minimum Use\n\n\n\n\n\n\n\n\n\n(c) Difference between maximum and minimum use\n\n\n\n\nFigure 5.17: Average claim severity vs. use for each day\n\n\n\n\n\nContinuing with the analysis related to the days of vehicle usage, two additional variables can also be explored:\n1) A variable identifying the day of the week when the vehicle is most used; 2) A variable identifying the day of the week when the vehicle is least used.\nThe graphs below attempt to verify if the claim frequency differs for those who use their vehicle more or less on specific days of the week. It is evident that Friday is the day when insured individuals tend to use their car more frequently. Conversely, Sunday is the day when the car appears to be used the least. No significant results.\n\n\nCode\ndf2 &lt;- train %&gt;%\n  mutate(max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin))\n\ndf2$Dayformax &lt;- factor(df2$Dayformax , levels=c(\"Monday\",\"Tuesday\",\"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"))\ndf2$Dayformin &lt;- factor(df2$Dayformin , levels=c(\"Monday\",\"Tuesday\",\"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"))\n\n\ntemp &lt;- df2 %&gt;%\n  mutate(Var_ = Dayformax) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(msev = mean(M_Claim),\n            expo = sum(Duration/365.25)) %&gt;%\n  mutate(sev = msev/1)\n\ndiv &lt;- min(temp$expo/temp$sev)/0.5\n\nggplot(data = temp, aes(x = Var_, y = expo, linetype='Number of claims')) + #start plot by by plotting bars\n  geom_bar(stat = \"identity\", alpha=0.5) + \n  geom_point(data = temp, aes(x = Var_, y = (sev)*div, group = 1), color='red', size=3,\n             inherit.aes = FALSE) +\n  geom_line(data = temp, aes(x = Var_, y = (sev)*div, group = 1, linetype='Claim severity'), color='red', size=0.7,\n            inherit.aes = FALSE) +\n  labs(x = 'Day of the week',\n       y = 'Number of claims') +\n  #scale_x_discrete(labels = NULL, breaks = NULL)+\n  scale_y_continuous(sec.axis = sec_axis(~./div, name = \"Claim severity\")) + \n  guides(linetype=guide_legend(title=\"\")) +\n  theme_classic()+\n  theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\ntemp &lt;- df2 %&gt;%\n  mutate(Var_ = Dayformin) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(msev = mean(M_Claim),\n            expo = sum(Duration/365.25)) %&gt;%\n  mutate(sev = msev/1)\n\ndiv &lt;- min(temp$expo/temp$sev)/0.2\n\nggplot(data = temp, aes(x = Var_, y = expo, linetype='Number of claims')) + #start plot by by plotting bars\n  geom_bar(stat = \"identity\", alpha=0.5) + \n  geom_point(data = temp, aes(x = Var_, y = (sev)*div, group = 1), color='red', size=3,\n             inherit.aes = FALSE) +\n  geom_line(data = temp, aes(x = Var_, y = (sev)*div, group = 1, linetype='Claim severity'), color='red', size=0.7,\n            inherit.aes = FALSE) +\n  labs(x = 'Day of the week',\n       y = 'Number of claims') +\n  #scale_x_discrete(labels = NULL, breaks = NULL)+\n  scale_y_continuous(sec.axis = sec_axis(~./div, name = \"Claim severity\")) + \n  guides(linetype=guide_legend(title=\"\")) +\n  theme_classic()+\n  theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n\n\n\n\n\n(a) Day with the maximum use\n\n\n\n\n\n\n\n(b) Day with the minimum use\n\n\n\n\nFigure 5.18: Claim severity vs. sse for each day\n\n\n\n\n\nA grouping of the vehicle usage variables for the days of the week has been performed directly in the database. Thus, the usage percentages for the days from Monday to Friday have been summed, and the usage for Saturday and Sunday has been summed into another variable. Knowing that the two covariates are complementary (since the sum of both equals 100%), only one of the two variables needs to be kept. No significant relationship.\n\n\nCode\nvar &lt;- 'Pct.drive.wkday'\ndiv &lt;- 1/25\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Usage percentage per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Pct.drive.wkend'\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(get(var)/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Usage percentage per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\n\n\n\n\n\n\n(a) Week day\n\n\n\n\n\n\n\n(b) Weekend day\n\n\n\n\nFigure 5.19: Average claim severity vs. percentage of use week day and week-end day\n\n\n\n\n\nThe common explanation for claim probability highlights the use of highways. The graphs of claim severity as a function of the percentage of trips exceeding 2, 3, or 4 hours are illustrated below. No significant impact.\n\n\nCode\nvar &lt;- 'Pct.drive.2hrs'\n\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Percent of driving',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Pct.drive.3hrs'\n\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Percent of driving',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Pct.drive.4hrs'\n\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Percent of driving',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\n\n\n\n\n\n\n(a) 2 hours\n\n\n\n\n\n\n\n(b) 3 hours\n\n\n\n\n\n\n\n\n\n(c) 4 hours\n\n\n\n\nFigure 5.20: Average claim severity vs. percentage of vehicule driven by XX hours\n\n\n\n\n\nAnother common hypothesis links the frequency of automobile insurance claims to traffic congestion. Thus, the proportion of trips made in traffic jams, whether in the morning or evening, is also available in the database. The graphs of severity linked to these two variables are illustrated below. No significant impact.\n\n\nCode\nvar &lt;- 'Pct.drive.rush.am'\n\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Percent of driving',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\nvar &lt;- 'Pct.drive.rush.pm'\n\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Percent of driving',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\n\n\n\n\n\n\n(a) AM Rush\n\n\n\n\n\n\n\n(b) PM Rush\n\n\n\n\nFigure 5.21: Average claim severity vs. Percentage of vehicule driven during rush hours\n\n\n\n\n\nThe dependency between each studied covariate is illustrated below.\n\n\nCode\ntrain2 &lt;- train %&gt;%\n  mutate(Miles.per.day = Total.miles.driven/Duration,\n         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),\n         expo = Duration/365.25)\n\n\ntrad.vars  &lt;- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\",\n                \"Pct.drive.mon\", \"Pct.drive.tue\", \"Pct.drive.wed\", \"Pct.drive.thr\", \"Pct.drive.fri\", \"Pct.drive.sat\", \"Pct.drive.sun\")\n\ndata = data.matrix(train2[, paste(trad.vars)])\ncorrelation_matrix &lt;- abs(cor(data))\n\ncorr1 &lt;- correlation_matrix[6:12, 6:12]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=4) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\ncorr1 &lt;- correlation_matrix[1:5, 6:12]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=6) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\n\ntrad.vars  &lt;- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\",\n                \"max.day\", \"min.day\", \"Dayformax\", \"Dayformin\", \"Pct.drive.wkend\", \n                \"Pct.drive.2hrs\", \"Pct.drive.3hrs\", \"Pct.drive.4hrs\",\n                \"Pct.drive.rush.am\", \"Pct.drive.rush.pm\")\n\ndata = data.matrix(train2[, paste(trad.vars)])\ncorrelation_matrix &lt;- abs(cor(data))\n\ncorr1 &lt;- correlation_matrix[6:15, 6:15]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=4) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\ncorr1 &lt;- correlation_matrix[1:5, 6:15]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=6) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\n\n\n\n\n\n\n\n(a) Telematic covariates\n\n\n\n\n\n\n\n(b) With sensible covariates\n\n\n\n\n\n\n\n\n\n(c) Telematic covariates\n\n\n\n\n\n\n\n(d) With sensible covariates\n\n\n\n\nFigure 5.22: Correlation between covariates\n\n\n\n\n\n\n\n\n5.3.3 Driving behavior\nBeyond the intensity of vehicle usage, telematics devices also allow for the compilation of various statistics on driving behavior. This primarily includes sudden braking, rapid acceleration, or high-speed turns (both left and right). In this final part of the analysis of segmentation variables available in the database, we will therefore work on analyzing and transforming these variables.\n\nBrakesAccelerationsRight turnsLeft turnsCorrelation\n\n\nIn the database, we have access to a series of variables counting the number of abrupt braking events, with decelerations of 6mph, 8mph, 9mph, 11mph, 12mph, and 14mph per 1000 miles traveled. As indicated in the description, the number of abrupt braking events is normalized by the distance traveled and not by the number of insured days. Since we choose to use the number of insured days as a measure of exposure to risk, a transformation of these variables is necessary.\nThe 6 graphs below illustrate the relationship between the number of daily abrupt braking events and claim severity. For the first 4 scenarios, a weak relationship can be observed.\n\n\nCode\nvar &lt;- 'Brake.06miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of brakes per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Brake.08miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of brakes per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Brake.09miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of brakes per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\nvar &lt;- 'Brake.11miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of brakes per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Brake.12miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of brakes per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Brake.14miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of brakes per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\n\n\n\n\n\n\n(a) Brakes of 6 mph\n\n\n\n\n\n\n\n(b) Brakes of 8 mph\n\n\n\n\n\n\n\n(c) Brakes of 9 mph\n\n\n\n\n\n\n\n\n\n(d) Brakes of 11 mph\n\n\n\n\n\n\n\n(e) Brakes of 12 mph\n\n\n\n\n\n\n\n(f) Brakes of 14 mph\n\n\n\n\nFigure 5.23: Average claim severity vs. Average number of brakes\n\n\n\n\n\nSimilarly to braking events, we need to convert accelerations, which are normalized by miles driven, into average daily accelerations. The 6 graphs below illustrate the relationship between the average acceleration and claim severity.\nNo significant impact.\n\n\nCode\nvar &lt;- 'Accel.06miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of accelerations per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Accel.08miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of accelerations per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Accel.09miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of accelerations per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Accel.11miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of accelerations per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Accel.12miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of accelerations per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Accel.14miles'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of accelerations per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\n\n\n\n\n\n\n(a) Accelerations of 6 mph\n\n\n\n\n\n\n\n(b) Accelerations of 8 mph\n\n\n\n\n\n\n\n(c) Accelerations of 9 mph\n\n\n\n\n\n\n\n\n\n(d) Accelerations of 11 mph\n\n\n\n\n\n\n\n(e) Accelerations of 12 mph\n\n\n\n\n\n\n\n(f) Accelerations of 14 mph\n\n\n\n\nFigure 5.24: Average claim severity vs. Average number of accelerations\n\n\n\n\n\nNo clear impact.\n\n\nCode\nvar &lt;- 'Right.turn.intensity08'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of right turns per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Right.turn.intensity09'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of right turns per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Right.turn.intensity10'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of right turns per day',\n       y = 'Claims frequency') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Right.turn.intensity11'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of right turns per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\nvar &lt;- 'Right.turn.intensity12'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of right turns per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\n\n\n\n\n\n\n(a) Right turns of 8 mph\n\n\n\n\n\n\n\n(b) Right turns of 9 mph\n\n\n\n\n\n\n\n(c) Right turns of 10 mph\n\n\n\n\n\n\n\n\n\n(d) Right turns of 11 mph\n\n\n\n\n\n\n\n(e) Right turns of 12 mph\n\n\n\n\nFigure 5.25: Average claim severity vs. Average number of right turns\n\n\n\n\n\nNo clear impact.\n\n\nCode\nvar &lt;- 'Left.turn.intensity08'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of left turns per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\nvar &lt;- 'Left.turn.intensity09'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of left turns per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\nvar &lt;- 'Left.turn.intensity10'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of left turns per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\nvar &lt;- 'Left.turn.intensity11'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of left turns per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\nvar &lt;- 'Left.turn.intensity12'\ndf2 &lt;- train %&gt;% mutate(VAR = get(var))\nq99 &lt;- quantile(df2$VAR, 0.99)\ndiv &lt;- q99/15\ntrain %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25,\n                VAR = ifelse(get(var) &gt; q99, q99, get(var)),\n                Group = ceiling(VAR/div) * div) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=mean(M_Claim),\n            expo=sum(Duration.y)) %&gt;% \n  mutate(sev = M_Claim/1) %&gt;%\n  ggplot(aes(x=Group, y=sev)) + \n  geom_point(aes(size=expo), color='black') + scale_size_continuous(range = c(1,4)) +\n  geom_smooth(aes(weight = expo),se=T, color='red', size=1) + \n  labs(x = 'Average number of left turns per day',\n       y = 'Claim severity') +\n  guides(size = guide_legend(title = \"Number of claims\")) +\n  #xlim(0,40000)+\n  #ylim(-0.01, 0.06)+\n  theme_bw()\n\n\n\n\n\n\n\n\n(a) Left turns of 8 mph\n\n\n\n\n\n\n\n(b) Left turns of 9 mph\n\n\n\n\n\n\n\n(c) Left turns of 10 mph\n\n\n\n\n\n\n\n\n\n(d) Left turns of 11 mph\n\n\n\n\n\n\n\n(e) Left turns of 12 mph\n\n\n\n\nFigure 5.26: Average claim severity vs. Average number of left turns\n\n\n\n\n\n\n\nCode\ntrain2 &lt;- train %&gt;%\n  mutate(Miles.per.day = Total.miles.driven/Duration,\n         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),\n         expo = Duration/365.25)\n\n\ntrad.vars  &lt;- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\",\n                \"Accel.06miles\", \"Accel.08miles\", \"Accel.09miles\", \"Accel.11miles\", \"Accel.12miles\", \"Accel.14miles\", \n               \"Brake.06miles\", \"Brake.08miles\", \"Brake.09miles\", \"Brake.11miles\", \"Brake.12miles\", \"Brake.14miles\", \n               \"Left.turn.intensity08\", \"Left.turn.intensity09\", \"Left.turn.intensity10\", \"Left.turn.intensity11\", \"Left.turn.intensity12\",\n               \"Right.turn.intensity08\", \"Right.turn.intensity09\", \"Right.turn.intensity10\", \"Right.turn.intensity11\", \"Right.turn.intensity12\")\n\ndata = data.matrix(train2[, paste(trad.vars)])\ncorrelation_matrix &lt;- abs(cor(data))\n\ncorr1 &lt;- correlation_matrix[6:17, 6:17]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=4) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\ncorr1 &lt;- correlation_matrix[1:5, 6:17]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=6) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\n\ncorr1 &lt;- correlation_matrix[18:27, 18:27]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=4) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\ncorr1 &lt;- correlation_matrix[1:5, 18:27]\n\nggplot(data = as.data.frame(as.table(corr1)), aes(x = Var1, y = Var2, fill = Freq)) +\n    geom_tile() +\n    scale_fill_gradientn(colors = colorRampPalette(c(\"gray\", \"blue\", \"red\"))(50), name = \"Correlation\") +\n    geom_text(aes(label = round(Freq, 2)), color = \"white\", size=6) +\n    #scale_fill_continuous(na.value = 'white') +\n    theme_minimal() +\n    theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(size=13, angle = 45, hjust = 1), \n          axis.text.y = element_text(size = 13),\n          plot.title = element_text(size = 13),   # Adjust plot title font size\n          legend.text = element_text(size = 13),\n          legend.title = element_text(size=13))\n\n\n\n\n\n\n\n\n(a) Telematic covariates\n\n\n\n\n\n\n\n(b) With sensible covariates\n\n\n\n\n\n\n\n\n\n(c) Telematic covariates\n\n\n\n\n\n\n\n(d) With sensible covariates\n\n\n\n\nFigure 5.27: Correlation between covariates"
  },
  {
    "objectID": "severityVarTrad.html#preamble",
    "href": "severityVarTrad.html#preamble",
    "title": "6  Traditional Covariates",
    "section": "6.1 Preamble",
    "text": "6.1 Preamble\n\nChapter ObjectivePackagesData\n\n\nUsing only traditional covariates, the objective of this chapter is to propose various statistical models for estimation and variable selection to predict the number of claims. More specifically, the following model families will be examined:\n\nBasic GLM,\n\nGLM family, including elastic-net,\n\nXGBoost.\n\nAs mentioned in the theory review chapter, to compare models and strike a balance between bias and variance while avoiding overfitting, an interesting approach is to assess the prediction quality of models when applied to new data. The following R script presents a function for calculating various scores:\nTo analyze severities, we define two scores:\n\nthe negative loglikelihood based on the Gamma distribution with shape parameter = 1/phi and scale = (mu)(phi), where mu is the expected value and phi is the dispersion parameter;\n\nthe mean squared error (MSE) divided by 1,000,000.\n\n\n\nCode\nScore.pred.sev &lt;- function(mu, x, phi) {\n  Sc.log  &lt;- -sum(dgamma(x, shape = 1/phi, scale = mu*phi, log=TRUE))\n  Sc.MSE  &lt;- sum((x - mu)^2)/1000000\n  return(c(Sc.log, Sc.MSE))\n}\n\n\n\n\nThe analyses in this chapter will be conducted using the same data as in the previous chapter. Here is the list of packages that will be used:\n\n\nCode\nlibrary(tidyverse)\nlibrary(vtable)\nlibrary(rpart)\nlibrary(repr)\nlibrary(rpart.plot)\n#library(rfCountData)\nlibrary(gam)\nlibrary(sjPlot)\nlibrary(glmnet)\n\n\n\n\n\n\nCode\ndataS &lt;- read.csv('Data/Synthetic.csv')\n#dataS &lt;- read.csv(\"~/Library/CloudStorage/Dropbox/AquiLoss/CAS/Telematics/telematics_syn-032021.csv\")\n\ndata &lt;- dataS[dataS$AMT_Claim &gt; 0,]\ndata$M_Claim &lt;- data$AMT_Claim/data$NB_Claim\n\n# Modifications \ndata &lt;- data %&gt;%\n  mutate(Territory = as.factor(Territory)) %&gt;%\n  select(-c('Annual.pct.driven', 'Annual.miles.drive'))\n\ndata.select &lt;- data\n\n# Train-test et folds\nset.seed(123)\ntrain &lt;- data.select %&gt;% sample_frac(0.8, replace = FALSE)\ntest &lt;- data.select %&gt;% anti_join(train)\ntest &lt;- test[-640,]\n\ntrain2 &lt;- train %&gt;%\n  mutate(Miles.per.day = Total.miles.driven/Duration,\n         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),\n         expo = Duration/365.25)\n\ntransform.fct &lt;- function(var){\n  df &lt;- train2 %&gt;% mutate(var_ = get(var)*Total.miles.driven/(1000*Duration))\n  q99 &lt;- quantile(df$var_, 0.99)\n  df &lt;- df %&gt;% mutate(var_ = ifelse(var_ &gt; q99, q99, var_))\n  #colnames(df)[ncol(df)] &lt;- paste0(var, '_')\n  return(df)\n}\n\ntrain2 &lt;- transform.fct(\"Brake.06miles\")\ntrain2 &lt;- transform.fct(\"Brake.08miles\")\ntrain2 &lt;- transform.fct(\"Brake.09miles\")\ntrain2 &lt;- transform.fct(\"Brake.11miles\")\ntrain2 &lt;- transform.fct(\"Brake.14miles\")\ntrain2 &lt;- transform.fct(\"Accel.06miles\")\ntrain2 &lt;- transform.fct(\"Accel.08miles\")\ntrain2 &lt;- transform.fct(\"Accel.09miles\")\ntrain2 &lt;- transform.fct(\"Accel.11miles\")\ntrain2 &lt;- transform.fct(\"Accel.12miles\")\ntrain2 &lt;- transform.fct(\"Accel.14miles\")\ntrain2 &lt;- transform.fct(\"Left.turn.intensity08\")\ntrain2 &lt;- transform.fct(\"Left.turn.intensity09\")\ntrain2 &lt;- transform.fct(\"Left.turn.intensity10\")\ntrain2 &lt;- transform.fct(\"Left.turn.intensity11\")\ntrain2 &lt;- transform.fct(\"Left.turn.intensity12\")\ntrain2 &lt;- transform.fct(\"Right.turn.intensity08\")\ntrain2 &lt;- transform.fct(\"Right.turn.intensity09\")\ntrain2 &lt;- transform.fct(\"Right.turn.intensity10\")\ntrain2 &lt;- transform.fct(\"Right.turn.intensity11\")\ntrain2 &lt;- transform.fct(\"Right.turn.intensity12\")\n\n# Create folds\nnb.fold &lt;- 5\nfold &lt;- sample(1:nb.fold, nrow(train2), replace = TRUE)\ntrain2$fold &lt;- fold\n\n##\n\ntest2 &lt;- test %&gt;%\n  mutate(Miles.per.day = Total.miles.driven/Duration,\n         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),\n         expo = Duration/365.25)\n\ntransform.fct &lt;- function(var){\n  df &lt;- test2 %&gt;% mutate(var_ = get(var)*Total.miles.driven/(1000*Duration))\n  q99 &lt;- quantile(df$var_, 0.99)\n  df &lt;- df %&gt;% mutate(var_ = ifelse(var_ &gt; q99, q99, var_))\n  #colnames(df)[ncol(df)] &lt;- paste0(var, '_')\n  return(df)\n}\n\ntest2 &lt;- transform.fct(\"Brake.06miles\")\ntest2 &lt;- transform.fct(\"Brake.08miles\")\ntest2 &lt;- transform.fct(\"Brake.09miles\")\ntest2 &lt;- transform.fct(\"Brake.11miles\")\ntest2 &lt;- transform.fct(\"Brake.14miles\")\ntest2 &lt;- transform.fct(\"Accel.06miles\")\ntest2 &lt;- transform.fct(\"Accel.08miles\")\ntest2 &lt;- transform.fct(\"Accel.09miles\")\ntest2 &lt;- transform.fct(\"Accel.11miles\")\ntest2 &lt;- transform.fct(\"Accel.12miles\")\ntest2 &lt;- transform.fct(\"Accel.14miles\")\ntest2 &lt;- transform.fct(\"Left.turn.intensity08\")\ntest2 &lt;- transform.fct(\"Left.turn.intensity09\")\ntest2 &lt;- transform.fct(\"Left.turn.intensity10\")\ntest2 &lt;- transform.fct(\"Left.turn.intensity11\")\ntest2 &lt;- transform.fct(\"Left.turn.intensity12\")\ntest2 &lt;- transform.fct(\"Right.turn.intensity08\")\ntest2 &lt;- transform.fct(\"Right.turn.intensity09\")\ntest2 &lt;- transform.fct(\"Right.turn.intensity10\")\ntest2 &lt;- transform.fct(\"Right.turn.intensity11\")\ntest2 &lt;- transform.fct(\"Right.turn.intensity12\")"
  },
  {
    "objectID": "severityVarTrad.html#basic-glm-models",
    "href": "severityVarTrad.html#basic-glm-models",
    "title": "6  Traditional Covariates",
    "section": "6.2 Basic GLM Models",
    "text": "6.2 Basic GLM Models\n\nSingle interceptCategorical covariatesEstimated Parameters\n\n\nA baseline model corresponding to a Generalized Linear Model (GLM) with intercept and predicting for each contract only the observed mean multiplied by the observed frequency is used as a point of comparison.\n\n\nCode\n## Model on each fold\nResult_  &lt;- data.frame()\nResult2_  &lt;- data.frame()\nfor(i in 1:nb.fold) {\n  learn &lt;- train2[train2$fold != i,]\n  valid &lt;- train2[train2$fold == i,]\n  \n  mean &lt;- sum(learn$AMT_Claim)/sum(learn$NB_Claim) \n  variance &lt;- sd(learn$AMT_Claim)^2\n  phi &lt;- variance/mean(learn$AMT_Claim)^2\n  learn$pred.base &lt;- mean*learn$NB_Claim\n  valid$pred.base &lt;- mean*valid$NB_Claim\n  \n  Result_ &lt;- rbind(Result_, c(i, Score.pred.sev(valid$pred.base, valid$AMT_Claim, phi)/nrow(valid)))\n  Result2_ &lt;- rbind(Result2_, c(i, Score.pred.sev(valid$pred.base, valid$AMT_Claim, phi)))\n}\n\n## Show results\ncolnames(Result_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\")\ncolnames(Result2_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\")\ntot &lt;- colSums(Result2_)/nrow(train2)\ntot$Fold &lt;- 'Total'\nResult_ &lt;- rbind(Result_ , tot)\n\nResult.base &lt;- Result_  \nBase &lt;- Result.base[nb.fold+1,]\n\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n\n\nTable 6.1: Prediction scores for the base model (severity)\n\n\nFold\nSc.log\nSc.MSE\n\n\n\n\n1\n9.28096\n48.72725\n\n\n2\n9.28378\n21.80556\n\n\n3\n9.30844\n31.17175\n\n\n4\n9.34049\n30.61554\n\n\n5\n9.26105\n18.96634\n\n\nTotal\n9.29513\n30.43693\n\n\n\n\n\n\n\n\nThe model is, therefore, estimated on the entire train database, and the predictions are made on the test database, which was not used during the calibration phase.\n\n\nCode\nmean &lt;- sum(train2$AMT_Claim)/sum(train2$NB_Claim) \nvariance &lt;- sd(train2$AMT_Claim)^2\nphi &lt;- variance/mean(train2$AMT_Claim)^2 \n  \ntest2$pred.base &lt;- mean*test2$NB_Claim\n\nResult_ &lt;- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ &lt;- cbind('Base', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all &lt;- Result_\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n\n\nTable 6.2: Prediction scores for the base model (testing set) (severity)\n\n\nModel\nSc.log\nSc.MSE\n\n\n\n\nBase\n9.29504\n21.82679\n\n\n\n\n\n\n\n\n\n\nA first regression approach is attempted using only the traditional categorical variables, namely:\n\nGender,\n\nMarital status,\n\nVehicle usage,\n\nRegion.\n\nEven though territory should also be considered since it consists of more than fifty different factors, it will not be integrated into the model immediately.\nAs we saw in the overview of variables in a previous section, the insured gender did not appear to be an important variable for predicting the number of claims. This GLM approach confirms this observation. Therefore, this variable is excluded from the model. In the table below, we can see the impact of adding traditional variables on the prediction quality.\nBelow are the prediction scores of the model with all categorical covariates. As expected, the addition of segmentation variables improves the prediction scores compared to the simple baseline model with only an intercept.\n\n\nCode\n## Model \nscore.base &lt;- as.formula(M_Claim ~ 1)\nscore.glm &lt;- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region )\n\n## Model on each fold\nResult_  &lt;- data.frame()\nResult2_  &lt;- data.frame()\nfor(i in 1:nb.fold) {\n  learn &lt;- train2[train2$fold != i,]\n  valid &lt;- train2[train2$fold == i,]\n  glm.fit &lt;- glm(score.glm, family = Gamma(link = \"log\"), data = learn)\n  \n  learn$pred.base &lt;- predict(glm.fit, newdata=learn, type='response')*learn$NB_Claim\n  valid$pred.base &lt;- predict(glm.fit, newdata=valid, type='response')*valid$NB_Claim\n  phi &lt;- summary(glm.fit)$dispersion\n  \n  Result_ &lt;- rbind(Result_, c(i, Score.pred.sev(valid$pred.base, valid$AMT_Claim, phi)/nrow(valid)))\n  Result2_ &lt;- rbind(Result2_, c(i, Score.pred.sev(valid$pred.base, valid$AMT_Claim, phi)))\n}\n\n## Model on all data from train\nglm.base &lt;- glm(score.base, family = Gamma(link = \"log\"), data = train2)\nglm.fit &lt;- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\ntrain2$pred.glm1 &lt;- predict(glm.fit, newdata=train2, type='response')*train2$NB_Claim\nResult.glm1 &lt;- Result_  \n\n## Show results\ncolnames(Result_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\")\ncolnames(Result2_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\")\ntot &lt;- colSums(Result2_)/nrow(train2)\ntot$Fold &lt;- 'Total'\nResult_ &lt;- rbind(Result_ , tot)\nResult_ &lt;- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] &lt;- 'Improvement'\n\nfor(i in 2:3){\n  Result_[nb.fold+2,i] &lt;-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) &lt;- NULL\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n\n\nTable 6.3: Prediction scores for the GLM1 model (severity)\n\n\nFold\nSc.log\nSc.MSE\n\n\n\n\n1\n9.26014\n47.85313\n\n\n2\n9.25790\n22.01984\n\n\n3\n9.28552\n31.20756\n\n\n4\n9.31168\n30.29863\n\n\n5\n9.23947\n18.59595\n\n\nTotal\n9.27110\n30.17198\n\n\nImprovement\n-0.02403\n-0.26495\n\n\n\n\n\n\n\n\n\n\nCode\nscore.glm &lt;- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region )\n\nglm.fit &lt;- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\ntest2$pred.base &lt;- predict(glm.fit, newdata=test2, type='response')*test2$NB_Claim\nphi &lt;- summary(glm.fit)$dispersion\n\nResult_ &lt;- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ &lt;- cbind('GLM (trad.)', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all &lt;- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n\n\nTable 6.4: Prediction scores for the GLM model with traditional covariates (testing set) (severity)\n\n\nModel\nSc.log\nSc.MSE\n\n\n\n\nBase\n9.29504\n21.82679\n\n\nGLM (trad.)\n9.27546\n21.77177\n\n\n\n\n\n\n\n\n\n\nThe table below shows the estimators obtained for the GLM-Gamma approach and compares them with the baseline model having only an intercept.\n\n\nCode\n## Model \nscore.base &lt;- as.formula(M_Claim ~ 1)\nscore.glm &lt;- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region)\n\n## Model on all data from train\nglm.base &lt;- glm(score.base, family = Gamma(link = \"log\"), data = train2)\nglm.fit &lt;- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ntab_model(glm.base, glm.fit, transform = NULL)\n\n\n\n\nTable 6.5: Estimated parameters for the GLM1 model (severity)\n\n\n \nM Claim\nM Claim\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n8.12\n8.07 – 8.18\n&lt;0.001\n7.76\n7.46 – 8.09\n&lt;0.001\n\n\nInsured sex [Male]\n\n\n\n0.07\n-0.04 – 0.18\n0.196\n\n\nMarital [Single]\n\n\n\n0.14\n0.03 – 0.26\n0.013\n\n\nCar use [Commute]\n\n\n\n0.28\n-0.03 – 0.56\n0.061\n\n\nCar use [Farmer]\n\n\n\n-0.93\n-1.65 – -0.06\n0.019\n\n\nCar use [Private]\n\n\n\n-0.01\n-0.32 – 0.27\n0.936\n\n\nRegion [Urban]\n\n\n\n0.12\n-0.03 – 0.26\n0.112\n\n\nObservations\n3091\n3091\n\n\nR2 Nagelkerke\n0.000\n0.045"
  },
  {
    "objectID": "severityVarTrad.html#glm-net",
    "href": "severityVarTrad.html#glm-net",
    "title": "6  Traditional Covariates",
    "section": "6.3 GLM-Net",
    "text": "6.3 GLM-Net\nSome traditional continuous segmentation variables are available:\n\nCredit score,\n\nAge of the insured,\n\nAge of the vehicle,\n\nNumber of claim-free years.\n\nFurthermore, the territory is also treated as a continuous variable.\nAn approach using Generalized Additive Models (GAM) theory will first be introduced for all these continuous variables. This will allow us to observe the general form of the covariate to explain the number of claims. A parametric form will then be proposed to achieve the best possible correspondence with the spline obtained by the GAM.\n\n6.3.1 Parametric transformation of continuous covariates\n\nCredit ScoreAge of the insuredAge of the carYears without claimsTerritory\n\n\nThe first covariate studied is the credit score. We include all categorical variables in the analysis and apply a spline approach with a GAM. The spline analysis indicates that the following parametric form appears to be appropriate for capturing the relationship:\n\\[s(Credit.Score) \\approx Credit.Score + Credit.Score^2\\]\n\n\n\n\n\nFigure 6.1: Smoothing of the credit score (severity)\n\n\n\n\n\n\nA spline to examine the relationship between the age of the insured and the claim deverity has also been produced. The most appropriate parametric form is as follows: \\[s(Insured.age) \\approx Insured.age  + Insured.age^2\\]\n\n\n\n\n\nFigure 6.2: Smoothing of the age of the insured (severity)\n\n\n\n\n\n\nThe link between the response variable and the car age is approximated by\n\\[s(Car.age) \\approx Car.age + Car.age^2 + Car.age^3\\]\n\n\n\n\n\nFigure 6.3: Smoothing of the age of the car (severity)\n\n\n\n\n\n\nFinally, the link between the response variable and the number of year(s) without claims is best approximated by\n\\[s(Years.noclaims) \\approx Years.noclaims + Years.noclaims^2 + Years.noclaims^3\\]\n\n\n\n\n\nFigure 6.4: Smoothing of years without claim (severity)\n\n\n\n\n\n\nWe proceed with the covariate Territory as we did for the analysis of the frequency. The parametric function is:\n\\[s(terr.code) \\approx terr.code + terr.code^2 + terr.code^3\\]\n\n\n\n\n\nFigure 6.5: Smoothing of the territories (encoded) (severity)\n\n\n\n\n\n\n\n\n\n6.3.2 Fitting the GLM-Net model\n\nOptimal valueParsimonious modelCategorical covariatesContinuous covariates\n\n\nThe parameters of the GLM-net were calibrated using cross-validation to obtain the model hyperparameters. Using these values, we can calculate the prediction scores of the model based on all covariates.\n\n\nCode\nglm.score &lt;- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region\n                                  + Credit.score +  I(Credit.score^2) \n                                  + Insured.age + I(Insured.age^2) \n                                  + Car.age + I(Car.age^2) + I(Car.age^3)\n                                  + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nResult_  &lt;- data.frame()\nResult2_  &lt;- data.frame()\nfor(i in 1:nb.fold) {\n    learn &lt;- train2[train2$fold != i,]\n    valid &lt;- train2[train2$fold == i,]\n    \n    matrix.x &lt;- model.matrix(glm.score, data=learn)[,-1]\n    y &lt;- learn$M_Claim\n\n    lambda.min &lt;- 0.01995262\n    lambda.1se &lt;- 0.07943282\n    \n    lambda.select &lt;- lambda.min\n    fit &lt;- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, alpha = 0.6, lambda = lambda.select)\n    learn$pred &lt;- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*learn$NB_Claim\n    \n    \n    matrix.x &lt;- model.matrix(glm.score, data=valid)[,-1]\n    y &lt;- valid$M_Claim\n\n    valid$pred &lt;- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*valid$NB_Claim\n    variance &lt;- (sum((learn$AMT_Claim - learn$pred)^2)/(nrow(learn) - length(fit$beta)))\n    phi &lt;- variance/mean(learn$AMT_Claim)^2\n    \n    Result_ &lt;- rbind(Result_, c(i, Score.pred.sev(valid$pred, valid$AMT_Claim, phi)/nrow(valid)))\n    Result2_ &lt;- rbind(Result2_, c(i, Score.pred.sev(valid$pred, valid$AMT_Claim, phi)))\n}\n\n\n## Show results\ncolnames(Result_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\")\ncolnames(Result2_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\")\ntot &lt;- colSums(Result2_)/nrow(train2)\ntot$Fold &lt;- 'Total'\nResult_ &lt;- rbind(Result_ , tot)\nResult_ &lt;- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] &lt;- 'Improvement'\n\nfor(i in 2:3){\n  Result_[nb.fold+2,i] &lt;-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) &lt;- NULL\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n\n\nTable 6.6: Prediction scores for the GLM-net model (alpha=1) (severity)\n\n\nFold\nSc.log\nSc.MSE\n\n\n\n\n1\n9.20427\n45.11001\n\n\n2\n9.22953\n20.98561\n\n\n3\n9.24011\n28.93837\n\n\n4\n9.27740\n28.99576\n\n\n5\n9.21277\n17.93211\n\n\nTotal\n9.23286\n28.56121\n\n\nImprovement\n-0.06228\n-1.87573\n\n\n\n\n\n\n\n\nThe same model can be used to compute the scores on the test set.\n\n\nCode\nglm.score &lt;- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region\n                                  + Credit.score +  I(Credit.score^2) \n                                  + Insured.age + I(Insured.age^2) \n                                  + Car.age + I(Car.age^2) + I(Car.age^3)\n                                  + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x &lt;- model.matrix(glm.score, data=train2)[,-1]\ny &lt;- train2$M_Claim\n\nlambda.min &lt;- 0.01995262\nlambda.1se &lt;- 0.07943282\n\nlambda.select &lt;- lambda.min\nfit &lt;- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, alpha = 0.6, lambda = lambda.select)\n#fit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 0.6, lambda = lambda.select)\n\ntrain2$pred &lt;- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*train2$NB_Claim\ntrain2$pred.tele &lt;- train2$pred\n\nmatrix.x &lt;- model.matrix(glm.score, data=test2)[,-1]\ny &lt;- test2$M_Claim\n\ntest2$pred.base &lt;- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*test2$NB_Claim\nvariance &lt;- (sum((train2$AMT_Claim - train2$pred)^2)/(nrow(train2) - length(fit$beta)))\nphi &lt;- variance/mean(train2$AMT_Claim)^2\n\nResult_ &lt;- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ &lt;- cbind('LASSO (optimal)', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all &lt;- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n\n\nTable 6.7: Prediction scores for the GLM-net model (testing set)\n\n\nModel\nSc.log\nSc.MSE\n\n\n\n\nBase\n9.29504\n21.82679\n\n\nGLM (trad.)\n9.27546\n21.77177\n\n\nLASSO (optimal)\n9.23357\n20.23523\n\n\n\n\n\n\n\n\n\n\nInstead of using the optimal value of the penalty \\(\\lambda\\) in the elastic-net approach, it is often advised to use a penalty value located at one standard error (\\(\\lambda_{1se}\\)). This helps to obtain a more parsimonious model. The prediction scores of such a model are displayed below.\n\n\nCode\nResult_  &lt;- data.frame()\nResult2_  &lt;- data.frame()\nfor(i in 1:nb.fold) {\n  learn &lt;- train2[train2$fold != i,]\n  valid &lt;- train2[train2$fold == i,]\n  \n  matrix.x &lt;- model.matrix(glm.score, data=learn)[,-1]\n  y &lt;- learn$M_Claim\n  \n  lambda.min &lt;- 0.01995262\n  lambda.1se &lt;- 0.07943282\n  \n  lambda.select &lt;- lambda.1se\n  fit &lt;- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=TRUE, alpha = 0.6, lambda = lambda.select)\n  learn$pred &lt;- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*learn$NB_Claim\n  \n  matrix.x &lt;- model.matrix(glm.score, data=valid)[,-1]\n  y &lt;- valid$M_Claim\n\n  \n  valid$pred &lt;- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*valid$NB_Claim\n  variance &lt;- (sum((learn$AMT_Claim - learn$pred)^2)/(nrow(learn) - length(fit$beta)))\n  phi &lt;- variance/mean(learn$AMT_Claim)^2\n  \n  Result_ &lt;- rbind(Result_, c(i, Score.pred.sev(valid$pred, valid$AMT_Claim, phi)/nrow(valid)))\n  Result2_ &lt;- rbind(Result2_, c(i, Score.pred.sev(valid$pred, valid$AMT_Claim, phi)))\n}\n\n\n## Show results\ncolnames(Result_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\")\ncolnames(Result2_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\")\ntot &lt;- colSums(Result2_)/nrow(train2)\ntot$Fold &lt;- 'Total'\nResult_ &lt;- rbind(Result_ , tot)\nResult_ &lt;- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] &lt;- 'Improvement'\n\nfor(i in 2:3){\n  Result_[nb.fold+2,i] &lt;-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) &lt;- NULL\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n\n\nTable 6.8: Prediction scores for the GLM-net model (alpha=1) (severity)\n\n\nFold\nSc.log\nSc.MSE\n\n\n\n\n1\n9.21249\n45.73294\n\n\n2\n9.22998\n20.68548\n\n\n3\n9.24605\n29.09385\n\n\n4\n9.28279\n29.08124\n\n\n5\n9.21318\n17.88075\n\n\nTotal\n9.23698\n28.66605\n\n\nImprovement\n-0.05816\n-1.77089\n\n\n\n\n\n\n\n\nThe same model can be used to compute the scores on the test set.\n\n\nCode\nglm.score &lt;- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region\n                                  + Credit.score +  I(Credit.score^2) \n                                  + Insured.age + I(Insured.age^2) \n                                  + Car.age + I(Car.age^2) + I(Car.age^3)\n                                  + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x &lt;- model.matrix(glm.score, data=train2)[,-1]\ny &lt;- train2$M_Claim\n\nlambda.min &lt;- 0.01995262\nlambda.1se &lt;- 0.07943282\n\nlambda.select &lt;- lambda.1se\n#fit &lt;- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, alpha = 0.6, lambda = lambda.select)\nfit &lt;- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=TRUE, alpha = 0.6, lambda = lambda.select)\n\ntrain2$pred &lt;- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*train2$NB_Claim\ntrain2$pred.tele &lt;- train2$pred\n\nmatrix.x &lt;- model.matrix(glm.score, data=test2)[,-1]\ny &lt;- test2$M_Claim\n\ntest2$pred.base &lt;- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*test2$NB_Claim\nvariance &lt;- (sum((train2$AMT_Claim - train2$pred)^2)/(nrow(train2) - length(fit$beta)))\nphi &lt;- variance/mean(train2$AMT_Claim)^2\n\nResult_ &lt;- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ &lt;- cbind('LASSO (parsimonious)', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all &lt;- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n\n\nTable 6.9: Prediction scores for the GLM-net model (testing set)\n\n\nModel\nSc.log\nSc.MSE\n\n\n\n\nBase\n9.29504\n21.82679\n\n\nGLM (trad.)\n9.27546\n21.77177\n\n\nLASSO (optimal)\n9.23357\n20.23523\n\n\nLASSO (parsimonious)\n9.23729\n20.21870\n\n\n\n\n\n\n\n\n\n\nFor categorical variables, the relativity values obtained for both GLM-net approaches are displayed below.\n\n\n\n\n\n\n\n(a) Sex of the insured\n\n\n\n\n\n\n\n(b) Marital status of the insured\n\n\n\n\n\n\n\n\n\n(c) Car use\n\n\n\n\n\n\n\n(d) Region\n\n\n\n\nFigure 6.6: Interprétation des variables catégorielles du GLM-net (severity)\n\n\n\n\nAs with categorical variables, the relativities obtained are illustrated below for continuous variables.\n\n\nCode\nglm.score &lt;- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region\n                        + Credit.score +  I(Credit.score^2) \n                        + Insured.age + I(Insured.age^2) \n                        + Car.age + I(Car.age^2) + I(Car.age^3) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x &lt;- model.matrix(glm.score, data=train2)[,-1]\ny &lt;- train2$M_Claim\n\n\nlambda.min &lt;- 0.01995262\nlambda.1se &lt;- 0.07943282\n\nlasso.min &lt;- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, alpha = 0.6, lambda = lambda.min)\nlasso.1se &lt;- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=TRUE, alpha = 0.6, lambda = lambda.1se)\n#cbind(coef(lasso.min), coef(lasso.1se))\n\n### Credit Score ###\nCredit.score &lt;- seq(from=min(train2$Credit.score), to=max(train2$Credit.score), by=1)\n\nbeta &lt;- coef(lasso.1se)[8:9]\ncurve1 &lt;- exp(beta[1]*Credit.score + beta[2]*Credit.score^2) \nbase1 &lt;- exp(beta[1]*mean(train2$Credit.score) + beta[2]*mean(train2$Credit.score)^2) \n\nbeta &lt;- coef(lasso.min)[8:9]\ncurve2 &lt;- exp(beta[1]*Credit.score + beta[2]*Credit.score^2) \nbase2 &lt;- exp(beta[1]*mean(train2$Credit.score) + beta[2]*mean(train2$Credit.score)^2) \n\ncurve1 &lt;- curve1/base1\ncurve2 &lt;- curve2/base2\ndb &lt;- data.frame(cbind(Credit.score, curve1, curve2))\n\nggplot()+\n  geom_line(aes(x=Credit.score, y=curve1, color = 'lambda.1se' ), data=db)+\n  geom_line(aes(x=Credit.score, y=curve2, color = 'lambda.min' ), data=db)+\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Credit Score',\n       y = 'Relativity') +\n  theme_bw()\n\n### Insured.age \nInsured.age &lt;- seq(from=min(train2$Insured.age ), to=max(train2$Insured.age ), by=1)\nbeta &lt;- coef(lasso.1se)[10:11]\ncurve1 &lt;- exp(beta[1]*Insured.age  + beta[2]*Insured.age^2)       \nbase1  &lt;- exp(beta[1]*mean(train2$Insured.age) + beta[2]*mean(train2$Insured.age)^2) \n\nbeta &lt;- coef(lasso.min)[10:11]\ncurve2 &lt;- exp(beta[1]*Insured.age  + beta[2]*Insured.age^2)       \nbase2  &lt;- exp(beta[1]*mean(train2$Insured.age) + beta[2]*mean(train2$Insured.age)^2) \n\ncurve1 &lt;- curve1/base1\ncurve2 &lt;- curve2/base2\ndb &lt;- data.frame(cbind(Insured.age, curve1, curve2))\n\nggplot()+\n  geom_line(aes(x=Insured.age, y=curve1, color = 'lambda.1se' ), data=db)+\n  geom_line(aes(x=Insured.age, y=curve2, color = 'lambda.min' ), data=db)+\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Age of the insured',\n       y = 'Relativity') +\n  theme_bw()\n\n\n### Car Age ###\nCar.age &lt;- seq(from=min(train2$Car.age), to=max(train2$Car.age), by=1)\nbeta &lt;- coef(lasso.1se)[12:14]\ncurve1 &lt;- exp(beta[1]*Car.age + beta[2]*Car.age^2 + beta[3]*Car.age^3)\nbase1  &lt;- exp(beta[1]*mean(train2$Car.age) + beta[2]*mean(train2$Car.age)^2 + beta[3]*mean(train2$Car.age)^3) \n\nbeta &lt;- coef(lasso.min)[12:14]\ncurve2 &lt;- exp(beta[1]*Car.age + beta[2]*Car.age^2 + beta[3]*Car.age^3)\nbase2  &lt;- exp(beta[1]*mean(train2$Car.age) + beta[2]*mean(train2$Car.age)^2 + beta[3]*mean(train2$Car.age)^3) \n\ncurve1 &lt;- curve1/base1\ncurve2 &lt;- curve2/base2\ndb &lt;- data.frame(cbind(Car.age, curve1, curve2))\n\nggplot()+\n  geom_line(aes(x=Car.age, y=curve1, color = 'lambda.1se' ), data=db)+\n  geom_line(aes(x=Car.age, y=curve2, color = 'lambda.min' ), data=db)+\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Age of the car',\n       y = 'Relativity') +\n  theme_bw()\n\n### Years.noclaims \nYears.noclaims &lt;- seq(from=min(train2$Years.noclaims ), to=max(train2$Years.noclaims ), by=1)\nbeta &lt;- coef(lasso.1se)[15:17]\ncurve1 &lt;- exp(beta[1]*Years.noclaims  + beta[2]*Years.noclaims^2 + beta[3]*Years.noclaims ^3)        \nbase1  &lt;- exp(beta[1]*mean(train2$Years.noclaims) + beta[2]*mean(train2$Years.noclaims)^2 + beta[3]*mean(train2$Years.noclaims)^3) \n\nbeta &lt;- coef(lasso.min)[15:17]\ncurve2 &lt;- exp(beta[1]*Years.noclaims  + beta[2]*Years.noclaims^2 + beta[3]*Years.noclaims ^3)        \nbase2  &lt;- exp(beta[1]*mean(train2$Years.noclaims) + beta[2]*mean(train2$Years.noclaims)^2 + beta[3]*mean(train2$Years.noclaims)^3) \n\ncurve1 &lt;- curve1/base1\ncurve2 &lt;- curve2/base2\ndb &lt;- data.frame(cbind(Years.noclaims, curve1, curve2))\n\nggplot()+\n  geom_line(aes(x=Years.noclaims, y=curve1, color = 'lambda.1se' ), data=db)+\n  geom_line(aes(x=Years.noclaims, y=curve2, color = 'lambda.min' ), data=db)+\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Years without claim',\n       y = 'Relativity') +\n  theme_bw()\n\n### terr.code  \nterr.code  &lt;- seq(from=min(train2$terr.code  ), to=max(train2$terr.code  ), by=0.01)\nbeta &lt;- coef(lasso.1se)[18:20]\ncurve1 &lt;- exp(beta[1]*terr.code + beta[2]*terr.code^2 + beta[3]*terr.code^3)\nbase1  &lt;- exp(beta[1]*mean(train2$terr.code) + beta[2]*mean(train2$terr.code)^2 + beta[3]*mean(train2$terr.code)^3)\n\nbeta &lt;- coef(lasso.min)[18:20]\ncurve2 &lt;- exp(beta[1]*terr.code + beta[2]*terr.code^2 + beta[3]*terr.code^3)\nbase2  &lt;- exp(beta[1]*mean(train2$terr.code) + beta[2]*mean(train2$terr.code)^2 + beta[3]*mean(train2$terr.code)^3)\n\ncurve1 &lt;- curve1/base1\ncurve2 &lt;- curve2/base2\ndb &lt;- data.frame(cbind(terr.code, curve1, curve2))\n\nggplot()+\n  geom_line(aes(x=terr.code, y=curve1, color = 'lambda.1se' ), data=db)+\n  geom_line(aes(x=terr.code, y=curve2, color = 'lambda.min' ), data=db)+\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Territory (encoded)',\n       y = 'Relativity') +\n  theme_bw()\n\n\n\n\n\n\n\n\n(a) Credit Score\n\n\n\n\n\n\n\n(b) Annal Miles Drive\n\n\n\n\n\n\n\n\n\n(c) Age of the car\n\n\n\n\n\n\n\n(d) Age of the insured\n\n\n\n\n\n\n\n\n\n(e) Years without claim\n\n\n\n\nFigure 6.7: Interprétation des variables continues du GLM-net (severity)"
  },
  {
    "objectID": "severityVarTrad.html#xgboost",
    "href": "severityVarTrad.html#xgboost",
    "title": "6  Traditional Covariates",
    "section": "6.4 XGBoost",
    "text": "6.4 XGBoost\nAs we did for the analysis of claim frequency, we also consider the XGBoost approach for claim severity. We utilized Bayesian optimization to find the hyperparameters of the model.\n\nPrediction ScoresVariables Importance\n\n\nUsing the hyperparameters we identified, we can calculate the model’s prediction scores. The obtained scores demonstrate a notable enhancement compared to alternative approaches evaluated.\n\n\nCode\nparam &lt;- list(\n  eta = 0.1442358,\n  max_depth = 4,\n  subsample = 0.4589511,\n  min_child_weight = 121.1358,\n  booster = \"gbtree\",\n  objective = \"reg:gamma\",\n  eval_metric = \"gamma-nloglik\")\n\nset.seed(333)\nxgbcv &lt;- xgb.cv(params = param,\n                nrounds = 120,\n                data = dtrain,\n                folds = folds,\n                prediction = TRUE,\n                early_stopping_rounds = 10,\n                verbose = 0,\n                maximize = F)\n\n\nvariance &lt;- sapply(xgbcv$folds, function(x){sum((train2$AMT_Claim[x]-unlist(xgbcv$pred[x])*train2$NB_Claim[x])^2)/length(train2$AMT_Claim[x])})  \nmean &lt;- sapply(xgbcv$folds, function(x){mean(train2$AMT_Claim[x])})\nphi &lt;- unlist(variance)/mean^2\n\nSc.log1 &lt;- -dgamma(train2$AMT_Claim[xgbcv$folds$fold1], shape = 1/phi[1], scale = unlist(xgbcv$pred[xgbcv$folds$fold1])*train2$NB_Claim[xgbcv$folds$fold1]*phi[1], log=TRUE)\nSc.log2 &lt;- -dgamma(train2$AMT_Claim[xgbcv$folds$fold2], shape = 1/phi[2], scale = unlist(xgbcv$pred[xgbcv$folds$fold2])*train2$NB_Claim[xgbcv$folds$fold2]*phi[2], log=TRUE)\nSc.log3 &lt;- -dgamma(train2$AMT_Claim[xgbcv$folds$fold3], shape = 1/phi[3], scale = unlist(xgbcv$pred[xgbcv$folds$fold3])*train2$NB_Claim[xgbcv$folds$fold3]*phi[3], log=TRUE)\nSc.log4 &lt;- -dgamma(train2$AMT_Claim[xgbcv$folds$fold4], shape = 1/phi[4], scale = unlist(xgbcv$pred[xgbcv$folds$fold4])*train2$NB_Claim[xgbcv$folds$fold4]*phi[4], log=TRUE)\nSc.log5 &lt;- -dgamma(train2$AMT_Claim[xgbcv$folds$fold5], shape = 1/phi[5], scale = unlist(xgbcv$pred[xgbcv$folds$fold5])*train2$NB_Claim[xgbcv$folds$fold5]*phi[5], log=TRUE)\n\nSc.MSE &lt;- sapply(xgbcv$folds, function(x){(train2$AMT_Claim[x]-unlist(xgbcv$pred[x])*train2$NB_Claim[x])^2/1000000})\n\n\nResult_  &lt;- rbind(\n  c(1,mean(Sc.log1), mean(Sc.MSE[1]$fold1)),\n  c(2,mean(Sc.log2), mean(Sc.MSE[2]$fold2)),\n  c(3,mean(Sc.log3), mean(Sc.MSE[3]$fold3)),\n  c(4,mean(Sc.log4), mean(Sc.MSE[4]$fold4)),\n  c(5,mean(Sc.log5), mean(Sc.MSE[5]$fold5))\n)\n\nRes.sum  &lt;- rbind(\n  c(sum(Sc.log1), sum(Sc.MSE[1]$fold1)),\n  c(sum(Sc.log2), sum(Sc.MSE[2]$fold2)),\n  c(sum(Sc.log3), sum(Sc.MSE[3]$fold3)),\n  c(sum(Sc.log4), sum(Sc.MSE[4]$fold4)),\n  c(sum(Sc.log5), sum(Sc.MSE[5]$fold5))\n)\nsum &lt;- c('Total', colSums(Res.sum)/nrow(train2))\n\nResult_  &lt;- data.frame(rbind(Result_, sum)) \n\n## Show results\ncolnames(Result_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\")\nResult_ &lt;- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] &lt;- 'Improvement'\n\nfor(i in 2:3){\n  Result_[,i] &lt;- as.numeric(Result_[,i])  \n  Result_[nb.fold+2,i] &lt;-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\nrownames(Result_) &lt;- NULL\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n\n\nTable 6.10: Prediction scores for the XGBoost model (severity)\n\n\nFold\nSc.log\nSc.MSE\n\n\n\n\n1\n9.35734\n44.35244\n\n\n2\n9.14014\n20.12582\n\n\n3\n9.21035\n28.22455\n\n\n4\n9.23525\n28.30773\n\n\n5\n9.09730\n17.68123\n\n\nTotal\n9.20981\n27.90189\n\n\nImprovement\n-0.08533\n-2.53505\n\n\n\n\n\n\n\n\nThe same model can be used to compute the scores on the test set. We also observe that the XGBoost approach is the most effective.\n\n\nCode\nparam &lt;- list(\n  eta = 0.1442358,\n  max_depth = 4,\n  subsample = 0.4589511,\n  min_child_weight = 121.1358,\n  booster = \"gbtree\",\n  objective = \"reg:gamma\",\n  eval_metric = \"gamma-nloglik\")\n\nset.seed(633)\nfit.xgb &lt;- xgb.train(params = param,\n                     nrounds = 120,\n                     data = dtrain)\n\ntrain2$pred.xgb &lt;- predict(fit.xgb, dtrain, type='response')*train2$NB_Claim\ntrain2$pred.xgb.off &lt;- predict(fit.xgb, dtrain, type='response')\n\ndtest &lt;- xgb.DMatrix(data = data.matrix(test2[, paste(trad.vars)]), label = test2$M_Claim)\n#setinfo(dtest,\"base_margin\",log(test2$expo))\ntest2$pred.xgb &lt;- predict(fit.xgb, dtest, type='response')*test2$NB_Claim\ntest2$pred.xgb.off &lt;- predict(fit.xgb, dtest, type='response')\n\ntest2$pred.base &lt;- test2$pred.xgb\n\nvariance &lt;- (sum((train2$pred.xgb - (train2$AMT_Claim))^2)/(length(train2$AMT_Claim)))\nphi &lt;- variance/mean(train2$AMT_Claim)^2\n\nResult_ &lt;- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ &lt;- cbind('XGBoost', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all &lt;- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n\n\nTable 6.11: Prediction scores for the XGBoost model with traditional covariates (severity)\n\n\nModel\nSc.log\nSc.MSE\n\n\n\n\nBase\n9.29504\n21.82679\n\n\nGLM (trad.)\n9.27546\n21.77177\n\n\nLASSO (optimal)\n9.23357\n20.23523\n\n\nLASSO (parsimonious)\n9.23729\n20.21870\n\n\nXGBoost\n9.19011\n20.06725\n\n\n\n\n\n\n\n\n\n\nThe following graph depicts the most crucial variables in the XGBoost model.\n\n\nCode\nparam &lt;- list(\n  eta = 0.1442358,\n  max_depth = 4,\n  subsample = 0.4589511,\n  min_child_weight = 121.1358,\n  booster = \"gbtree\",\n  objective = \"reg:gamma\",\n  eval_metric = \"gamma-nloglik\")\n\nset.seed(333)\nfit.xgb &lt;- xgb.train(params = param,\n                     nrounds = 120,\n                     data = dtrain,\n#                     prediction = TRUE,\n                     verbose = 0,\n                     maximize = F)\n\nimportance_matrix &lt;- xgb.importance(dimnames(dtrain)[[2]], model = fit.xgb)\nxgb.ggplot.importance(importance_matrix,top_n=10) + theme(text = element_text(size=15))\n\n\n\n?(caption)"
  },
  {
    "objectID": "severityVarTelematique.html#preamble",
    "href": "severityVarTelematique.html#preamble",
    "title": "7  Telematic Covariates",
    "section": "7.1 Preamble",
    "text": "7.1 Preamble\n\nChapter ObjectivePackagesDataData Transformation\n\n\nWe continue our analysis of claim severity based on the available covariates in the database. Compared to the previous chapter, we are now adding telematics variables to the exercise while removing protected variables. Thus, the following five covariates are excluded, for the moment, from the analysis:\n\nCredit.score,\n\nInsured age,\n\nInsured.sex,\n\nMarital, and\n\nTerritory.\n\nWe use the same two main models as in the previous chapters, namely Generalized Linear Model (GLM) family, including elastic-net and XGBoost. For each model, the response variable is the average cost of a claim, given that at least one claim has occurred. To analyze severities, we still use the same two scores used previously.\n\n\nCode\nScore.pred.sev &lt;- function(mu, x, phi) {\n  Sc.log  &lt;- -sum(dgamma(x, shape = 1/phi, scale = mu*phi, log=TRUE))\n  Sc.MSE  &lt;- sum((x - mu)^2)/1000000\n  return(c(Sc.log, Sc.MSE))\n}\n\n\n\n\nFor this chapter, we need the following packages:\n\n\nCode\nlibrary(tidyverse)\nlibrary(vtable)\nlibrary(rpart)\nlibrary(repr)\nlibrary(rpart.plot)\nlibrary(gam)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(glmnet)\nlibrary(scoringRules)\nlibrary(sjPlot)\n\n\n\n\nIn this chapter, we conduct analyses using the same data as in the previous chapters amd we use the same train/test division.\n\n\nCode\ndataS &lt;- read.csv('Data/Synthetic.csv')\n\ndata &lt;- dataS[dataS$AMT_Claim &gt; 0,]\ndata$M_Claim &lt;- data$AMT_Claim/data$NB_Claim\n\n# Modifications \ndata &lt;- data %&gt;%\n  mutate(Territory = as.factor(Territory)) %&gt;%\n  select(-c('Annual.pct.driven', 'Annual.miles.drive'))\n\ndata.select &lt;- data\n\n# Train-test \nset.seed(123)\ntrain &lt;- data.select %&gt;% sample_frac(0.8, replace = FALSE)\ntest &lt;- data.select %&gt;% anti_join(train)\n\ntest &lt;- test[-640,]\n\n\n\n\nAs we concluded at the end of our overview of the data, certain variables also need to be transformed.\n\n\nCode\n# Modif data\ntrain2 &lt;- train %&gt;%\n  mutate(Miles.per.day = Total.miles.driven/Duration,\n         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),\n         expo = Duration/365.25)\n\ntransform.fct &lt;- function(var){\n  df &lt;- train2 %&gt;% mutate(var_ = get(var)*Total.miles.driven/(1000*Duration))\n  q99 &lt;- quantile(df$var_, 0.99)\n  df &lt;- df %&gt;% mutate(var_ = ifelse(var_ &gt; q99, q99, var_))\n  #colnames(df)[ncol(df)] &lt;- paste0(var, '_')\n  return(df)\n}\n\ntrain2 &lt;- transform.fct(\"Brake.06miles\")\ntrain2 &lt;- transform.fct(\"Brake.08miles\")\ntrain2 &lt;- transform.fct(\"Brake.09miles\")\ntrain2 &lt;- transform.fct(\"Brake.11miles\")\ntrain2 &lt;- transform.fct(\"Brake.14miles\")\ntrain2 &lt;- transform.fct(\"Accel.06miles\")\ntrain2 &lt;- transform.fct(\"Accel.08miles\")\ntrain2 &lt;- transform.fct(\"Accel.09miles\")\ntrain2 &lt;- transform.fct(\"Accel.11miles\")\ntrain2 &lt;- transform.fct(\"Accel.12miles\")\ntrain2 &lt;- transform.fct(\"Accel.14miles\")\ntrain2 &lt;- transform.fct(\"Left.turn.intensity08\")\ntrain2 &lt;- transform.fct(\"Left.turn.intensity09\")\ntrain2 &lt;- transform.fct(\"Left.turn.intensity10\")\ntrain2 &lt;- transform.fct(\"Left.turn.intensity11\")\ntrain2 &lt;- transform.fct(\"Left.turn.intensity12\")\ntrain2 &lt;- transform.fct(\"Right.turn.intensity08\")\ntrain2 &lt;- transform.fct(\"Right.turn.intensity09\")\ntrain2 &lt;- transform.fct(\"Right.turn.intensity10\")\ntrain2 &lt;- transform.fct(\"Right.turn.intensity11\")\ntrain2 &lt;- transform.fct(\"Right.turn.intensity12\")\n\n# Create folds\nnb.fold &lt;- 5\nfold &lt;- sample(1:nb.fold, nrow(train2), replace = TRUE)\ntrain2$fold &lt;- fold\n\n##\n\ntest2 &lt;- test %&gt;%\n  mutate(Miles.per.day = Total.miles.driven/Duration,\n         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),\n         expo = Duration/365.25)\n\ntransform.fct &lt;- function(var){\n  df &lt;- test2 %&gt;% mutate(var_ = get(var)*Total.miles.driven/(1000*Duration))\n  q99 &lt;- quantile(df$var_, 0.99)\n  df &lt;- df %&gt;% mutate(var_ = ifelse(var_ &gt; q99, q99, var_))\n  #colnames(df)[ncol(df)] &lt;- paste0(var, '_')\n  return(df)\n}\n\ntest2 &lt;- transform.fct(\"Brake.06miles\")\ntest2 &lt;- transform.fct(\"Brake.08miles\")\ntest2 &lt;- transform.fct(\"Brake.09miles\")\ntest2 &lt;- transform.fct(\"Brake.11miles\")\ntest2 &lt;- transform.fct(\"Brake.14miles\")\ntest2 &lt;- transform.fct(\"Accel.06miles\")\ntest2 &lt;- transform.fct(\"Accel.08miles\")\ntest2 &lt;- transform.fct(\"Accel.09miles\")\ntest2 &lt;- transform.fct(\"Accel.11miles\")\ntest2 &lt;- transform.fct(\"Accel.12miles\")\ntest2 &lt;- transform.fct(\"Accel.14miles\")\ntest2 &lt;- transform.fct(\"Left.turn.intensity08\")\ntest2 &lt;- transform.fct(\"Left.turn.intensity09\")\ntest2 &lt;- transform.fct(\"Left.turn.intensity10\")\ntest2 &lt;- transform.fct(\"Left.turn.intensity11\")\ntest2 &lt;- transform.fct(\"Left.turn.intensity12\")\ntest2 &lt;- transform.fct(\"Right.turn.intensity08\")\ntest2 &lt;- transform.fct(\"Right.turn.intensity09\")\ntest2 &lt;- transform.fct(\"Right.turn.intensity10\")\ntest2 &lt;- transform.fct(\"Right.turn.intensity11\")\ntest2 &lt;- transform.fct(\"Right.turn.intensity12\")\n\n# Mean Encoding with White Noise pour les territoires\ncardi &lt;- length(unique(train$Territory))\n\nenc.terr &lt;- train2 %&gt;%\n  group_by(Territory) %&gt;%\n  summarize(freq = sum(NB_Claim)/sum(expo)) %&gt;%\n  arrange(freq) %&gt;%\n  mutate(terr.code= row_number()/(cardi+1)) %&gt;%\n  select(Territory, terr.code)\n\ntrain2 &lt;- train2 %&gt;%\n  group_by(Territory) %&gt;%\n  left_join(enc.terr, by='Territory') %&gt;%\n  ungroup()\n\ntest2 &lt;- test2 %&gt;%\n  group_by(Territory) %&gt;%\n  left_join(enc.terr, by='Territory') %&gt;%\n  ungroup()"
  },
  {
    "objectID": "severityVarTelematique.html#basic-glm-models",
    "href": "severityVarTelematique.html#basic-glm-models",
    "title": "7  Telematic Covariates",
    "section": "7.2 Basic GLM Models",
    "text": "7.2 Basic GLM Models\n\nSingle interceptTraditional covariates already used (without protected variables)Estimated parameters\n\n\nFor comparison, we use a baseline model corresponding to a GLM with an intercept that predicts only the mean severity multiplied by the observed frequency for each contract. We do not present the results based on the train dataset to avoid burdening the report unnecessarily. However, if necessary, uncomment the last portion of the code to produce the results table.\n\n\nCode\n## Model on each fold\nResult_  &lt;- data.frame()\nResult2_  &lt;- data.frame()\nfor(i in 1:nb.fold) {\n  learn &lt;- train2[train2$fold != i,]\n  valid &lt;- train2[train2$fold == i,]\n  \n  mean &lt;- sum(learn$AMT_Claim)/sum(learn$NB_Claim) \n  variance &lt;- sd(learn$AMT_Claim)^2\n  phi &lt;- variance/mean(learn$AMT_Claim)^2\n  \n  learn$pred.base &lt;- mean*learn$NB_Claim\n  valid$pred.base &lt;- mean*valid$NB_Claim\n  \n  Result_ &lt;- rbind(Result_, c(i, Score.pred.sev(valid$pred.base, valid$AMT_Claim, phi)/nrow(valid)))\n  Result2_ &lt;- rbind(Result2_, c(i, Score.pred.sev(valid$pred.base, valid$AMT_Claim, phi)))\n}\n\n## Show results\ncolnames(Result_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\")\ncolnames(Result2_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\")\ntot &lt;- colSums(Result2_)/nrow(train2)\ntot$Fold &lt;- 'Total'\nResult_ &lt;- rbind(Result_ , tot)\n\nResult.base &lt;- Result_  \nBase &lt;- Result.base[nb.fold+1,]\n\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n\n\nTable 7.1: Prediction scores for the base model (severity)\n\n\nFold\nSc.log\nSc.MSE\n\n\n\n\n1\n9.28096\n48.72725\n\n\n2\n9.28378\n21.80556\n\n\n3\n9.30844\n31.17175\n\n\n4\n9.34049\n30.61554\n\n\n5\n9.26105\n18.96634\n\n\nTotal\n9.29513\n30.43693\n\n\n\n\n\n\n\n\nThe model is, therefore, estimated on the entire train database, and the predictions are made on the test database, which was not used during the calibration phase.\n\n\nCode\nmean &lt;- sum(train2$AMT_Claim)/sum(train2$NB_Claim) \nvariance &lt;- sd(train2$AMT_Claim)^2\nphi &lt;- variance/mean(train2$AMT_Claim)^2 \n  \ntest2$pred.base &lt;- mean*test2$NB_Claim\n\nResult_ &lt;- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ &lt;- cbind('Base', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all &lt;- Result_\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n\n\nTable 7.2: Prediction scores for the base model (testing set) (severity)\n\n\nModel\nSc.log\nSc.MSE\n\n\n\n\nBase\n9.29504\n21.82679\n\n\n\n\n\n\n\n\n\n\nWe construct a first GLM with only the following covariates:\n\nCar.use,\n\nRegion,\n\nCar.age, and\nYears.noclaims.\n\nWe calculate the prediction scores of the model with all categorical covariates on the train and the test dataset. As expected, adding segmentation variables improves the prediction scores compared to the simple baseline model with only an intercept.\n\n\nCode\n## Model \nscore.base &lt;- as.formula(M_Claim ~ 1)\nscore.glm &lt;- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2))\n## Model on each fold\nResult_  &lt;- data.frame()\nResult2_  &lt;- data.frame()\nfor(i in 1:nb.fold) {\n  learn &lt;- train2[train2$fold != i,]\n  valid &lt;- train2[train2$fold == i,]\n  glm.fit &lt;- glm(score.glm, family = Gamma(link = \"log\"), data = learn)\n  \n  learn$pred.base &lt;- predict(glm.fit, newdata=learn, type='response')*learn$NB_Claim\n  valid$pred.base &lt;- predict(glm.fit, newdata=valid, type='response')*valid$NB_Claim\n  phi &lt;- summary(glm.fit)$dispersion\n  \n  Result_ &lt;- rbind(Result_, c(i, Score.pred.sev(valid$pred.base, valid$AMT_Claim, phi)/nrow(valid)))\n  Result2_ &lt;- rbind(Result2_, c(i, Score.pred.sev(valid$pred.base, valid$AMT_Claim, phi)))\n}\n\n## Model on all data from train\nglm.base &lt;- glm(score.base, family = Gamma(link = \"log\"), data = train2)\nglm.fit &lt;- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\ntrain2$pred.glm1 &lt;- predict(glm.fit, newdata=train2, type='response')*train2$NB_Claim\nphi &lt;- summary(glm.fit)$dispersion\nResult.glm1 &lt;- Result_  \n\n## Show results\ncolnames(Result_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\")\ncolnames(Result2_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\")\ntot &lt;- colSums(Result2_)/nrow(train2)\ntot$Fold &lt;- 'Total'\nResult_ &lt;- rbind(Result_ , tot)\nResult_ &lt;- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] &lt;- 'Improvement'\n\nfor(i in 2:3){\n  Result_[nb.fold+2,i] &lt;-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) &lt;- NULL\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n\n\nTable 7.3: Prediction scores for the GLM model with traditional covariates (without protected variables) (severity)\n\n\nFold\nSc.log\nSc.MSE\n\n\n\n\n1\n9.22622\n47.33748\n\n\n2\n9.22946\n22.23070\n\n\n3\n9.25050\n30.70171\n\n\n4\n9.27857\n29.97837\n\n\n5\n9.20165\n17.97995\n\n\nTotal\n9.23751\n29.82819\n\n\nImprovement\n-0.05762\n-0.60874\n\n\n\n\n\n\n\n\n\n\nCode\nscore.glm &lt;- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2))\n\nglm.fit &lt;- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\ntest2$pred.base &lt;- predict(glm.fit, newdata=test2, type='response')*test2$NB_Claim\nphi &lt;- summary(glm.fit)$dispersion\n\nResult_ &lt;- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ &lt;- cbind('GLM (trad.)', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all &lt;- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n\n\nTable 7.4: Prediction scores for the GLM model with traditional covariates (testing set) (severity)\n\n\nModel\nSc.log\nSc.MSE\n\n\n\n\nBase\n9.29504\n21.82679\n\n\nGLM (trad.)\n9.23655\n21.16556\n\n\n\n\n\n\n\n\n\n\nThe table below shows the estimators obtained for the GLM-Gamma approach and compares them with the baseline model, which has only an intercept. We note that except for the variable “Car.use”, the traditional variables do not seem significant in the model.\n\n\nCode\n## Model \nscore.base &lt;- as.formula(M_Claim ~ 1)\n\nscore.glm &lt;- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2))\n\n## Model on all data from train\nglm.base &lt;- glm(score.base, family = Gamma(link = \"log\"), data = train2)\nglm.fit &lt;- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ntab_model(glm.base, glm.fit, transform = NULL)\n\n\n\n\nTable 7.5: Estimated parameters for the GLM model with traditional covariates (without protected variables) (severity)\n\n\n \nM Claim\nM Claim\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n8.12\n8.07 – 8.18\n&lt;0.001\n8.30\n7.95 – 8.66\n&lt;0.001\n\n\nCar use [Commute]\n\n\n\n0.19\n-0.10 – 0.46\n0.186\n\n\nCar use [Farmer]\n\n\n\n-0.91\n-1.61 – -0.07\n0.019\n\n\nCar use [Private]\n\n\n\n0.04\n-0.26 – 0.32\n0.773\n\n\nRegion [Urban]\n\n\n\n0.05\n-0.09 – 0.19\n0.440\n\n\nCar age\n\n\n\n-0.03\n-0.07 – 0.01\n0.141\n\n\nCar age^2\n\n\n\n0.00\n-0.00 – 0.00\n0.973\n\n\nYears noclaims\n\n\n\n-0.01\n-0.02 – 0.01\n0.421\n\n\nYears noclaims^2\n\n\n\n-0.00\n-0.00 – 0.00\n0.181\n\n\nObservations\n3091\n3091\n\n\nR2 Nagelkerke\n0.000\n0.098"
  },
  {
    "objectID": "severityVarTelematique.html#glm-net",
    "href": "severityVarTelematique.html#glm-net",
    "title": "7  Telematic Covariates",
    "section": "7.3 GLM-Net",
    "text": "7.3 GLM-Net\nFirst, we consider the GLM-net model. To make the approach as effective as possible, we need to adjust the continuous segmentation variables.\n\n7.3.1 Parametric transformation of telematic covariates\nAs we did in the other chapter, we first introduce an approach using the Generalized Additive Models (GAM) theory for all continuous variables. This approach allows us to observe the general form of the covariate to explain the severity. A parametric form will then be proposed to achieve the best possible correspondence with the spline obtained by the GAM.\n\nVehicle Usage levelType of vehicle usageDriving behavior\n\n\nFor the two covariates related to usage level, the proposed parametric forms are as follows:\n\\[\\begin{align*}\ns(Miles.per.day) \\approx& Miles.per.day + log(Miles.per.day)\\\\\ns(Avgdays.week) \\approx& Avgdays.week + Avgdays.week^2 + Avgdays.week^3\n\\end{align*}\\]\nThe graphs below compare the fit of the parametric approach with that of the GAM model.\n\n\nCode\nmin_ &lt;- min(train2$Miles.per.day) \nmax_ &lt;- max(train2$Miles.per.day) \nby_ &lt;-  (max_ - min_)/(nrow(train2)-1) \nadd &lt;- data.frame(seq(min_, max_, by_)) \ncolnames(add) &lt;- 'Miles.per.day'\n\nq99 &lt;- quantile(train2$Miles.per.day, 0.99)\n\ndb &lt;- train2 %&gt;%\n  select(-'Miles.per.day') %&gt;%\n  dplyr::slice(1) \ndb &lt;- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb &lt;- cbind(db, add)\n\n##\n\nscore.gam &lt;- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)\n                        + s(Miles.per.day))\n\nscore.glm &lt;- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)\n                        + Miles.per.day + log(Miles.per.day))\n\ngam.fit &lt;- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit &lt;- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam &lt;- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm &lt;- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase &lt;- db %&gt;%\n  mutate(diff = abs(Miles.per.day - mean(train2$Miles.per.day))) %&gt;%\n  filter(diff == min(diff))\ndb$pred.gam &lt;- db$pred.gam/base$pred.gam[1]\ndb$pred.glm &lt;- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=Miles.per.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=Miles.per.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Miles per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n##\n\nmin_ &lt;- quantile(train2$Avgdays.week, 0.01) \nmax_ &lt;- max(train2$Avgdays.week) \nby_ &lt;-  (max_ - min_)/(nrow(train2)-1) \nadd &lt;- data.frame(seq(min_, max_, by_)) \ncolnames(add) &lt;- 'Avgdays.week'\n\nq99 &lt;- quantile(train2$Avgdays.week, 0.99)\n\ndb &lt;- train2 %&gt;%\n  select(-'Avgdays.week') %&gt;%\n  dplyr::slice(1) \ndb &lt;- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb &lt;- cbind(db, add)\n\n##\n\nscore.gam &lt;- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)\n                        + s(Avgdays.week))\n\nscore.glm &lt;- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)\n                        + Avgdays.week + I(Avgdays.week^2) + I(Avgdays.week^3))\n\ngam.fit &lt;- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit &lt;- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam &lt;- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm &lt;- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase &lt;- db %&gt;%\n  mutate(diff = abs(Avgdays.week - mean(train2$Avgdays.week))) %&gt;%\n  filter(diff == min(diff))\ndb$pred.gam &lt;- db$pred.gam/base$pred.gam[1]\ndb$pred.glm &lt;- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=Avgdays.week, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=Avgdays.week, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Avgdays.week',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n\n\n\n\n\n(a) Miles.per.day\n\n\n\n\n\n\n\n(b) Avgdays.week\n\n\n\n\nFigure 7.1: Smoothing of Usage level covariates (severity)\n\n\n\n\n\nSeveral covariates are available in the category Type of vehicle usage:\n\nWe propose the same parametric form for all variants of the variable Pct.drive.day (Monday to Sunday);\n\nThe same parametric form will also be proposed For Pct.drive.rush.am, Pct.drive.rush.pm, Pct.drive.2hrs, Pct.drive.3hrs, and Pct.drive.4hrs;\n\nThe other 3 covariates have their own parametric form.\n\nWe then have:\n\\[\\begin{align*}\ns(Pct.drive.day) &\\approx Pct.drive.day + Pct.drive.day^2 \\\\\ns(Pct.drive) &\\approx Pct.drive + \\sqrt{Pct.drive} \\\\\ns(max.day) &\\approx max.day + max.day^2 + max.day^3 \\\\\ns(min.day) &\\approx min.day + min.day^2 + min.day^3 \\\\\ns(max.min) &\\approx max.min + max.min^2\n\\end{align*}\\]\n\n\nCode\ntrain2$Pct.drive &lt;- train2$Pct.drive.mon\n\nmin_ &lt;- min(train2$Pct.drive) \nmax_ &lt;- max(train2$Pct.drive) \nby_ &lt;-  (max_ - min_)/(nrow(train2)-1) \nadd &lt;- data.frame(seq(min_, max_, by_)) \ncolnames(add) &lt;- 'Pct.drive'\n\nq99 &lt;- quantile(train2$Pct.drive, 0.99)\n\ndb &lt;- train2 %&gt;%\n  select(-'Pct.drive') %&gt;%\n  dplyr::slice(1) \ndb &lt;- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb &lt;- cbind(db, add)\n\n##\n\nscore.gam &lt;- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)\n                        + s(Pct.drive))\n\nscore.glm &lt;- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)\n                        + Pct.drive + I(Pct.drive^2))\n\ngam.fit &lt;- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit &lt;- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam &lt;- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm &lt;- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase &lt;- db %&gt;%\n  mutate(diff = abs(Pct.drive - mean(train2$Pct.drive))) %&gt;%\n  filter(diff == min(diff))\ndb$pred.gam &lt;- db$pred.gam/base$pred.gam[1]\ndb$pred.glm &lt;- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=Pct.drive, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=Pct.drive, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Pct.drive',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Rush\n\ntrain2$use.day &lt;- train2$Pct.drive.rush.am\n\nmin_ &lt;- min(train2$use.day) \nmax_ &lt;- max(train2$use.day) \nby_ &lt;-  (max_ - min_)/(nrow(train2)-1) \nadd &lt;- data.frame(seq(min_, max_, by_)) \ncolnames(add) &lt;- 'use.day'\n\nq99 &lt;- quantile(train2$use.day, 0.99)\n\ndb &lt;- train2 %&gt;%\n  select(-'use.day') %&gt;%\n  dplyr::slice(1) \ndb &lt;- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb &lt;- cbind(db, add)\n\n##\n\nscore.gam &lt;- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2) \n                        + s(use.day))\n\nscore.glm &lt;- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  \n                        + use.day + I(use.day^0.5))\n\ngam.fit &lt;- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit &lt;- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam &lt;- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm &lt;- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase &lt;- db %&gt;%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %&gt;%\n  filter(diff == min(diff))\ndb$pred.gam &lt;- db$pred.gam/base$pred.gam[1]\ndb$pred.glm &lt;- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Max.day\n\ntrain2$use.day &lt;- train2$max.day\n\nmin_ &lt;- min(train2$use.day) \nmax_ &lt;- max(train2$use.day) \nby_ &lt;-  (max_ - min_)/(nrow(train2)-1) \nadd &lt;- data.frame(seq(min_, max_, by_)) \ncolnames(add) &lt;- 'use.day'\n\nq99 &lt;- quantile(train2$use.day, 0.99)\n\ndb &lt;- train2 %&gt;%\n  select(-'use.day') %&gt;%\n  dplyr::slice(1) \ndb &lt;- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb &lt;- cbind(db, add)\n\n##\n\nscore.gam &lt;- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2) \n                        + s(use.day))\n\nscore.glm &lt;- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  \n                        + use.day + I(use.day^2) + I(use.day^3) )\n\ngam.fit &lt;- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit &lt;- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam &lt;- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm &lt;- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase &lt;- db %&gt;%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %&gt;%\n  filter(diff == min(diff))\ndb$pred.gam &lt;- db$pred.gam/base$pred.gam[1]\ndb$pred.glm &lt;- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, 1) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Min.day\n\ntrain2$use.day &lt;- train2$min.day\n\nmin_ &lt;- min(train2$use.day) \nmax_ &lt;- max(train2$use.day) \nby_ &lt;-  (max_ - min_)/(nrow(train2)-1) \nadd &lt;- data.frame(seq(min_, max_, by_)) \ncolnames(add) &lt;- 'use.day'\n\nq99 &lt;- quantile(train2$use.day, 0.99)\n\ndb &lt;- train2 %&gt;%\n  select(-'use.day') %&gt;%\n  dplyr::slice(1) \ndb &lt;- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb &lt;- cbind(db, add)\n\n##\n\nscore.gam &lt;- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  \n                        + s(use.day))\n\nscore.glm &lt;- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  \n                        + use.day + I(use.day^2)+ I(use.day^3) )\n\ngam.fit &lt;- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit &lt;- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam &lt;- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm &lt;- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase &lt;- db %&gt;%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %&gt;%\n  filter(diff == min(diff))\ndb$pred.gam &lt;- db$pred.gam/base$pred.gam[1]\ndb$pred.glm &lt;- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Max.min\n\ntrain2$use.day &lt;- train2$max.min\n\nmin_ &lt;- min(train2$use.day) \nmax_ &lt;- max(train2$use.day) \nby_ &lt;-  (max_ - min_)/(nrow(train2)-1) \nadd &lt;- data.frame(seq(min_, max_, by_)) \ncolnames(add) &lt;- 'use.day'\n\nq99 &lt;- quantile(train2$use.day, 0.99)\n\ndb &lt;- train2 %&gt;%\n  select(-'use.day') %&gt;%\n  dplyr::slice(1) \ndb &lt;- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb &lt;- cbind(db, add)\n\n##\n\nscore.gam &lt;- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  \n                        + s(use.day))\n\nscore.glm &lt;- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  \n                        + use.day + I(use.day^2) )\n\ngam.fit &lt;- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit &lt;- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam &lt;- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm &lt;- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase &lt;- db %&gt;%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %&gt;%\n  filter(diff == min(diff))\ndb$pred.gam &lt;- db$pred.gam/base$pred.gam[1]\ndb$pred.glm &lt;- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n\n\n\n\n\n(a) Pct.drive.mon\n\n\n\n\n\n\n\n(b) Pct.drive.rush.am\n\n\n\n\n\n\n\n\n\n(c) max.day\n\n\n\n\n\n\n\n(d) min.day\n\n\n\n\n\n\n\n\n\n(e) max.min\n\n\n\n\nFigure 7.2: Smoothing of Type of vehicle usage covariates (severity)\n\n\n\n\n\nThe same parametric form is proposed for the different variants of the Accel and Brake variables, i.e., Accel.06miles to Accel.14miles, and Brake.06miles to Brake.14miles. For the different variants of the turn variable, a single parametric form is also used:\n\\[\\begin{align*}\ns(Brake.Accel) &\\approx Brake.Accel + Brake.Accel^2 + Brake.Accel^3\\\\\ns(Turn) &\\approx Turn + log(Turn)\n\\end{align*}\\]\nThe graphs below compare the fit of the parametric approach for Accel.06miles and Right.turn.intensity08 with that of the GAM model.\n\n\nCode\ntrain2$use.day &lt;- train2$Accel.06miles\n\nmin_ &lt;- min(train2$use.day) \nmax_ &lt;- max(train2$use.day) \nby_ &lt;-  (max_ - min_)/(nrow(train2)-1) \nadd &lt;- data.frame(seq(min_, max_, by_)) \ncolnames(add) &lt;- 'use.day'\n\nq99 &lt;- quantile(train2$use.day, 0.99)\n\ndb &lt;- train2 %&gt;%\n  select(-'use.day') %&gt;%\n  dplyr::slice(1) \ndb &lt;- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb &lt;- cbind(db, add)\n\n##\n\nscore.gam &lt;- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2) \n                        + s(use.day))\n\nscore.glm &lt;- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2) \n                        + use.day + I(use.day^2) + I(use.day^3) )\n\ngam.fit &lt;- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit &lt;- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam &lt;- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm &lt;- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase &lt;- db %&gt;%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %&gt;%\n  filter(diff == min(diff))\ndb$pred.gam &lt;- db$pred.gam/base$pred.gam[1]\ndb$pred.glm &lt;- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\ntrain2$use.day &lt;- train2$Right.turn.intensity08\n\nq99 &lt;- quantile(train2$use.day, 0.99)\n\nmin_ &lt;- min(train2$use.day) \nmax_ &lt;- q99\nby_ &lt;-  (max_ - min_)/(nrow(train2)-1) \nadd &lt;- data.frame(seq(min_, max_, by_)) \ncolnames(add) &lt;- 'use.day'\n\ndb &lt;- train2 %&gt;%\n  select(-'use.day') %&gt;%\n  dplyr::slice(1) \ndb &lt;- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb &lt;- cbind(db, add)\n\n##\n\ntemp &lt;- train2 %&gt;%\n  mutate(use.day = pmin(q99, use.day))\n\nscore.gam &lt;- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2) \n                        + s(use.day))\n\nscore.glm &lt;- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)\n                        + use.day + log1p(use.day))\n\ngam.fit &lt;- gam(score.gam, family = Gamma(link = \"log\"), data = temp)\nglm.fit &lt;- glm(score.glm, family = Gamma(link = \"log\"), data = temp)\n\ndb$pred.gam &lt;- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm &lt;- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase &lt;- db %&gt;%\n  mutate(diff = abs(use.day - mean(temp$use.day))) %&gt;%\n  filter(diff == min(diff))\ndb$pred.gam &lt;- db$pred.gam/base$pred.gam[1]\ndb$pred.glm &lt;- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  # xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n\n\n\n\n\n(a) Accel.06miles\n\n\n\n\n\n\n\n(b) Right.turn.intensity08\n\n\n\n\nFigure 7.3: Smoothing of Driving behavior covariates (severity)\n\n\n\n\n\n\n\n\n7.3.2 Fitting the GLM-Net model\nTo solve some convergence issues, we remove from all GML-Net models Accel. and Brake. terms.\n\nOptimal valueParsimonious modelResiduals and protected variablesGLM-net on residuals\n\n\nThe parameters of the GLM-net were calibrated using cross-validation to obtain the model hyperparameters. It leads to a LASSO approach (\\(\\alpha = 1\\)). Using the optimal value of the penalty \\(\\lambda\\), we can calculate the model’s prediction scores based on all covariates.\n\n\nCode\nglm.score &lt;- as.formula(M_Claim ~ \n                          Car.use + Region + Car.age + I(Car.age^2) + I(Car.age^3) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2) + I(Avgdays.week^3)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + I(max.day^2) + I(max.day^3) \n                        + min.day + I(min.day^2) + I(min.day^3)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\nResult_  &lt;- data.frame()\nResult2_  &lt;- data.frame()\nfor(i in 1:nb.fold) {\n    learn &lt;- train2[train2$fold != i,]\n    valid &lt;- train2[train2$fold == i,]\n    \n    matrix.x &lt;- model.matrix(glm.score, data=learn)[,-1]\n    y &lt;- learn$M_Claim\n\n    lambda.min &lt;- 0.003162278\n    lambda.1se &lt;- 0.05011872\n    \n    lambda.select &lt;- lambda.min\n    fit &lt;- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, alpha = 1, lambda = lambda.select)\n    #fit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n    learn$pred &lt;- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*learn$NB_Claim\n    \n  \n    matrix.x &lt;- model.matrix(glm.score, data=valid)[,-1]\n    y &lt;- valid$M_Claim\n\n    valid$pred &lt;- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*valid$NB_Claim\n    variance &lt;- (sum((learn$AMT_Claim - learn$pred)^2)/(nrow(learn) - length(fit$beta)))\n    phi &lt;- variance/mean(learn$AMT_Claim)^2\n    \n    \n    Result_ &lt;- rbind(Result_, c(i, Score.pred.sev(valid$pred, valid$AMT_Claim, phi)/nrow(valid)))\n    Result2_ &lt;- rbind(Result2_, c(i, Score.pred.sev(valid$pred, valid$AMT_Claim, phi)))\n}\n\n\n## Show results\ncolnames(Result_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\")\ncolnames(Result2_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\")\ntot &lt;- colSums(Result2_)/nrow(train2)\ntot$Fold &lt;- 'Total'\nResult_ &lt;- rbind(Result_ , tot)\nResult_ &lt;- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] &lt;- 'Improvement'\n\nfor(i in 2:3){\n  Result_[nb.fold+2,i] &lt;-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\nrownames(Result_) &lt;- NULL\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n\n\nTable 7.6: Prediction scores for the GLM-net model\n\n\nFold\nSc.log\nSc.MSE\n\n\n\n\n1\n9.22057\n47.19565\n\n\n2\n9.23640\n21.15998\n\n\n3\n9.24562\n28.91845\n\n\n4\n9.27703\n27.73318\n\n\n5\n9.21091\n17.12773\n\n\nTotal\n9.23828\n28.61542\n\n\nImprovement\n-0.05685\n-1.82152\n\n\n\n\n\n\n\n\nThe same model can be used to compute the scores on the test set.\n\n\nCode\nglm.score &lt;- as.formula(M_Claim ~ \n                          Car.use + Region + Car.age + I(Car.age^2) + I(Car.age^3) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2) + I(Avgdays.week^3)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + I(max.day^2) + I(max.day^3) \n                        + min.day + I(min.day^2) + I(min.day^3)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\n\nmatrix.x &lt;- model.matrix(glm.score, data=train2)[,-1]\ny &lt;- train2$M_Claim\n\nlambda.min &lt;- 0.003162278\nlambda.1se &lt;- 0.05011872\n\nlambda.select &lt;- lambda.min\nfit &lt;- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, alpha = 1, lambda = lambda.select)\n#fit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n\ntrain2$pred &lt;- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*train2$NB_Claim\ntrain2$pred.tele &lt;- train2$pred\n\nmatrix.x &lt;- model.matrix(glm.score, data=test2)[,-1]\ny &lt;- test2$M_Claim\n\ntest2$pred.base &lt;- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*test2$NB_Claim\nvariance &lt;- (sum((train2$AMT_Claim - train2$pred)^2)/(nrow(train2) - length(fit$beta)))\nphi &lt;- variance/mean(train2$AMT_Claim)^2\n\nResult_ &lt;- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ &lt;- cbind('LASSO (optimal)', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all &lt;- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n\n\nTable 7.7: Prediction scores for the GLM-net model (testing set)\n\n\nModel\nSc.log\nSc.MSE\n\n\n\n\nBase\n9.29504\n21.82679\n\n\nGLM (trad.)\n9.23655\n21.16556\n\n\nLASSO (optimal)\n9.22435\n19.83764\n\n\n\n\n\n\n\n\n\n\nInstead of using the optimal value of the penalty \\(\\lambda\\) in the elastic-net approach, it is often advised to use a penalty value located at one standard error (\\(\\lambda_{1se}\\)). This approach helps to obtain a more parsimonious model. The prediction scores of such a model are displayed below.\n\n\nCode\nResult_  &lt;- data.frame()\nResult2_  &lt;- data.frame()\nfor(i in 1:nb.fold) {\n  learn &lt;- train2[train2$fold != i,]\n  valid &lt;- train2[train2$fold == i,]\n  \n  matrix.x &lt;- model.matrix(glm.score, data=learn)[,-1]\n  y &lt;- learn$M_Claim\n  \n  lambda.min &lt;- 0.003162278\n  lambda.1se &lt;- 0.05011872\n  \n  lambda.select &lt;- lambda.1se\n  #fit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\n  fit &lt;- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=TRUE, alpha = 1, lambda = lambda.select)\n  learn$pred &lt;- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*learn$NB_Claim\n  \n  \n  matrix.x &lt;- model.matrix(glm.score, data=valid)[,-1]\n  y &lt;- valid$M_Claim\n\n  \n  valid$pred &lt;- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*valid$NB_Claim\n  variance &lt;- (sum((learn$AMT_Claim - learn$pred)^2)/(nrow(learn) - length(fit$beta)))\n  phi &lt;-  variance/mean(learn$AMT_Claim)^2\n\n  Result_ &lt;- rbind(Result_, c(i, Score.pred.sev(valid$pred, valid$AMT_Claim, phi)/nrow(valid)))\n  Result2_ &lt;- rbind(Result2_, c(i, Score.pred.sev(valid$pred, valid$AMT_Claim, phi)))\n}\n\n\n## Show results\ncolnames(Result_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\")\ncolnames(Result2_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\")\ntot &lt;- colSums(Result2_)/nrow(train2)\ntot$Fold &lt;- 'Total'\nResult_ &lt;- rbind(Result_ , tot)\nResult_ &lt;- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] &lt;- 'Improvement'\n\nfor(i in 2:3){\n  Result_[nb.fold+2,i] &lt;-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) &lt;- NULL\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n\n\nTable 7.8: Prediction scores for the GLM-net model\n\n\nFold\nSc.log\nSc.MSE\n\n\n\n\n1\n9.24111\n47.51428\n\n\n2\n9.25252\n21.11620\n\n\n3\n9.27098\n29.80730\n\n\n4\n9.30253\n29.03769\n\n\n5\n9.22546\n17.06356\n\n\nTotal\n9.25871\n29.09671\n\n\nImprovement\n-0.03642\n-1.34022\n\n\n\n\n\n\n\n\nThe value of \\(\\lambda_{1se}\\) is also used to compute the scores on the test set.\n\n\nCode\nglm.score &lt;- as.formula(M_Claim ~ \n                          Car.use + Region + Car.age + I(Car.age^2) + I(Car.age^3) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2) + I(Avgdays.week^3)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + I(max.day^2) + I(max.day^3) \n                        + min.day + I(min.day^2) + I(min.day^3)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\nmatrix.x &lt;- model.matrix(glm.score, data=train2)[,-1]\ny &lt;- train2$M_Claim\n\nlambda.min &lt;- 0.003162278\nlambda.1se &lt;- 0.05011872\n\nlambda.select &lt;- lambda.1se\n#fit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\nfit &lt;- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=TRUE, alpha = 1, lambda = lambda.select)\n\ntrain2$pred &lt;- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*train2$NB_Claim\n\nmatrix.x &lt;- model.matrix(glm.score, data=test2)[,-1]\ny &lt;- test2$M_Claim\n\n\ntest2$pred.base &lt;- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*test2$NB_Claim\nvariance &lt;- (sum((train2$AMT_Claim - train2$pred)^2)/(nrow(train2) - length(fit$beta)))\nphi &lt;- variance/mean(train2$AMT_Claim)^2\n\nResult_ &lt;- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ &lt;- cbind('LASSO (parsimonious)', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all &lt;- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n\n\nTable 7.9: Prediction scores for the GLM-net model (testing set)\n\n\nModel\nSc.log\nSc.MSE\n\n\n\n\nBase\n9.29504\n21.82679\n\n\nGLM (trad.)\n9.23655\n21.16556\n\n\nLASSO (optimal)\n9.22435\n19.83764\n\n\nLASSO (parsimonious)\n9.24845\n20.32402\n\n\n\n\n\n\n\n\n\n\nBy comparing the results obtained with pricing models based solely on traditional variables, we observe an enhancement in prediction quality upon incorporating telematics variables. Nonetheless, we aim to assess whether there is an added advantage in retaining protected segmentation variables even with the availability of telematics data. To gauge this benefit, we compute the residuals of the GLM-Net approach. Specifically, we proceed as follows:\n\nWe fit a GLM-Net model using the available covariates, as previously outlined.\n\nWe predict the expected severity of the model based on the train data.\n\nWe utilize these predictions as an offset variable.\n\nWith this modeling approach, we can now assess whether telematics data effectively eliminates the predictive power of protected variables.\nThe graphs below depict the extent to which protected variables contribute to the residuals from a model utilizing telematics covariates:\n\nCredit score and territory still exhibit a (slight) impact on severity.\n\nThe impact of insured age appears to have been absorbed by telematic covariates.\n\nInsured age and marital status still appear to explain severity.\n\n\n\nCode\n## Credit Score\n\nmeansev.inv &lt;- sum(train2$NB_Claim)/sum(train2$M_Claim)\nmeanpred.inv &lt;- sum(train2$pred.tele)/sum(train2$M_Claim)\n\ntemp2 &lt;- train2 %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(Credit.score/25) * 25) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.tele),\n            nbclaim = n()) %&gt;% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*M_Claim/pred)\n\n\nggplot() + \n  geom_smooth(aes(x=Group, y=sev, weight = nbclaim, color='Observed'),se=F, size=1, data=temp2) + \n  geom_smooth(aes(x=Group, y=sev2, weight = nbclaim, color='Residuals'),se=F, size=1, data=temp2) + \n  labs(x = 'Credit Score',\n       y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  guides(color = guide_legend(title = \"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n## Insured Age\n\ntemp2 &lt;- train2 %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Insured.age = pmin(Insured.age, 80),\n                Group = ceiling(Insured.age/5) * 5) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.tele),\n            nbclaim = n()) %&gt;% \n   mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*M_Claim/pred)\n\nggplot() + \n  geom_smooth(aes(x=Group, y=sev, weight = nbclaim, color='Observed'),se=F, size=1, data=temp2) + \n  geom_smooth(aes(x=Group, y=sev2, weight = nbclaim, color='Residuals'),se=F, size=1, data=temp2) + \n  labs(x = 'Age of the insured',\n       y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  guides(color = guide_legend(title = \"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n## Sex of the insured\n\ntemp &lt;- train2 %&gt;%\n  mutate(Var_ = Insured.sex) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.tele),\n            nbclaim = n()) %&gt;% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n          sev2 = meanpred.inv*M_Claim/pred)\n\ntemp$sev &lt;- temp$sev/temp$sev[1]\ntemp$sev2 &lt;- temp$sev2/temp$sev2[1]\n\nggplot() + #start plot by by plotting bars\n  geom_point(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=0.7) +\n  geom_point(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=0.7) +\n  labs(x = 'Sex of the insured', y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  #ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+\n  guides(color=guide_legend(title=\"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n## Marital\n\ntemp &lt;- train2 %&gt;%\n  mutate(Var_ = Marital) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.tele),\n            nbclaim = n()) %&gt;% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*pred/nbclaim)\n\ntemp$sev &lt;- temp$sev/temp$sev[1]\ntemp$sev2 &lt;- temp$sev2/temp$sev2[1]\n\nggplot() + #start plot by by plotting bars\n  geom_point(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=0.7) +\n  geom_point(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=0.7) +\n  labs(x = 'Marital status of the insured', y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  #ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+\n  guides(color=guide_legend(title=\"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n## Territory\n\ntemp &lt;- train2 %&gt;%\n  mutate(Var_ = Territory) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.tele),\n            nbclaim = n()) %&gt;% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*M_Claim/pred)\n\n#temp$sev &lt;- temp$sev/temp$sev[1]\n#temp$sev2 &lt;- temp$sev2/temp$sev2[1]\n\nggplot() + #start plot by by plotting bars\n  #geom_point(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=0.7) +\n  #geom_point(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=0.7) +\n  labs(x = 'Territory', y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  scale_x_discrete(labels = NULL, breaks = NULL)+\n  #ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+\n  guides(color=guide_legend(title=\"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n\n\n\n\n\n(a) Credit Score\n\n\n\n\n\n\n\n(b) Age of the Insured\n\n\n\n\n\n\n\n\n\n(c) Sex of the Insured\n\n\n\n\n\n\n\n(d) Marital Status of the Insured\n\n\n\n\n\n\n\n\n\n(e) Territory\n\n\n\n\nFigure 7.4: Observed Relativity vs. Residuals Relativity\n\n\n\n\n\nBy utilizing the GLM-Net prediction as an offset variable, we can fit another GLM-Net model, this time exclusively employing the protected variables. The table below presents the predicted scores of the two new models: the LASSO* with optimal \\(\\lambda\\), and the LASSO* with a penalty value situated at one standard error. Overall, we observe a slight improvement in the various scores with the addition of protected variables.\nWe thus fit a GLM-net model using telematics and traditional covariates, and we predict the expected severity of the model on the train database. Using the prediction as an offset variable, we analyze the impact of each variable.\n\n\nCode\nglm.score &lt;- as.formula(M_Claim ~ \n                          Car.use + Region + Car.age + I(Car.age^2) + I(Car.age^3) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2) + I(Avgdays.week^3)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + I(max.day^2) + I(max.day^3) \n                        + min.day + I(min.day^2) + I(min.day^3)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\nmatrix.x &lt;- model.matrix(glm.score, data=train2)[,-1]\ny &lt;- train2$M_Claim\n\n\nlambda.min &lt;- 0.003162278\nlasso.min &lt;- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, alpha = 1, lambda = lambda.min)\ntrain2$pred.tele &lt;- predict(lasso.min, newx = matrix.x, type='response', lambda = lambda.min)\n\n\nmatrix.x &lt;- model.matrix(glm.score, data=test2)[,-1]\ny &lt;- test2$M_Claim\n\ntest2$pred.tele &lt;- predict(lasso.min, newx = matrix.x, type='response', lambda = lambda.min)\n\nglm.score &lt;- as.formula(M_Claim ~ Insured.sex + Marital \n                        + Credit.score +  I(Credit.score^2) \n                        + Insured.age +  I(Insured.age^2) \n                        + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x &lt;- model.matrix(glm.score, data=train2)[,-1]\ny &lt;- train2$M_Claim\noffset &lt;- log(train2$pred.tele)\n\nlambda.min &lt;- 0.003162278\nlambda.1se &lt;- 0.1258925\n\nlambda.select &lt;- lambda.min\n#fit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\nfit &lt;- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n\n\ntrain2$pred &lt;- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)*train2$NB_Claim\n\nmatrix.x &lt;- model.matrix(glm.score, data=test2)[,-1]\ny &lt;- test2$M_Claim\noffset &lt;- log(test2$pred.tele)\n\ntest2$pred.base &lt;- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)*test2$NB_Claim\nvariance &lt;- (sum((train2$AMT_Claim - train2$pred)^2)/(nrow(train2) - length(fit$beta)))\nphi &lt;- variance/mean(train2$AMT_Claim)^2\n\nResult_ &lt;- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ &lt;- cbind('LASSO* (optimal)', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all &lt;- rbind(Result_all, Result_)\n\n###\n\nglm.score &lt;- as.formula(M_Claim ~ Insured.sex + Marital \n                        + Credit.score +  I(Credit.score^2) \n                        + Insured.age + I(Insured.age^2) \n                        + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x &lt;- model.matrix(glm.score, data=train2)[,-1]\ny &lt;- train2$M_Claim\noffset &lt;- log(train2$pred.tele)\n\nlambda.min &lt;- 0.003162278\nlambda.1se &lt;- 0.1258925\n\nlambda.select &lt;- lambda.1se\nfit &lt;- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\n#fit &lt;- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n\ntrain2$pred &lt;- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)*train2$NB_Claim\n\nmatrix.x &lt;- model.matrix(glm.score, data=test2)[,-1]\ny &lt;- test2$M_Claim\noffset &lt;- log(test2$pred.tele)\n\ntest2$pred.base &lt;- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)*test2$NB_Claim\nvariance &lt;- (sum((train2$AMT_Claim - train2$pred)^2)/(nrow(train2) - length(fit$beta)))\nphi &lt;- variance/mean(train2$AMT_Claim)^2\n\nResult_ &lt;- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ &lt;- cbind('LASSO* (parsimonious)', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all &lt;- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n\n\nTable 7.10: Prediction scores for the GLM-net model (testing set)\n\n\nModel\nSc.log\nSc.MSE\n\n\n\n\nBase\n9.29504\n21.82679\n\n\nGLM (trad.)\n9.23655\n21.16556\n\n\nLASSO (optimal)\n9.22435\n19.83764\n\n\nLASSO (parsimonious)\n9.24845\n20.32402\n\n\nLASSO* (optimal)\n9.20054\n19.31021\n\n\nLASSO* (parsimonious)\n9.20843\n19.59126"
  },
  {
    "objectID": "severityVarTelematique.html#xgboost",
    "href": "severityVarTelematique.html#xgboost",
    "title": "7  Telematic Covariates",
    "section": "7.4 XGBoost",
    "text": "7.4 XGBoost\nWe now consider an XGBoost model. As with the frequency model, hyperparameter values are obtained by performing a Bayesian search on a grid of possible values.\n\nPrediction ScoresVariables ImportanceRESIDUALS AND PROTECTED VARIABLESXGBOOST ON RESIDUALS\n\n\nUsing these values, we can calculate the model’s prediction scores based on all classical and telematics covariates. One can see that the XGBoost approach is particularly effective in capturing the effect of all available telematic covariates. Indeed, the scores obtained are significantly improved compared to other tested approaches.\n\n\nCode\nparam &lt;- list(\n  eta = 0.1914908,\n  max_depth = 5,\n  subsample = 0.8674295,\n  min_child_weight = 100.0327,\n  booster = \"gbtree\",\n  objective = \"reg:gamma\",\n  eval_metric = \"gamma-nloglik\")\n\nset.seed(133)\nxgbcv &lt;- xgb.cv(params = param,\n                nrounds = 145,\n                data = dtrain,\n                folds = folds,\n                prediction = TRUE,\n                early_stopping_rounds = 10,\n                verbose = 0,\n                maximize = F)\n  \nvariance &lt;- sapply(xgbcv$folds, function(x){sum((train2$AMT_Claim[x]-unlist(xgbcv$pred[x])*train2$NB_Claim[x])^2)/length(train2$AMT_Claim[x])})  \nmean &lt;- sapply(xgbcv$folds, function(x){mean(train2$AMT_Claim[x])})\nphi &lt;- unlist(variance)/mean^2\n\nSc.log1 &lt;- -dgamma(train2$AMT_Claim[xgbcv$folds$fold1], shape = 1/phi[1], scale = unlist(xgbcv$pred[xgbcv$folds$fold1])*train2$NB_Claim[xgbcv$folds$fold1]*phi[1], log=TRUE)\nSc.log2 &lt;- -dgamma(train2$AMT_Claim[xgbcv$folds$fold2], shape = 1/phi[2], scale = unlist(xgbcv$pred[xgbcv$folds$fold2])*train2$NB_Claim[xgbcv$folds$fold2]*phi[2], log=TRUE)\nSc.log3 &lt;- -dgamma(train2$AMT_Claim[xgbcv$folds$fold3], shape = 1/phi[3], scale = unlist(xgbcv$pred[xgbcv$folds$fold3])*train2$NB_Claim[xgbcv$folds$fold3]*phi[3], log=TRUE)\nSc.log4 &lt;- -dgamma(train2$AMT_Claim[xgbcv$folds$fold4], shape = 1/phi[4], scale = unlist(xgbcv$pred[xgbcv$folds$fold4])*train2$NB_Claim[xgbcv$folds$fold4]*phi[4], log=TRUE)\nSc.log5 &lt;- -dgamma(train2$AMT_Claim[xgbcv$folds$fold5], shape = 1/phi[5], scale = unlist(xgbcv$pred[xgbcv$folds$fold5])*train2$NB_Claim[xgbcv$folds$fold5]*phi[5], log=TRUE)\n\nSc.MSE &lt;- sapply(xgbcv$folds, function(x){(train2$AMT_Claim[x]-unlist(xgbcv$pred[x])*train2$NB_Claim[x])^2/1000000})\n\n \nResult_  &lt;- rbind(\nc(1,mean(Sc.log1), mean(Sc.MSE[1]$fold1)),\nc(2,mean(Sc.log2), mean(Sc.MSE[2]$fold2)),\nc(3,mean(Sc.log3), mean(Sc.MSE[3]$fold3)),\nc(4,mean(Sc.log4), mean(Sc.MSE[4]$fold4)),\nc(5,mean(Sc.log5), mean(Sc.MSE[5]$fold5))\n)\n\nRes.sum  &lt;- rbind(\nc(sum(Sc.log1), sum(Sc.MSE[1]$fold1)),\nc(sum(Sc.log2), sum(Sc.MSE[2]$fold2)),\nc(sum(Sc.log3), sum(Sc.MSE[3]$fold3)),\nc(sum(Sc.log4), sum(Sc.MSE[4]$fold4)),\nc(sum(Sc.log5), sum(Sc.MSE[5]$fold5))\n)\nsum &lt;- c('Total', colSums(Res.sum)/nrow(train2))\n\nResult_  &lt;- data.frame(rbind(Result_, sum)) \n\n## Show results\ncolnames(Result_) &lt;- c('Fold', \"Sc.log\", \"Sc.MSE\")\nResult_ &lt;- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] &lt;- 'Improvement'\n\nfor(i in 2:3){\n  Result_[,i] &lt;- as.numeric(Result_[,i])  \n  Result_[nb.fold+2,i] &lt;-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\nrownames(Result_) &lt;- NULL\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n\n\nTable 7.11: Prediction scores for the XGBoost model with telematics (severity)\n\n\nFold\nSc.log\nSc.MSE\n\n\n\n\n1\n9.32070\n41.62366\n\n\n2\n9.03860\n16.25485\n\n\n3\n9.09227\n22.19734\n\n\n4\n9.06896\n20.72986\n\n\n5\n8.98434\n14.54670\n\n\nTotal\n9.10319\n23.23889\n\n\nImprovement\n-0.19194\n-7.19805\n\n\n\n\n\n\n\n\nThe same model can be used to compute the scores on the test set. We also observe that the XGBoost approach is the most effective.\n\n\nCode\ndtrain &lt;- xgb.DMatrix(data = data.matrix(train2[, paste(all.vars2)]), label = train2$M_Claim)\n#setinfo(dtrain,\"base_margin\",log(train2$expo))\nfolds &lt;-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n\nparam &lt;- list(\n  eta = 0.1914908,\n  max_depth = 5,\n  subsample = 0.8674295,\n  min_child_weight = 100.0327,\n  booster = \"gbtree\",\n  objective = \"reg:gamma\",\n  eval_metric = \"gamma-nloglik\")\n\nset.seed(133)\nfit.xgb &lt;- xgb.train(params = param,\n                     nrounds = 145,\n                     data = dtrain)\n\ntrain2$pred.xgb &lt;- predict(fit.xgb, dtrain, type='response')*train2$NB_Claim\ntrain2$pred.xgb.off &lt;- predict(fit.xgb, dtrain, type='response')\n\ndtest &lt;- xgb.DMatrix(data = data.matrix(test2[, paste(all.vars2)]), label = test2$M_Claim)\n#setinfo(dtest,\"base_margin\",log(test2$expo))\ntest2$pred.xgb &lt;- predict(fit.xgb, dtest, type='response')*test2$NB_Claim\ntest2$pred.xgb.off &lt;- predict(fit.xgb, dtest, type='response')\n\ntest2$pred.base &lt;- test2$pred.xgb\n\nvariance &lt;- (sum((train2$pred.xgb - (train2$AMT_Claim))^2)/(length(train2$AMT_Claim)))\nphi &lt;- variance/mean(train2$AMT_Claim)^2\n\nResult_ &lt;- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ &lt;- cbind('XGBoost', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all &lt;- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n\n\nTable 7.12: Prediction scores for the XGBoost model with telematics (severity)\n\n\nModel\nSc.log\nSc.MSE\n\n\n\n\nBase\n9.29504\n21.82679\n\n\nGLM (trad.)\n9.23655\n21.16556\n\n\nLASSO (optimal)\n9.22435\n19.83764\n\n\nLASSO (parsimonious)\n9.24845\n20.32402\n\n\nLASSO* (optimal)\n9.20054\n19.31021\n\n\nLASSO* (parsimonious)\n9.20843\n19.59126\n\n\nXGBoost\n9.03389\n15.72516\n\n\n\n\n\n\n\n\n\n\nThe graph below illustrates the most important variables in the XGBoost model. We see that the most important telematic variable is related to the vehicle usage intensity, even for severity. Nevertheless, the XGBoost model is capable of identifying a few other important covariates.\n\n\nCode\nimportance_matrix &lt;- xgb.importance(dimnames(dtrain)[[2]], model = fit.xgb)\nxgb.ggplot.importance(importance_matrix,top_n=15) + theme(text = element_text(size=15))\n\n\n\n\n\n\n\nAs we did for the GLM-net model, we can check whether the protected variables we excluded from the analysis retain predictive capacity.\nThe graphs below depict the extent to which protected variables contribute to the residuals from a model utilizing telematics covariates. The conclusions are consistent with those obtained with the GLM-net model: most of the effect seems to be captured by the traditional and telematic variables. However, we see that for the Credit Score variable, there is a loss of information if the variable is not used in the model and that this loss is not completely compensated by the telematics covariates.\n\n\nCode\nmoy.xgb &lt;- sum(train2$pred.xgb.off)/sum(train2$M_Claim)\n\nmeansev.inv &lt;- sum(train2$NB_Claim)/sum(train2$M_Claim)\nmeanpred.inv &lt;- sum(train2$pred.xgb.off)/sum(train2$M_Claim)\n\n\ntemp2 &lt;- train2 %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Insured.age = pmin(Insured.age, 80),\n                Group = ceiling(Credit.score/25) * 25) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.xgb.off),\n            nbclaim = n()) %&gt;% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*M_Claim/pred)\n\nGraph_resCS &lt;- ggplot() + \n  geom_smooth(aes(x=Group, y=sev, weight = nbclaim, color='Observed'),se=F, size=1, data=temp2) + \n  geom_smooth(aes(x=Group, y=sev2, weight = nbclaim, color='Residuals'),se=F, size=1, data=temp2) + \n  labs(x = 'Credit Score',\n       y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  guides(color = guide_legend(title = \"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\nprint(Graph_resCS)\nsave(Graph_resCS, file = \"Data/Graph_resCS_sev.rdata\")\n\n\n### Age of the insured\n\n\nmeansev.inv &lt;- sum(train2$NB_Claim)/sum(train2$M_Claim)\nmeanpred.inv &lt;- sum(train2$pred.xgb.off)/sum(train2$M_Claim)\n\n\ntemp2 &lt;- train2 %&gt;%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Insured.age = pmin(Insured.age, 80),\n                Group = ceiling(Insured.age/5) * 5) %&gt;%\n  group_by(Group) %&gt;% \n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.xgb.off),\n            nbclaim = n()) %&gt;% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*M_Claim/pred)\n\nGraph_resAge &lt;- ggplot() + \n  geom_smooth(aes(x=Group, y=sev, weight = nbclaim, color='Observed'),se=F, size=1, data=temp2) + \n  geom_smooth(aes(x=Group, y=sev2, weight = nbclaim, color='Residuals'),se=F, size=1, data=temp2) + \n  labs(x = 'Insured.age',\n       y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  guides(color = guide_legend(title = \"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\nprint(Graph_resAge)\nsave(Graph_resAge, file = \"Data/Graph_resAge_sev.rdata\")\n\n###\n\ntemp &lt;- train2 %&gt;%\n  mutate(Var_ = Insured.sex) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.xgb),\n            nbclaim = n()) %&gt;% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*M_Claim/pred)\n\ntemp$sev &lt;- temp$sev/temp$sev[1]\ntemp$sev2 &lt;- temp$sev2/temp$sev2[1]\n\nggplot() + #start plot by by plotting bars\n  geom_point(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=0.7) +\n  geom_point(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=0.7) +\n  labs(x = 'Sex of the insured', y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  #ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+\n  guides(color=guide_legend(title=\"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n###\n\n temp &lt;- train2 %&gt;%\n   mutate(Var_ = Marital) %&gt;%\n   group_by(Var_) %&gt;%\n   summarize(M_Claim=sum(M_Claim),\n             pred=sum(pred.xgb),\n             nbclaim = n()) %&gt;% \n   mutate(sev = meansev.inv*M_Claim/nbclaim,\n          sev2 = meanpred.inv*M_Claim/pred)\n \n temp$sev &lt;- temp$sev/temp$sev[1]\n temp$sev2 &lt;- temp$sev2/temp$sev2[1]\n \n ggplot() + #start plot by by plotting bars\n   geom_point(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=3) +\n   geom_line(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=0.7) +\n   geom_point(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=3) +\n   geom_line(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=0.7) +\n   labs(x = 'Marital status of the insured', y = 'Relativity') +\n   geom_hline(yintercept = 1, linetype='dashed')+\n   #ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+\n   guides(color=guide_legend(title=\"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n \n###\n\ntemp &lt;- train2 %&gt;%\n  mutate(Var_ = Territory) %&gt;%\n  group_by(Var_) %&gt;%\n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.xgb.off),\n            nbclaim = n()) %&gt;% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*M_Claim/pred)\n\n#temp$sev &lt;- temp$sev/temp$sev[1]\n#temp$sev2 &lt;- temp$sev2/temp$sev2[1]\n\nGraph_resTerr &lt;- ggplot() + #start plot by by plotting bars\n  #geom_point(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=0.7) +\n  #geom_point(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=0.7) +\n  labs(x = 'Territory', y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  scale_x_discrete(labels = NULL, breaks = NULL)+\n  #ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+\n  guides(color=guide_legend(title=\"\")) +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\nprint(Graph_resTerr)\nsave(Graph_resTerr, file = \"Data/Graph_resTerr_sev.rdata\")\n\n\n\n\n\n\n\n\n(a) Credit Score\n\n\n\n\n\n\n\n(b) Age of the Insured\n\n\n\n\n\n\n\n\n\n(c) Sex of the Insured\n\n\n\n\n\n\n\n(d) Marital Status of the Insured\n\n\n\n\n\n\n\n\n\n(e) Territory\n\n\n\n\nFigure 7.5: Observed Relativity vs. Residuals Relativity\n\n\n\n\n\nAs we did with the GLM-Net approach, we use the predictions of the XGBoost model as an offset variable and fit another XGBoost model on the data, using only the protected covariates. The prediction scores are show in the table below. We still observe a slight improvement in prediction scores, indicating that protected variables still retain some predictive power despite the use of telematic information.\n\n\nCode\nvar.sens &lt;- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\")    \n\ndtrain &lt;- xgb.DMatrix(data = data.matrix(train2[, paste(var.sens)]), label = train2$M_Claim)\nsetinfo(dtrain,\"base_margin\",log(train2$pred.xgb.off))\nfolds &lt;-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n\nparam &lt;- list(\n  eta = 0.02337437,\n  max_depth = 26,\n  subsample = 0.8097923,\n  min_child_weight = 123.6033,\n  booster = \"gbtree\",\n  objective = \"reg:gamma\",\n  eval_metric = \"gamma-nloglik\")\n\nset.seed(533)\nfit.xgb2 &lt;- xgb.train(params = param,\n                      nrounds = 103,\n                      data = dtrain)\n\nvar.sens &lt;- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\")    \n\ntrain2$pred.xgb &lt;- predict(fit.xgb2, dtrain, type='response')*train2$NB_Claim\n\ndtest &lt;- xgb.DMatrix(data = data.matrix(test2[, paste(var.sens)]), label = test2$M_Claim)\nsetinfo(dtest,\"base_margin\",log(test2$pred.xgb.off))\n\ntest2$pred.base &lt;- predict(fit.xgb2, dtest, type='response')*test2$NB_Claim\n\nvariance &lt;- (sum((train2$pred.xgb - (train2$AMT_Claim))^2)/(length(train2$AMT_Claim)))\nphi &lt;- variance/mean(train2$AMT_Claim)^2\n\n\n\n\nCode\nResult_ &lt;- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ &lt;- cbind('XGBoost*', Result_)\ncolnames(Result_) &lt;- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all &lt;- rbind(Result_all, Result_)\n\nsave(Result_all, file='Data/ResultsSynth_sev.Rda')\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n\n\nTable 7.13: Prediction scores for the XGBoost model with telematics. (severity)\n\n\nModel\nSc.log\nSc.MSE\n\n\n\n\nBase\n9.29504\n21.82679\n\n\nGLM (trad.)\n9.23655\n21.16556\n\n\nLASSO (optimal)\n9.22435\n19.83764\n\n\nLASSO (parsimonious)\n9.24845\n20.32402\n\n\nLASSO* (optimal)\n9.20054\n19.31021\n\n\nLASSO* (parsimonious)\n9.20843\n19.59126\n\n\nXGBoost\n9.03389\n15.72516\n\n\nXGBoost*\n9.01906\n15.40489\n\n\n\n\n\n\n\n\n\n\n\nThe graph below illustrates the most important protected variables in the XGBoost model. Unsurprisingly, the insured’s sex and marital status come at the very bottom. Additionally, the most important protected variable is credit score. These results are consistent with the conclusions of the GLM-net model.\n\n\nCode\nimportance_matrix &lt;- xgb.importance(dimnames(dtrain)[[2]], model = fit.xgb2)\nxgb.ggplot.importance(importance_matrix,top_n=15) + theme(text = element_text(size=15))"
  },
  {
    "objectID": "Discussion.html#validation-on-the-original-dataset",
    "href": "Discussion.html#validation-on-the-original-dataset",
    "title": "8  Conclusion",
    "section": "8.1 Validation on the original dataset",
    "text": "8.1 Validation on the original dataset\nWe conducted our analysis on a synthetic database constructed from an actual database from a Canadian insurer. The use of synthetic databases in actuarial science is slowly developing (e.g., Individual Claims History Simulation Machine from (Gabrielli and V. Wuthrich 2018) and SynthETIC: Synthetic Experience Tracking Insurance Claims on CRAN), but the history is still short. Thus, we consider that the conclusions must be validated on an actual database. Indeed, we want to prevent the mechanics that led to these artificial databases from creating distortions between variables. For example, one could imagine that a database created using a particular technique favors models using that same technique.\nOverall, for both frequency and severity, we observe similar links between the average response variable by group and each of the explanatory variables, taken individually. This result is not surprising and confirms that the database creation process was correctly constructed. The correlation between the variables Credit.score and Insured.age is also observed in the original database.\nThe results of the first model containing only the categorical covariates are similar to those obtained with the synthetic database. In particular, we note the weak impact (but which does not seem zero) of the Marital variable on both frequency and severity.\nOverall, adding continuous variables leads to the same conclusions as those obtained from the synthetic data. Nevertheless, we can raise a yellow flag: while the XGBoost model performs significantly better than the other models for the simulated data, this is not the case on the actual data (see tables below). Indeed, there is a slight improvement, but the cost/benefit ratio works against the XGBoost model for actual data. Having not thoroughly analyzed the method for creating the synthetic database, we can not offer a clear explanation for this phenomenon.\nMost conclusions obtained regarding the impact of telematics on the usefulness of protected variables remain valid on the original database with the following caveat: the effect is sometimes less significant on the actual data."
  },
  {
    "objectID": "Discussion.html#claims-frequency",
    "href": "Discussion.html#claims-frequency",
    "title": "8  Conclusion",
    "section": "8.2 Claims Frequency",
    "text": "8.2 Claims Frequency\nThe table below illustrates the various scores achieved for each database, allowing us to delve deeper into the comparison between synthetic data and real data. The XGBoost model appears to perform the best for both databases; however, as previously mentioned, it exhibits even better performance with synthetic data.\n\n\nCode\nload('Data/ResultsSynth.Rda')\nSynth &lt;- Result_all[,1:3]\n\nload('Data/ResultsReal.Rda')\nReal &lt;- Result_all[,2:3]\n\nResult_all &lt;- cbind(Synth, Real)\nResult_all &lt;- Result_all[,c(1,2,4,3,5)]\ncolnames(Result_all) &lt;- c('Model', 'Synthetic Data', 'Original Data', 'Synthetic Data', 'Original Data')\n\nknitr::kable(Result_all, align = \"ccccc\", digits = c(0, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  %&gt;%\n  add_header_above(c(\"\", \"Sc.log\" = 2, \"Sc.MSE\" = 2)) \n\n\n\n\nTable 8.1: Prediction scores (frequency)\n\n\n\n\n\n\n\n\n\n\n\nSc.log\n\n\nSc.MSE\n\n\n\nModel\nSynthetic Data\nOriginal Data\nSynthetic Data\nOriginal Data\n\n\n\n\nBase\n0.17674\n0.16794\n0.04545\n0.04206\n\n\nGLM (trad.)\n0.17359\n0.16564\n0.04514\n0.04186\n\n\nLASSO (optimal)\n0.15536\n0.15152\n0.04239\n0.03992\n\n\nLASSO (parsimonious)\n0.15704\n0.15187\n0.04261\n0.04014\n\n\nLASSO* (optimal)\n0.15373\n0.15072\n0.04209\n0.03973\n\n\nLASSO* (parsimonious)\n0.15481\n0.15152\n0.04226\n0.03992\n\n\nXGBoost\n0.12142\n0.14995\n0.03110\n0.03935\n\n\nXGBoost*\n0.12373\n0.14893\n0.03250\n0.03909\n\n\n\n\n\n\n\n\n\n8.2.1 Residuals and Protected variables\nWe can also revisit some of the graphs developed in previous chapters to compare better the results obtained with the two databases. One of the most critical graphs is the analysis of model residuals concerning protected covariates.\nAs a reminder, we utilize a model’s prediction as an offset variable and assess whether the protected covariates still appear to capture a trend. If the resulting curve is horizontal and close to 1 for all possible values of a protected covariate, it indicates that telemetric variables seem to have captured that covariate’s predictive capacity.\nThe various tabs below thus compare the residual curves for real and synthetic data. The main difference lies in the credit score: we observe that reducing its impact through the addition of telemetric information is more significant with synthetic data than with real data.\n\nCredit ScoreAge of the InsuredSex of the InsuredMarital StatusTerritory\n\n\n\n\nCode\nload(\"Data/Graph_resCS.rdata\")\nprint(Graph_resCS)\nload(\"Data/Graph_resCS2.rdata\")\nprint(Graph_resCS)\n\n\n\n\n\n\n\nFigure 8.1: Synthetic Dataset\n\n\n\n\n\n\n\nFigure 8.2: Original Dataset\n\n\n\n\n\n\n\n\n\n\nCode\nload(\"Data/Graph_resAge.rdata\")\nprint(Graph_resAge)\nload(\"Data/Graph_resAge2.rdata\")\nprint(Graph_resAge)\n\n\n\n\n\n\n\nFigure 8.3: Synthetic Dataset\n\n\n\n\n\n\n\nFigure 8.4: Original Dataset\n\n\n\n\n\n\n\n\n\n\nCode\nload(\"Data/Graph_resSex.rdata\")\nprint(Graph_resSex)\nload(\"Data/Graph_resSex2.rdata\")\nprint(Graph_resSex)\n\n\n\n\n\n\n\nFigure 8.5: Synthetic Dataset\n\n\n\n\n\n\n\nFigure 8.6: Original Dataset\n\n\n\n\n\n\n\n\n\n\nCode\nload(\"Data/Graph_resMarital.rdata\")\nprint(Graph_resMarital)\nload(\"Data/Graph_resMarital2.rdata\")\nprint(Graph_resMarital)\n\n\n\n\n\n\n\nFigure 8.7: Synthetic Dataset\n\n\n\n\n\n\n\nFigure 8.8: Original Dataset\n\n\n\n\n\n\n\n\n\n\nCode\nload(\"Data/Graph_resTerr.rdata\")\nprint(Graph_resTerr)\nload(\"Data/Graph_resTerr2.rdata\")\nprint(Graph_resTerr)\n\n\n\n\n\n\n\nFigure 8.9: Synthetic Dataset\n\n\n\n\n\n\n\nFigure 8.10: Original Dataset"
  },
  {
    "objectID": "Discussion.html#claims-severity",
    "href": "Discussion.html#claims-severity",
    "title": "8  Conclusion",
    "section": "8.3 Claims Severity",
    "text": "8.3 Claims Severity\nWe can also compare different models for severity. The table below indicates the prediction scores obtained for various models across both databases. The conclusion aligns with the frequency analysis: the XGBoost model produces the best prediction scores, but the improvement in predictive quality is more significant with synthetic data than with original data. For instance, concerning real data, we observe that the scores obtained for the LASSO model (optimal) are pretty close to those achieved by XGBoost. In contrast, the difference between these two models is much more pronounced with synthetic data.\n\n\nCode\nload('Data/ResultsSynth_sev.Rda')\nSynth &lt;- Result_all[,1:3]\n\nload('Data/ResultsReal_sev.Rda')\nReal &lt;- Result_all[,2:3]\n\nResult_all &lt;- cbind(Synth, Real)\nResult_all &lt;- Result_all[,c(1,2,4,3,5)]\ncolnames(Result_all) &lt;- c('Model', 'Synthetic Data', 'Original Data', 'Synthetic Data', 'Original Data')\n\nknitr::kable(Result_all, align = \"ccccc\", digits = c(0, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%&gt;%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  %&gt;%\n  add_header_above(c(\"\", \"Sc.log\" = 2, \"Sc.MSE\" = 2)) \n\n\n\n\nTable 8.2: Prediction scores (severity)\n\n\n\n\n\n\n\n\n\n\n\nSc.log\n\n\nSc.MSE\n\n\n\nModel\nSynthetic Data\nOriginal Data\nSynthetic Data\nOriginal Data\n\n\n\n\nBase\n9.29504\n9.51319\n21.82679\n62.21956\n\n\nGLM (trad.)\n9.23655\n9.46042\n21.16556\n61.55924\n\n\nLASSO (optimal)\n9.22435\n9.46367\n19.83764\n58.13092\n\n\nLASSO (parsimonious)\n9.24845\n9.50317\n20.32402\n60.99186\n\n\nLASSO* (optimal)\n9.20054\n9.42978\n19.31021\n56.38442\n\n\nLASSO* (parsimonious)\n9.20843\n9.44597\n19.59126\n57.90176\n\n\nXGBoost\n9.03389\n9.41885\n15.72516\n56.39918\n\n\nXGBoost*\n9.01906\n9.39847\n15.40489\n55.87990\n\n\n\n\n\n\n\n\n\n8.3.1 Residuals and Protected variables\nIn the tabs below, similar to frequency analysis, one can consult the trend analysis graph of residuals based on protected covariates. The results are similar, except for the credit score, which remains more significant in severity modeling for real data.\n\nCredit ScoreAge of the insuredSex of the InsuredMarital StatusTerritory\n\n\n\n\nCode\nload(\"Data/Graph_resCS_sev.rdata\")\nprint(Graph_resCS)\nload(\"Data/Graph_resCS_sev2.rdata\")\nprint(Graph_resCS)\n\n\n\n\n\n\n\nFigure 8.11: Synthetic Dataset\n\n\n\n\n\n\n\nFigure 8.12: Original Dataset\n\n\n\n\n\n\n\n\n\n\nCode\nload(\"Data/Graph_resAge_sev.rdata\")\nprint(Graph_resAge)\nload(\"Data/Graph_resAge_sev2.rdata\")\nprint(Graph_resAge)\n\n\n\n\n\n\n\nFigure 8.13: Synthetic Dataset\n\n\n\n\n\n\n\nFigure 8.14: Original Dataset\n\n\n\n\n\n\n\n\n\n\nCode\nload(\"Data/Graph_resSex_sev.rdata\")\nprint(Graph_resSex)\nload(\"Data/Graph_resSex_sev2.rdata\")\nprint(Graph_resSex)\n\n\n\n\n\n\n\nFigure 8.15: Synthetic Dataset\n\n\n\n\n\n\n\nFigure 8.16: Original Dataset\n\n\n\n\n\n\n\n\n\n\nCode\nload(\"Data/Graph_resMarital_sev.rdata\")\nprint(Graph_resMarital)\nload(\"Data/Graph_resMarital_sev2.rdata\")\nprint(Graph_resMarital)\n\n\n\n\n\n\n\nFigure 8.17: Synthetic Dataset\n\n\n\n\n\n\n\nFigure 8.18: Original Dataset\n\n\n\n\n\n\n\n\n\n\nCode\nload(\"Data/Graph_resTerr_sev.rdata\")\nprint(Graph_resTerr)\nload(\"Data/Graph_resTerr_sev2.rdata\")\nprint(Graph_resTerr)\n\n\n\n\n\n\n\nFigure 8.19: Synthetic Dataset\n\n\n\n\n\n\n\nFigure 8.20: Original Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\nGabrielli, Andrea, and Mario V. Wuthrich. 2018. “An Individual Claims History Simulation Machine.” Risks 6 (2): 29."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ayuso, Mercedes, Montserrat Guillen, and Ana Maria Perez Marin. 2016.\n“Using GPS Data to Analyse the Distance Travelled to the First\nAccident at Fault in Pay-as-You-Drive Insurance.”\nTransportation Research Part C: Emerging Technologies 68:\n160–67.\n\n\nAyuso, Mercedes, Montserrat Guillen, and Jens Perch Nielsen. 2019.\n“Improving Automobile Insurance Ratemaking Using Telematics:\nIncorporating Mileage and Driver Behaviour Data.”\nTransportation 46 (3): 735–52.\n\n\nAyuso, Mercedes, Montserrat Guillen, and Ana Maria Perez-Marin. 2014.\n“Time and Distance to First Accident and Driving Patterns of Young\nDrivers with Pay-as-You-Drive Insurance.” Accident Analysis\n& Prevention 73: 125–31.\n\n\n———. 2016. “Telematics and Gender Discrimination: Some Usage-Based\nEvidence on Whether Men’s Risk of Accidents Differs from\nWomen’s.” Risks 4 (2).\n\n\nBoucher, Jean-Philippe, Steven Côte, and Montserrat Guillen. 2017.\n“Exposure as Duration and Distance in Telematics Motor Insurance\nUsing Generalized Additive Models.” Risks 5 (4).\n\n\nBoucher, Jean-Philippe, and Michel Denuit. 2007. “Duration\nDependence Models for Claim Counts.” Blätter Der\nDGVFM 28 (1): 29–45.\n\n\nBoucher, Jean-Philippe, Ana Maria Perez-Marin, and Miguel Santolino.\n2013. “Pay-as-You-Drive Insurance: The Effect of the Kilometers on\nthe Risk of Accident.” In Anales Del Instituto de Actuarios\nEspanoles, 19:135–54. 3. Instituto de Actuarios Espanoles Madrid.\n\n\nBoucher, Jean-Philippe, and Roxane Turcotte. 2020. “A Longitudinal\nAnalysis of the Impact of Distance Driven on the Probability of Car\nAccidents.” Risks 8 (3): 91.\n\n\nChibanda, Kudakwashe F. 2022. “Defining Discrimination in\nInsurance.” Cas Research Paper: A Special Series On Race And\nInsurance Pricing.\n\n\nCzado, Claudia, Tilmann Gneiting, and Leonhard Held. 2009.\n“Predictive Model Assessment for Count Data.”\nBiometrics 65 (4): 1254–61.\n\n\nDuval, Francis, Jean-Philippe Boucher, and Mathieu Pigeon. 2022.\n“How Much Telematics Information Do Insurers Need for Claim\nClassification?” North American Actuarial Journal 26\n(4): 570–90.\n\n\n———. 2023a. “Enhancing Claim Classification with Feature\nExtraction from Anomaly-Detection-Derived Routine and Peculiarity\nProfiles.” Journal of Risk and Insurance 90 (2): 421–58.\n\n\n———. 2023b. “Telematics Combined Actuarial Neural Networks for\nCross-Sectional and Longitudinal Claim Count Data.” arXiv\nPreprint arXiv:2308.01729.\n\n\nEmbrechts, Paul, and Mario V Wuthrich. 2022. “Recent Challenges in\nActuarial Science.” Annual Review of Statistics and Its\nApplication 9: 119–40.\n\n\nGabrielli, Andrea, and Mario V. Wuthrich. 2018. “An Individual\nClaims History Simulation Machine.” Risks 6 (2): 29.\n\n\nGao, Guangyuan, Shengwang Meng, and Mario V Wuthrich. 2019.\n“Claims Frequency Modeling Using Telematics Car Driving\nData.” Scandinavian Actuarial Journal 2019 (2): 143–62.\n\n\nGao, Guangyuan, He Wang, and Mario V Wuthrich. 2022. “Boosting\nPoisson Regression Models with Telematics Car Driving Data.”\nMachine Learning 111 (2): 243–72.\n\n\nGao, Guangyuan, and Mario V Wuthrich. 2019. “Convolutional Neural\nNetwork Classification of Telematics Car Driving Data.”\nRisks 7 (1).\n\n\nGrari, Vincent, Arthur Charpentier, and Marcin Detyniecki. 2022.\n“A Fair Pricing Model via Adversarial Learning.” arXiv\nPreprint arXiv:2202.12008.\n\n\nGuillen, Montserrat, Jens Perch Nielsen, Mercedes Ayuso, and Ana M\nPerez-Marin. 2019. “The Use of Telematics Devices to Improve\nAutomobile Insurance Rates.” Risk Analysis 39 (3):\n662–72.\n\n\nGuillen, Montserrat, Jens Perch Nielsen, and Ana M Perez-Marin. 2021.\n“Near-Miss Telematics in Motor Insurance.” Journal of\nRisk and Insurance 88 (3): 569–89.\n\n\nHastie, Trevor, Robert Tibshirani, Jerome H Friedman, and Jerome H\nFriedman. 2009. The Elements of Statistical Learning: Data Mining,\nInference, and Prediction. Vol. 2. Springer.\n\n\nHuang, Yifan, and Shengwang Meng. 2019. “Automobile Insurance\nClassification Ratemaking Based on Telematics Driving Data.”\nDecision Support Systems 127: 113–56.\n\n\nLemaire, Jean. 1985. Automobile Insurance: Actuarial Models.\nBoston: Kluwer.\n\n\nLemaire, Jean, Sojung Carol Park, and Kili C. Wang. 2016. “The Use\nof Annual Mileage as a Rating Variable.” ASTIN Bulletin\n46 (1): 39–69.\n\n\nLichtenstein, Ellen. 2022. “Which States Ban Gender-Rating in\nInsurance Premiums?” AgentSync.\n\n\nLindholm, Mathias, Ronald Richman, Andreas Tsanakas, and Mario V\nWuthrich. 2022. “Discrimination-Free Insurance Pricing.”\nASTIN Bulletin: The Journal of the IAA 52 (1): 55–89.\n\n\nOueini, Christian, and Mathieu Pigeon. 2023. “Modèles Equitables\nde Tarification En Assurance Automobile.” Master’s Thesis -\nUniversite Du Quebec à Montreal.\n\n\nPaefgen, Johannes, Thorsten Staake, and Elgar Fleisch. 2014.\n“Multivariate Exposure Modeling of Accident Risk: Insights from\nPay-as-You-Drive Insurance Data.” Transportation Research\nPart A: Policy and Practice 61: 27–40.\n\n\nPaefgen, Johannes, Thorsten Staake, and Frederic Thiesse. 2013.\n“Evaluation and Aggregation of Pay-as-You-Drive Insurance Rate\nFactors: A Classification Analysis Approach.” Decision\nSupport Systems 56: 192–201.\n\n\nReid, T. R. 1985. “Montana Implements Policy of ‘Unisex‘\nInsurance.” Washington Post.\n\n\nSo, Banghee, Jean-Philippe Boucher, and Emiliano A Valdez. 2021.\n“Cost-Sensitive Multi-Class Adaboost for Understanding Driving\nBehavior Based on Telematics.” ASTIN Bulletin: The Journal of\nthe IAA 51 (3): 719–51.\n\n\nTselentis, Dimitrios I, George Yannis, and Eleni I Vlahogianni. 2016.\n“Innovative Insurance Schemes: Pay as/How You Drive.”\nTransportation Research Procedia 14: 362–71.\n\n\nVerbelen, Roel, Katrien Antonio, and Gerda Claeskens. 2018.\n“Unravelling the Predictive Power of Telematics Data in Car\nInsurance Pricing.” Journal of the Royal Statistical Society\nSeries C: Applied Statistics 67 (5): 1275–1304.\n\n\nWu, Cheng-Sheng Peter, and JC Guszcza. 2003. “Does Credit Score\nReally Explain Insurance Losses? Multivariate Analysis from a Data\nMining Point of View.” In Proceedings of the Casualty\nActuarial Society, 113–38.\n\n\nWuthrich, Mario V. 2017. “Covariate Selection from Telematics Car\nDriving Data.” European Actuarial Journal 7: 89–108.\n\n\nZou, Hui, and Trevor Hastie. 2003. “Regression Shrinkage and\nSelection via the Elastic Net, with Applications to Microarrays.”\nJournal of the Royal Statistical Society Series B 67: 301–20."
  },
  {
    "objectID": "Proposal.html",
    "href": "Proposal.html",
    "title": "Initial Proposal",
    "section": "",
    "text": "Introduction\nWe believe that the entire project can be broken down into milestone stages, defined below:"
  },
  {
    "objectID": "Proposal.html#main-objectives",
    "href": "Proposal.html#main-objectives",
    "title": "Initial Proposal",
    "section": "Main Objectives",
    "text": "Main Objectives\n\nOur main objective is to combine and extend the models developed in (Oueini and Pigeon 2023) by including the most critical telematics variables identified in the series of papers by (Duval, Boucher, and Pigeon 2022), (Duval, Boucher, and Pigeon 2023a) and (Duval, Boucher, and Pigeon 2023b). More specifically:\n\nwe plan to precisely and adequately describe, to North American actuaries, the problem of insurance pricing with unjustified discrimination by using real insurance data;\nwe plan to show, for different coverages (collision, comprehensive, accident benefits, etc.), the impact of the introduction of telematics data into automobile insurance pricing algorithms by checking if the additional telematic information reduces the importance given to classic segmentation variables, such as age, gender, or territory;\nwe plan to develop new penalty metrics to measure the level of fairness in a pricing model by first giving an overview of available algorithms penalizing unjustified segmentation."
  },
  {
    "objectID": "Proposal.html#proposed-approach",
    "href": "Proposal.html#proposed-approach",
    "title": "Initial Proposal",
    "section": "Proposed Approach",
    "text": "Proposed Approach\n\n\nTelematic Data\nA portion of the project can be seen as a generalization of the approaches of (Duval, Boucher, and Pigeon 2022) and (Duval, Boucher, and Pigeon 2023a), where the claims frequency and the claims severity will both be analyzed with telematics data. We will carry out a combination of telematics information with classic segmentation variables for these models. In order to obtain results that can easily be understood and interpreted, we will perform the selection of variables using the algorithms used in (Duval, Boucher, and Pigeon 2022) and (Duval, Boucher, and Pigeon 2023a), namely the Elastic-net regularization that can be seen as a compromise between Ridge and lasso penalties (see (Zou and Hastie 2003)).\nIn connection with the conclusions obtained in these papers and those obtained by (Boucher and Turcotte 2020), we will give particular importance to the distance traveled by each vehicle for protections linked to at-fault and non-fault accidents. Although maximum speed, average speed, or distracted driving are information that could make it possible to quantify the risk of accidents properly, several scientific articles have shown that the actual distance traveled is probably the most relevant telematics information. The combination of the distance traveled with the classic segmentation variables will thus correspond to our benchmark.\n\n\nModel Fairness\n(Oueini and Pigeon 2023) and (Grari, Charpentier, and Detyniecki 2022) suggest several metrics to measure the level of fairness of an actuarial model used in a pricing context. In particular, penalties can be applied during model training to improve some of these metrics. Thus, for example, in (Oueini and Pigeon 2023), a Kullback-Leibler penalty is used to reduce the distortion a pricing model introduces between the accident probability of a sensitive variable (gender and age) observed in the data and predicted by a machine learning model. We will adapt the models of (Oueini and Pigeon 2023) to telematics data and then generalize the models to the necessary distributions to see how introducing a fairness penalty impacts the classic covariates.\n\n\nAnalyzed Coverages\nInitially, we will analyze the collision coverage only (fault and not-at-fault). Subsequently, the analysis extends to comprehensive coverage for which we conjecture that the impact of telematics will be much less. Finally, we will analyze accident benefits and bodily injury coverages by separating the frequency and severity components. Indeed, for these two coverages, the risk comes in a significant proportion from the severity component linked to the victim’s salary and, therefore, more or less directly to sensitive variables. Thus, we think telematics’s impact will once again be limited."
  }
]