# Telematic Covariates

## Preamble

::: {.panel-tabset}

### Chapter Objective

We continue the analysis of claim frequency by adding telematic variables to the same three approaches of the last chapter. However, we will remove protected traditional variables from our analysis. Specifically, we will not use the following five covariates in our models:  

  1) Credit.score,    
  2) Insured.age,  
  3) Insured.sex,  
  4) Marital,  
  5) Territory.

The objective is to assess the performance of ratemaking approaches incorporating telematics information without protected covariates. By analyzing the residuals of the approach, we will gauge the relevance of those protected covariates.

To compare models, we will employ the following functions that calculate predictive scores:

```{r}
#| echo: true
#| message: FALSE
#| warning: FALSE
#| code-fold: true

Score.pred <- function(mu, x) {
  Sc.log  <- -sum(dpois(x, mu, log=TRUE))
  Sc.MSE  <- sum((x - mu)^2)
  Sc.quad <- sum(-2*dpois(x,lambda=mu) + sapply(mu, function(x){ sum(dpois(0:10,lambda=x)^2) }))
  Sc.sph <- sum(- dpois(x,mu) / sqrt(sapply(mu, function(x){ sum(dpois(0:10,lambda=x)^2) })))
  Sc.DSS <- sum(dss_pois(x, mu))
  Sc.CRPS <- sum(crps_pois(x, mu))
    
  return(c(Sc.log, Sc.MSE, Sc.quad, Sc.sph, Sc.DSS, Sc.CRPS))
}


```

### Packages

Here is the list of packages that will be used:

```{r}
#| echo: true
#| message: FALSE
#| warning: FALSE
#| code-fold: true

library(tidyverse)
library(vtable)
library(rpart)
library(repr)
library(rpart.plot)
library(gam)
library(knitr)
library(kableExtra)
library(janitor)
library(glmnet)
library(scoringRules)
library(sjPlot)

```

### Data

The same data is used.

```{r}
#| echo: true
#| message: FALSE
#| warning: FALSE
#| code-fold: true

dataS <- read.csv('Data/Synthetic.csv')

# Modifications 
dataS <- dataS %>%
  mutate(Territory = as.factor(Territory)) %>%
  select(-c('Annual.pct.driven', 'Annual.miles.drive'))
data.select <- dataS

# Train-test 
set.seed(123)
train <- data.select %>% sample_frac(0.8, replace = FALSE)
test <- data.select %>% anti_join(train)
```

### Data Transformation

As we concluded at the end of our overview of the data, a transformation of certain variables is also necessary.

```{r}
#| echo: true
#| message: FALSE
#| warning: FALSE
#| code-fold: true

# Modif data
train2 <- train %>%
  mutate(Miles.per.day = Total.miles.driven/Duration,
         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),
         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),
         max.min = max.day - min.day,
         Dayformax = 'Monday', 
         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),
         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),
         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),
         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),
         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),
         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),
         Dayformin = 'Monday', 
         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),
         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),
         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),
         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),
         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),
         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),
         expo = Duration/365.25)

transform.fct <- function(var){
  df <- train2 %>% mutate(var_ = get(var)*Total.miles.driven/(1000*Duration))
  q99 <- quantile(df$var_, 0.99)
  df <- df %>% mutate(var_ = ifelse(var_ > q99, q99, var_))
  #colnames(df)[ncol(df)] <- paste0(var, '_')
  return(df)
}

train2 <- transform.fct("Brake.06miles")
train2 <- transform.fct("Brake.08miles")
train2 <- transform.fct("Brake.09miles")
train2 <- transform.fct("Brake.11miles")
train2 <- transform.fct("Brake.14miles")
train2 <- transform.fct("Accel.06miles")
train2 <- transform.fct("Accel.08miles")
train2 <- transform.fct("Accel.09miles")
train2 <- transform.fct("Accel.11miles")
train2 <- transform.fct("Accel.12miles")
train2 <- transform.fct("Accel.14miles")
train2 <- transform.fct("Left.turn.intensity08")
train2 <- transform.fct("Left.turn.intensity09")
train2 <- transform.fct("Left.turn.intensity10")
train2 <- transform.fct("Left.turn.intensity11")
train2 <- transform.fct("Left.turn.intensity12")
train2 <- transform.fct("Right.turn.intensity08")
train2 <- transform.fct("Right.turn.intensity09")
train2 <- transform.fct("Right.turn.intensity10")
train2 <- transform.fct("Right.turn.intensity11")
train2 <- transform.fct("Right.turn.intensity12")

# Create folds
nb.fold <- 5
fold <- sample(1:nb.fold, nrow(train2), replace = TRUE)
train2$fold <- fold

##

test2 <- test %>%
  mutate(Miles.per.day = Total.miles.driven/Duration,
         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),
         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),
         max.min = max.day - min.day,
         Dayformax = 'Monday', 
         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),
         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),
         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),
         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),
         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),
         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),
         Dayformin = 'Monday', 
         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),
         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),
         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),
         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),
         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),
         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),
         expo = Duration/365.25)

transform.fct <- function(var){
  df <- test2 %>% mutate(var_ = get(var)*Total.miles.driven/(1000*Duration))
  q99 <- quantile(df$var_, 0.99)
  df <- df %>% mutate(var_ = ifelse(var_ > q99, q99, var_))
  #colnames(df)[ncol(df)] <- paste0(var, '_')
  return(df)
}

test2 <- transform.fct("Brake.06miles")
test2 <- transform.fct("Brake.08miles")
test2 <- transform.fct("Brake.09miles")
test2 <- transform.fct("Brake.11miles")
test2 <- transform.fct("Brake.14miles")
test2 <- transform.fct("Accel.06miles")
test2 <- transform.fct("Accel.08miles")
test2 <- transform.fct("Accel.09miles")
test2 <- transform.fct("Accel.11miles")
test2 <- transform.fct("Accel.12miles")
test2 <- transform.fct("Accel.14miles")
test2 <- transform.fct("Left.turn.intensity08")
test2 <- transform.fct("Left.turn.intensity09")
test2 <- transform.fct("Left.turn.intensity10")
test2 <- transform.fct("Left.turn.intensity11")
test2 <- transform.fct("Left.turn.intensity12")
test2 <- transform.fct("Right.turn.intensity08")
test2 <- transform.fct("Right.turn.intensity09")
test2 <- transform.fct("Right.turn.intensity10")
test2 <- transform.fct("Right.turn.intensity11")
test2 <- transform.fct("Right.turn.intensity12")

# Mean Encoding with White Noise pour les territoires
cardi <- length(unique(train$Territory))

enc.terr <- train2 %>%
  group_by(Territory) %>%
  summarize(freq = sum(NB_Claim)/sum(expo)) %>%
  arrange(freq) %>%
  mutate(terr.code= row_number()/(cardi+1)) %>%
  select(Territory, terr.code)

train2 <- train2 %>%
  group_by(Territory) %>%
  left_join(enc.terr, by='Territory') %>%
  ungroup()

test2 <- test2 %>%
  group_by(Territory) %>%
  left_join(enc.terr, by='Territory') %>%
  ungroup()


```

:::

## Basic GLM Models

::: {.panel-tabset}

### Single intercept

A baseline model corresponding to a Generalized Linear Model (GLM) with intercept and predicting for each contract only the observed mean multiplied by the exposure is used as a point of comparison.

```{r}
#| echo: true
#| message: FALSE
#| warning: FALSE
#| code-fold: true
#| label: tbl-Pscore_base
#| tbl-cap: Prediction scores for the base model

## Model on each fold
Result_  <- data.frame()
Result2_  <- data.frame()
for(i in 1:nb.fold) {
    learn <- train2[train2$fold != i,]
    valid <- train2[train2$fold == i,]

    mean <- sum(learn$NB_Claim)/sum(learn$expo) 
    learn$pred.base <- mean*learn$expo
    valid$pred.base <- mean*valid$expo

    Result_ <- rbind(Result_, c(i, Score.pred(valid$pred.base, valid$NB_Claim)/nrow(valid)))
    Result2_ <- rbind(Result2_, c(i, Score.pred(valid$pred.base, valid$NB_Claim)))
}

## Show results
colnames(Result_) <- c('Fold', "Sc.log", "Sc.MSE", "Sc.quad", "Sc.sph", "Sc.DSS", "Sc.CRPS")
colnames(Result2_) <- c('Fold', "Sc.log", "Sc.MSE", "Sc.quad", "Sc.sph", "Sc.DSS", "Sc.CRPS")
tot <- colSums(Result2_)/nrow(train2)
tot$Fold <- 'Total'
Result_ <- rbind(Result_ , tot)

Result.base <- Result_  
Base <- Result.base[nb.fold+1,]

knitr::kable(Result_, align = "ccccccc", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = ","))%>%   
  kable_styling(bootstrap_options = "striped", full_width = T)  

```

The model is trained on the entire training dataset and subsequently tested on the untouched test dataset, ensuring that the parameter calibration process remains independent from the test data.

```{r}
#| echo: true
#| message: FALSE
#| warning: FALSE
#| code-fold: true
#| label: tbl-Pscore_basetest
#| tbl-cap: Prediction scores for the base model (testing set) 

mean <- sum(train2$NB_Claim)/sum(train2$expo) 
test2$pred.base <- mean*test2$expo

Result_ <- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))
Result_ <- cbind('Base', Result_)
colnames(Result_) <- c("Model", "Sc.log", "Sc.MSE", "Sc.quad", "Sc.sph", "Sc.DSS", "Sc.CRPS")

Result_all <- Result_

knitr::kable(Result_all, align = "ccccccc", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = ","))%>%   
  kable_styling(bootstrap_options = "striped", full_width = T)  

```

### Traditional covariates already used (without protected variables)

For comparison, we start with a simple GLM model considering only the traditional covariates (excluding the protected variables). Therefore, we include:  

  - Car.use,  
  - Region,  
  - Car.age, 
  - Years.noclaims.  

The model's prediction scores are displayed below. As expected, adding segmentation variables improves the prediction scores compared to the simple baseline model with only an intercept.

```{r}
#| echo: true
#| message: FALSE
#| warning: FALSE
#| code-fold: true
#| label: tbl-TeleGLM2
#| tbl-cap: Prediction scores for the GLM model with traditional covariates (without protected)

## Model 
score.base <- as.formula(NB_Claim ~ 1 + offset(log(expo)))
score.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) 
                        + offset(log(expo)))
## Model on each fold
Result_  <- data.frame()
Result2_  <- data.frame()
for(i in 1:nb.fold) {
    learn <- train2[train2$fold != i,]
    valid <- train2[train2$fold == i,]
    glm.fit <- glm(score.glm, family = poisson(), data = learn)

    learn$pred.base <- predict(glm.fit, newdata=learn, type='response')
    valid$pred.base <- predict(glm.fit, newdata=valid, type='response')

    Result_ <- rbind(Result_, c(i, Score.pred(valid$pred.base, valid$NB_Claim)/nrow(valid)))
    Result2_ <- rbind(Result2_, c(i, Score.pred(valid$pred.base, valid$NB_Claim)))
}

## Model on all data from train
glm.base <- glm(score.base, family = poisson(), data = train2)
glm.fit <- glm(score.glm, family = poisson(), data = train2)
train2$pred.glm1 <- predict(glm.fit, newdata=train2, type='response')
Result.glm1 <- Result_  

## Show results
colnames(Result_) <- c('Fold', "Sc.log", "Sc.MSE", "Sc.quad", "Sc.sph", "Sc.DSS", "Sc.CRPS")
colnames(Result2_) <- c('Fold', "Sc.log", "Sc.MSE", "Sc.quad", "Sc.sph", "Sc.DSS", "Sc.CRPS")
tot <- colSums(Result2_)/nrow(train2)
tot$Fold <- 'Total'
Result_ <- rbind(Result_ , tot)
Result_ <- rbind(Result_, Base)

Result_[nb.fold+2,1] <- 'Improvement'

for(i in 2:7){
  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]
}


rownames(Result_) <- NULL
knitr::kable(Result_, align = "ccccccc", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = ","))%>%   
  kable_styling(bootstrap_options = "striped", full_width = T)  

```

The comparison with the test dataset is also depicted in the table below. 

```{r}
#| echo: true
#| message: FALSE
#| warning: FALSE
#| code-fold: true
#| label: tbl-Pscore_basetest2
#| tbl-cap: Prediction scores for the GLM model with traditional covariates (testing set) 

score.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) 
                        + offset(log(expo)))

glm.fit <- glm(score.glm, family = poisson(), data = train2)
test2$pred.base <- predict(glm.fit, newdata=test2, type='response')

Result_ <- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))
Result_ <- cbind('GLM (trad.)', Result_)
colnames(Result_) <- c("Model", "Sc.log", "Sc.MSE", "Sc.quad", "Sc.sph", "Sc.DSS", "Sc.CRPS")

Result_all <- rbind(Result_all, Result_)

knitr::kable(Result_all, align = "ccccccc", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = ","))%>%   
  kable_styling(bootstrap_options = "striped", full_width = T)  

```

### Estimated Parameters

The table below shows the estimators obtained for the GLM-Poisson approach and compares them with the baseline model, which has only an intercept.

```{r}
#| echo: true
#| message: FALSE
#| warning: FALSE
#| code-fold: true
#| label: tbl-TeleGLM1
#| tbl-cap: Estimated parameters for the GLM model with traditional covariates (without protected)

## Model 
score.base <- as.formula(NB_Claim ~ 1 + offset(log(expo)))

score.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) 
                        + offset(log(expo)))

## Model on all data from train
glm.base <- glm(score.base, family = poisson(), data = train2)
glm.fit <- glm(score.glm, family = poisson(), data = train2)

tab_model(glm.base, glm.fit, transform = NULL)

```



:::

## GLM-Net

### Parametric transformation of continuous covariates

Now, we add all available telematic variables to the model. Similar to the approach taken in the previous chapter, we will introduce a method utilizing the Generalized Additive Models (GAM) theory for all these continuous variables. This will enable us to observe the general form of the covariate to explain the number of claims. Subsequently, a parametric form will be proposed to achieve the best possible correspondence with the spline obtained by the GAM.

::: {.panel-tabset}

### VEHICLE USAGE LEVEL

For the two covariates related to usage level, the proposed parametric forms are as follows:

\begin{align*}
s(Miles.per.day) &\approx Miles.per.day + log(Miles.per.day)\\
s(Avgdays.week) &\approx Avgdays.week + Avgdays.week^2
\end{align*}

The graphs below compare the fit of the parametric approach with that of the GAM model.


```{r}
#| echo: true
#| eval: true
#| message: FALSE
#| warning: FALSE
#| code-fold: true
#| layout-ncol: 2
#| label: fig-MpD_GAM_sev_tel
#| fig-cap: "Smoothing of Usage level covariates"
#| fig-subcap: 
#|   - "Miles.per.day"
#|   - "Avgdays.week"

min_ <- min(train2$Miles.per.day) 
max_ <- max(train2$Miles.per.day) 
by_ <-  (max_ - min_)/(nrow(train2)-1) 
add <- data.frame(seq(min_, max_, by_)) 
colnames(add) <- 'Miles.per.day'

q99 <- quantile(train2$Miles.per.day, 0.99)

db <- train2 %>%
  select(-'Miles.per.day') %>%
  slice(1) 
db <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))
db <- cbind(db, add)

##

score.gam <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))
                        + s(Miles.per.day))

score.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))
                        + Miles.per.day + log(Miles.per.day) )

gam.fit <- gam(score.gam, family = poisson(), data = train2)
glm.fit <- glm(score.glm, family = poisson(), data = train2)

db$pred.gam <- predict(gam.fit, newdata=db, type='response')
db$pred.glm <- predict(glm.fit, newdata=db, type='response')
base <- db %>%
  mutate(diff = abs(Miles.per.day - mean(train2$Miles.per.day))) %>%
  filter(diff == min(diff))
db$pred.gam <- db$pred.gam/base$pred.gam[1]
db$pred.glm <- db$pred.glm/base$pred.glm[1]

ggplot()+
  geom_line(aes(x=Miles.per.day, y=pred.gam, color='GAM'), data=db) + 
  geom_line(aes(x=Miles.per.day, y=pred.glm, color='Parametric GLM'), data=db) +
  guides(color = guide_legend(title = "")) +
  labs(x = 'Miles per day',
       y = 'Relativity') +
  xlim(0, q99) +
  theme_classic()+
   theme(legend.position = 'bottom', legend.direction = "horizontal")


### Avgdays.week

min_ <- min(train2$Avgdays.week) 
max_ <- max(train2$Avgdays.week) 
by_ <-  (max_ - min_)/(nrow(train2)-1) 
add <- data.frame(seq(min_, max_, by_)) 
colnames(add) <- 'Avgdays.week'

q99 <- quantile(train2$Avgdays.week, 0.99)

db <- train2 %>%
  select(-'Avgdays.week') %>%
  slice(1) 
db <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))
db <- cbind(db, add)

##

score.gam <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))
                        + s(Avgdays.week))

score.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))
                        + Avgdays.week + I(Avgdays.week^2) )

gam.fit <- gam(score.gam, family = poisson(), data = train2)
glm.fit <- glm(score.glm, family = poisson(), data = train2)

db$pred.gam <- predict(gam.fit, newdata=db, type='response')
db$pred.glm <- predict(glm.fit, newdata=db, type='response')
base <- db %>%
  mutate(diff = abs(Avgdays.week - mean(train2$Avgdays.week))) %>%
  filter(diff == min(diff))
db$pred.gam <- db$pred.gam/base$pred.gam[1]
db$pred.glm <- db$pred.glm/base$pred.glm[1]

ggplot()+
  geom_line(aes(x=Avgdays.week, y=pred.gam, color='GAM'), data=db) + 
  geom_line(aes(x=Avgdays.week, y=pred.glm, color='Parametric GLM'), data=db) +
  guides(color = guide_legend(title = "")) +
  labs(x = 'Avgdays.week',
       y = 'Relativity') +
  xlim(0, q99) +
  theme_classic()+
   theme(legend.position = 'bottom', legend.direction = "horizontal")

```

### Type of Vehicle Usage


Several covariates are available in the category *Type of vehicle usage*:  

- We propose the same parametric form for all variants of the variable *Pct.drive.day* (Monday to Sunday);  
- The same parametric form will also be proposed for *Pct.drive.rush.am*, *Pct.drive.rush.pm*, *Pct.drive.2hrs*, *Pct.drive.3hrs*, and *Pct.drive.4hrs*;  
- The other three covariates have their own parametric form.

We then have:

\begin{align*}
s(Pct.drive.day) &\approx Pct.drive.day + Pct.drive.day^2 \\
s(Pct.drive) &\approx Pct.drive + \sqrt{Pct.drive} \\
s(max.day) &\approx max.day + \log(max.day) \\
s(min.day) &\approx min.day + min.day^2 \\
s(max.min) &\approx max.min + max.min^2 
\end{align*}


```{r}
#| echo: true
#| eval: true
#| message: FALSE
#| warning: FALSE
#| code-fold: true
#| layout-ncol: 2
#| layout-nrow: 3
#| label: fig-MpD_GAM_sev_tel3
#| fig-cap: "Smoothing of Type of vehicle usage covariates (severity)"
#| fig-subcap: 
#|   - "Pct.drive.mon"
#|   - "Pct.drive.rush.am"
#|   - "max.day"
#|   - "min.day"
#|   - "max.min"

train2$Pct.drive <- train2$Pct.drive.sun

min_ <- min(train2$Pct.drive) 
max_ <- max(train2$Pct.drive) 
by_ <-  (max_ - min_)/(nrow(train2)-1) 
add <- data.frame(seq(min_, max_, by_)) 
colnames(add) <- 'Pct.drive'

q99 <- quantile(train2$Pct.drive, 0.99)

db <- train2 %>%
  select(-'Pct.drive') %>%
  slice(1) 
db <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))
db <- cbind(db, add)

##

score.gam <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))
                        + s(Pct.drive))

score.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))
                        + Pct.drive + I(Pct.drive^2) )

gam.fit <- gam(score.gam, family = poisson(), data = train2)
glm.fit <- glm(score.glm, family = poisson(), data = train2)

db$pred.gam <- predict(gam.fit, newdata=db, type='response')
db$pred.glm <- predict(glm.fit, newdata=db, type='response')
base <- db %>%
  mutate(diff = abs(Pct.drive - mean(train2$Pct.drive))) %>%
  filter(diff == min(diff))
db$pred.gam <- db$pred.gam/base$pred.gam[1]
db$pred.glm <- db$pred.glm/base$pred.glm[1]

ggplot()+
  geom_line(aes(x=Pct.drive, y=pred.gam, color='GAM'), data=db) + 
  geom_line(aes(x=Pct.drive, y=pred.glm, color='Parametric GLM'), data=db) +
  guides(color = guide_legend(title = "")) +
  labs(x = 'Pct.drive',
       y = 'Relativity') +
  xlim(0, q99) +
  theme_classic()+
   theme(legend.position = 'bottom', legend.direction = "horizontal")


### Pct.drive 


train2$use.day <- train2$Pct.drive.rush.am

min_ <- min(train2$use.day) 
max_ <- max(train2$use.day) 
by_ <-  (max_ - min_)/(nrow(train2)-1) 
add <- data.frame(seq(min_, max_, by_)) 
colnames(add) <- 'use.day'

q99 <- quantile(train2$use.day, 0.99)

db <- train2 %>%
  select(-'use.day') %>%
  slice(1) 
db <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))
db <- cbind(db, add)

##

score.gam <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))
                        + s(use.day))

score.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))
                        + use.day + I(use.day^0.5))

gam.fit <- gam(score.gam, family = poisson(), data = train2)
glm.fit <- glm(score.glm, family = poisson(), data = train2)

db$pred.gam <- predict(gam.fit, newdata=db, type='response')
db$pred.glm <- predict(glm.fit, newdata=db, type='response')
base <- db %>%
  mutate(diff = abs(use.day - mean(train2$use.day))) %>%
  filter(diff == min(diff))
db$pred.gam <- db$pred.gam/base$pred.gam[1]
db$pred.glm <- db$pred.glm/base$pred.glm[1]

ggplot()+
  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + 
  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +
  guides(color = guide_legend(title = "")) +
  labs(x = 'Use per day',
       y = 'Relativity') +
  xlim(0, q99) +
  theme_classic()+
   theme(legend.position = 'bottom', legend.direction = "horizontal")



### Max day

train2$use.day <- train2$max.day

min_ <- min(train2$use.day) 
max_ <- max(train2$use.day) 
by_ <-  (max_ - min_)/(nrow(train2)-1) 
add <- data.frame(seq(min_, max_, by_)) 
colnames(add) <- 'use.day'

q99 <- quantile(train2$use.day, 0.99)

db <- train2 %>%
  select(-'use.day') %>%
  slice(1) 
db <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))
db <- cbind(db, add)

##

score.gam <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))
                        + s(use.day))

score.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))
                        + use.day + log(use.day) )

gam.fit <- gam(score.gam, family = poisson(), data = train2)
glm.fit <- glm(score.glm, family = poisson(), data = train2)

db$pred.gam <- predict(gam.fit, newdata=db, type='response')
db$pred.glm <- predict(glm.fit, newdata=db, type='response')
base <- db %>%
  mutate(diff = abs(use.day - mean(train2$use.day))) %>%
  filter(diff == min(diff))
db$pred.gam <- db$pred.gam/base$pred.gam[1]
db$pred.glm <- db$pred.glm/base$pred.glm[1]

ggplot()+
  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + 
  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +
  guides(color = guide_legend(title = "")) +
  labs(x = 'Use per day',
       y = 'Relativity') +
  xlim(0, q99) +
  theme_classic()+
   theme(legend.position = 'bottom', legend.direction = "horizontal")

### Min day


train2$use.day <- train2$min.day

min_ <- min(train2$use.day) 
max_ <- max(train2$use.day) 
by_ <-  (max_ - min_)/(nrow(train2)-1) 
add <- data.frame(seq(min_, max_, by_)) 
colnames(add) <- 'use.day'

q99 <- quantile(train2$use.day, 0.99)

db <- train2 %>%
  select(-'use.day') %>%
  slice(1) 
db <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))
db <- cbind(db, add)

##

score.gam <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))
                        + s(use.day))

score.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))
                        + use.day + I(use.day^2) )

gam.fit <- gam(score.gam, family = poisson(), data = train2)
glm.fit <- glm(score.glm, family = poisson(), data = train2)

db$pred.gam <- predict(gam.fit, newdata=db, type='response')
db$pred.glm <- predict(glm.fit, newdata=db, type='response')
base <- db %>%
  mutate(diff = abs(use.day - mean(train2$use.day))) %>%
  filter(diff == min(diff))
db$pred.gam <- db$pred.gam/base$pred.gam[1]
db$pred.glm <- db$pred.glm/base$pred.glm[1]

ggplot()+
  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + 
  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +
  guides(color = guide_legend(title = "")) +
  labs(x = 'Use per day',
       y = 'Relativity') +
  xlim(0, q99) +
  theme_classic()+
   theme(legend.position = 'bottom', legend.direction = "horizontal")

### Max min

train2$use.day <- train2$max.min

min_ <- min(train2$use.day) 
max_ <- max(train2$use.day) 
by_ <-  (max_ - min_)/(nrow(train2)-1) 
add <- data.frame(seq(min_, max_, by_)) 
colnames(add) <- 'use.day'

q99 <- quantile(train2$use.day, 0.99)

db <- train2 %>%
  select(-'use.day') %>%
  slice(1) 
db <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))
db <- cbind(db, add)

##

score.gam <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))
                        + s(use.day))

score.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))
                        + use.day + I(use.day^2) )

gam.fit <- gam(score.gam, family = poisson(), data = train2)
glm.fit <- glm(score.glm, family = poisson(), data = train2)

db$pred.gam <- predict(gam.fit, newdata=db, type='response')
db$pred.glm <- predict(glm.fit, newdata=db, type='response')
base <- db %>%
  mutate(diff = abs(use.day - mean(train2$use.day))) %>%
  filter(diff == min(diff))
db$pred.gam <- db$pred.gam/base$pred.gam[1]
db$pred.glm <- db$pred.glm/base$pred.glm[1]

ggplot()+
  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + 
  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +
  guides(color = guide_legend(title = "")) +
  labs(x = 'Use per day',
       y = 'Relativity') +
  xlim(0, q99) +
  theme_classic()+
   theme(legend.position = 'bottom', legend.direction = "horizontal")




```


### Driving Behavior

The same parametric form is proposed for the different variants of the *Accel* and *Brake* variables, i.e., *Accel.06miles* to *Accel.14miles*, and *Brake.06miles* to *Brake.14miles*.  For the different variants of the *turn* variable, a single parametric form is also used: 

\begin{align*}
s(Brake.Accel) &\approx Brake.Accel + Brake.Accel^2 + Brake.Accel^3\\
s(Turn) &\approx Turn + log(Turn)
\end{align*}

The graphs below compare the fit of the parametric approach for *Accel.06miles* and *Right.turn.intensity08* with that of the GAM model. 

```{r}
#| echo: true
#| eval: true
#| message: FALSE
#| warning: FALSE
#| code-fold: true
#| layout-ncol: 2
#| label: fig-MpD_GAM_sev_tel2
#| fig-cap: "Smoothing of Driving behavior covariates (severity)"
#| fig-subcap: 
#|   - "Accel.06miles"
#|   - "Right.turn.intensity08"

train2$use.day <- train2$Accel.06miles

min_ <- min(train2$use.day) 
max_ <- max(train2$use.day) 
by_ <-  (max_ - min_)/(nrow(train2)-1) 
add <- data.frame(seq(min_, max_, by_)) 
colnames(add) <- 'use.day'

q99 <- quantile(train2$use.day, 0.99)

db <- train2 %>%
  select(-'use.day') %>%
  slice(1) 
db <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))
db <- cbind(db, add)

##

score.gam <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))
                        + s(use.day))

score.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))
                        + use.day + I(use.day^2) + I(use.day^3) )

gam.fit <- gam(score.gam, family = poisson(), data = train2)
glm.fit <- glm(score.glm, family = poisson(), data = train2)

db$pred.gam <- predict(gam.fit, newdata=db, type='response')
db$pred.glm <- predict(glm.fit, newdata=db, type='response')
base <- db %>%
  mutate(diff = abs(use.day - mean(train2$use.day))) %>%
  filter(diff == min(diff))
db$pred.gam <- db$pred.gam/base$pred.gam[1]
db$pred.glm <- db$pred.glm/base$pred.glm[1]

ggplot()+
  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + 
  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +
  guides(color = guide_legend(title = "")) +
  labs(x = 'Use per day',
       y = 'Relativity') +
  xlim(0, q99) +
  theme_classic()+
   theme(legend.position = 'bottom', legend.direction = "horizontal")



### Right and left turns


train2$use.day <- train2$Right.turn.intensity08

q99 <- quantile(train2$use.day, 0.99)

min_ <- min(train2$use.day) 
max_ <- q99
by_ <-  (max_ - min_)/(nrow(train2)-1) 
add <- data.frame(seq(min_, max_, by_)) 
colnames(add) <- 'use.day'

db <- train2 %>%
  select(-'use.day') %>%
  slice(1) 
db <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))
db <- cbind(db, add)

##

temp <- train2 %>%
  mutate(use.day = pmin(q99, use.day))

score.gam <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))
                        + s(use.day))

score.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))
                         + use.day + log1p(use.day))

gam.fit <- gam(score.gam, family = poisson(), data = temp)
glm.fit <- glm(score.glm, family = poisson(), data = temp)

db$pred.gam <- predict(gam.fit, newdata=db, type='response')
db$pred.glm <- predict(glm.fit, newdata=db, type='response')
base <- db %>%
  mutate(diff = abs(use.day - mean(temp$use.day))) %>%
  filter(diff == min(diff))
db$pred.gam <- db$pred.gam/base$pred.gam[1]
db$pred.glm <- db$pred.glm/base$pred.glm[1]
  
ggplot()+
  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + 
  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +
  guides(color = guide_legend(title = "")) +
  labs(x = 'Use per day',
       y = 'Relativity') +
 # xlim(0, q99) +
  theme_classic()+
   theme(legend.position = 'bottom', legend.direction = "horizontal")




```

:::

### Fitting the GLM-Net model

```{r}
#| echo: false
#| eval: false
#| message: FALSE
#| warning: FALSE
#| code-fold: true

glm.score <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3)
                        + Dayformax + Dayformin +
                        + Miles.per.day + log(Miles.per.day)
                        + Avgdays.week + I(Avgdays.week^2)
                        + Pct.drive.mon + I(Pct.drive.mon^2)
                        + Pct.drive.tue + I(Pct.drive.tue^2)
                        + Pct.drive.wed + I(Pct.drive.wed^2)
                        + Pct.drive.thr + I(Pct.drive.thr^2)
                        + Pct.drive.fri + I(Pct.drive.fri^2)
                        + Pct.drive.sat + I(Pct.drive.sat^2)
                        + Pct.drive.sun + I(Pct.drive.sun^2)
                        + Pct.drive.wkend + I(Pct.drive.wkend^2)
                        + max.day + log(max.day) 
                        + min.day + I(min.day^2)
                        + max.min + I(max.min^2)
                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) 
                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   
                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) 
                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) 
                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) 
                        + Accel.06miles + I(Accel.06miles^2) + I(Accel.06miles^3)
                        + Accel.08miles + I(Accel.08miles^2) + I(Accel.08miles^3)
                        + Accel.09miles + I(Accel.09miles^2) + I(Accel.09miles^3)
                        + Accel.11miles + I(Accel.11miles^2) + I(Accel.11miles^3)
                        + Accel.12miles + I(Accel.12miles^2) + I(Accel.12miles^3)
                        + Accel.14miles + I(Accel.14miles^2) + I(Accel.14miles^3)
                        + Brake.06miles + I(Brake.06miles^2) + I(Brake.06miles^3)
                        + Brake.08miles + I(Brake.08miles^2) + I(Brake.08miles^3)
                        + Brake.09miles + I(Brake.09miles^2) + I(Brake.09miles^3)
                        + Brake.11miles + I(Brake.11miles^2) + I(Brake.11miles^3)
                        + Brake.12miles + I(Brake.12miles^2) + I(Brake.12miles^3)
                        + Brake.14miles + I(Brake.14miles^2) + I(Brake.14miles^3)
                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)
                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)
                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)
                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)
                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)
                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)
                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)
                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)
                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)
                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))
                        

matrix.x <- model.matrix(glm.score, data=train2)[,-1]
y <- train2$NB_Claim
offset <- log(train2$expo)
fold.id <- train2$fold

lambda_seq <- c(10^seq(0, -8, by = -.1), 0)
cvfit0  <- cv.glmnet(matrix.x, y, relax=FALSE, family = "poisson", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0)
cvfit.2 <- cv.glmnet(matrix.x, y, relax=FALSE, family = "poisson", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.2)
cvfit.4 <- cv.glmnet(matrix.x, y, relax=FALSE, family = "poisson", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.4)
cvfit.6 <- cv.glmnet(matrix.x, y, relax=FALSE, family = "poisson", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.6)
cvfit.8 <- cv.glmnet(matrix.x, y, relax=FALSE, family = "poisson", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.8)
cvfit1  <- cv.glmnet(matrix.x, y, relax=FALSE, family = "poisson", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 1)

c(cvfit0$lambda.min, cvfit.2$lambda.min, cvfit.4$lambda.min, cvfit.6$lambda.min, cvfit.8$lambda.min, cvfit1$lambda.min)

all.min <- data.frame(c(min(cvfit0$cvm), min(cvfit.2$cvm), min(cvfit.4$cvm), min(cvfit.6$cvm), min(cvfit.8$cvm), min(cvfit1$cvm))) %>%
  mutate(alpha = 2*(row_number()-1)/10)
colnames(all.min)[1] <- 'min' 
all.min %>% filter(min == min(min))

cvfit1$lambda.min
cvfit1$lambda.1se

```

::: {.panel-tabset}

### Optimal value

The parameters of the GLM-net were calibrated using cross-validation to obtain the model's hyperparameters. Using these values, we can calculate the prediction scores of the model based on all covariates.

```{r}
#| echo: true
#| cache: false
#| message: FALSE
#| warning: FALSE
#| code-fold: true
#| label: tbl-Pscore_teleGLMnet1
#| tbl-cap: Prediction scores for the GLM-net model (alpha=1)

glm.score <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3)
                        + Dayformax + Dayformin +
                        + Miles.per.day + log(Miles.per.day)
                        + Avgdays.week + I(Avgdays.week^2)
                        + Pct.drive.mon + I(Pct.drive.mon^2)
                        + Pct.drive.tue + I(Pct.drive.tue^2)
                        + Pct.drive.wed + I(Pct.drive.wed^2)
                        + Pct.drive.thr + I(Pct.drive.thr^2)
                        + Pct.drive.fri + I(Pct.drive.fri^2)
                        + Pct.drive.sat + I(Pct.drive.sat^2)
                        + Pct.drive.sun + I(Pct.drive.sun^2)
                        + Pct.drive.wkend + I(Pct.drive.wkend^2)
                        + max.day + log(max.day) 
                        + min.day + I(min.day^2)
                        + max.min + I(max.min^2)
                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) 
                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   
                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) 
                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) 
                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) 
                        + Accel.06miles + I(Accel.06miles^2) + I(Accel.06miles^3)
                        + Accel.08miles + I(Accel.08miles^2) + I(Accel.08miles^3)
                        + Accel.09miles + I(Accel.09miles^2) + I(Accel.09miles^3)
                        + Accel.11miles + I(Accel.11miles^2) + I(Accel.11miles^3)
                        + Accel.12miles + I(Accel.12miles^2) + I(Accel.12miles^3)
                        + Accel.14miles + I(Accel.14miles^2) + I(Accel.14miles^3)
                        + Brake.06miles + I(Brake.06miles^2) + I(Brake.06miles^3)
                        + Brake.08miles + I(Brake.08miles^2) + I(Brake.08miles^3)
                        + Brake.09miles + I(Brake.09miles^2) + I(Brake.09miles^3)
                        + Brake.11miles + I(Brake.11miles^2) + I(Brake.11miles^3)
                        + Brake.12miles + I(Brake.12miles^2) + I(Brake.12miles^3)
                        + Brake.14miles + I(Brake.14miles^2) + I(Brake.14miles^3)
                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)
                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)
                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)
                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)
                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)
                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)
                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)
                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)
                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)
                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))

Result_  <- data.frame()
Result2_  <- data.frame()
for(i in 1:nb.fold) {
    learn <- train2[train2$fold != i,]
    valid <- train2[train2$fold == i,]
    
    matrix.x <- model.matrix(glm.score, data=learn)[,-1]
    y <- learn$NB_Claim
    offset <- log(learn$expo)

    lambda.min <- 3.981072e-05
    lambda.1se <- 0.001258925
    
    lambda.select <- lambda.min
    fit <- glmnet(matrix.x, y, family = "poisson", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)
    #fit <- glmnet(matrix.x, y, family = "poisson", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)
  
    matrix.x <- model.matrix(glm.score, data=valid)[,-1]
    y <- valid$NB_Claim
    offset <- log(valid$expo)

    valid$pred <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)
    
    Result_ <- rbind(Result_, c(i, Score.pred(valid$pred, valid$NB_Claim)/nrow(valid)))
    Result2_ <- rbind(Result2_, c(i, Score.pred(valid$pred, valid$NB_Claim)))
}


## Show results
colnames(Result_) <- c('Fold', "Sc.log", "Sc.MSE", "Sc.quad", "Sc.sph", "Sc.DSS", "Sc.CRPS")
colnames(Result2_) <- c('Fold', "Sc.log", "Sc.MSE", "Sc.quad", "Sc.sph", "Sc.DSS", "Sc.CRPS")
tot <- colSums(Result2_)/nrow(train2)
tot$Fold <- 'Total'
Result_ <- rbind(Result_ , tot)
Result_ <- rbind(Result_, Base)

Result_[nb.fold+2,1] <- 'Improvement'

for(i in 2:7){
  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]
}


rownames(Result_) <- NULL
knitr::kable(Result_, align = "ccccccc", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = ","))%>%   
  kable_styling(bootstrap_options = "striped", full_width = T)  

```

On the *test* set, we obtain:

```{r}
#| echo: true
#| cache: false
#| message: FALSE
#| warning: FALSE
#| code-fold: true
#| label: tbl-Pscore_basetest3
#| tbl-cap: Prediction scores for the GLM-net model  (testing set) 

glm.score <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3)
                        + Dayformax + Dayformin +
                        + Miles.per.day + log(Miles.per.day)
                        + Avgdays.week + I(Avgdays.week^2)
                        + Pct.drive.mon + I(Pct.drive.mon^2)
                        + Pct.drive.tue + I(Pct.drive.tue^2)
                        + Pct.drive.wed + I(Pct.drive.wed^2)
                        + Pct.drive.thr + I(Pct.drive.thr^2)
                        + Pct.drive.fri + I(Pct.drive.fri^2)
                        + Pct.drive.sat + I(Pct.drive.sat^2)
                        + Pct.drive.sun + I(Pct.drive.sun^2)
                        + Pct.drive.wkend + I(Pct.drive.wkend^2)
                        + max.day + log(max.day) 
                        + min.day + I(min.day^2)
                        + max.min + I(max.min^2)
                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) 
                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   
                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) 
                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) 
                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) 
                        + Accel.06miles + I(Accel.06miles^2) + I(Accel.06miles^3)
                        + Accel.08miles + I(Accel.08miles^2) + I(Accel.08miles^3)
                        + Accel.09miles + I(Accel.09miles^2) + I(Accel.09miles^3)
                        + Accel.11miles + I(Accel.11miles^2) + I(Accel.11miles^3)
                        + Accel.12miles + I(Accel.12miles^2) + I(Accel.12miles^3)
                        + Accel.14miles + I(Accel.14miles^2) + I(Accel.14miles^3)
                        + Brake.06miles + I(Brake.06miles^2) + I(Brake.06miles^3)
                        + Brake.08miles + I(Brake.08miles^2) + I(Brake.08miles^3)
                        + Brake.09miles + I(Brake.09miles^2) + I(Brake.09miles^3)
                        + Brake.11miles + I(Brake.11miles^2) + I(Brake.11miles^3)
                        + Brake.12miles + I(Brake.12miles^2) + I(Brake.12miles^3)
                        + Brake.14miles + I(Brake.14miles^2) + I(Brake.14miles^3)
                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)
                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)
                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)
                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)
                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)
                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)
                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)
                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)
                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)
                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))


matrix.x <- model.matrix(glm.score, data=train2)[,-1]
y <- train2$NB_Claim
offset <- log(train2$expo)

lambda.min <- 3.981072e-05
lambda.1se <- 0.001258925
    
lambda.select <- lambda.min
fit <- glmnet(matrix.x, y, family = "poisson", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)
#fit <- glmnet(matrix.x, y, family = "poisson", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)

train2$pred.tele <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)

matrix.x <- model.matrix(glm.score, data=test2)[,-1]
y <- test2$NB_Claim
offset <- log(test2$expo)

test2$pred.base <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)

Result_ <- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))
Result_ <- cbind('LASSO (optimal)', Result_)
colnames(Result_) <- c("Model", "Sc.log", "Sc.MSE", "Sc.quad", "Sc.sph", "Sc.DSS", "Sc.CRPS")

Result_all <- rbind(Result_all, Result_)

knitr::kable(Result_all, align = "ccccccc", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = ","))%>%   
  kable_styling(bootstrap_options = "striped", full_width = T)  



```


### Parsimonious model

Instead of using the optimal value of the penalty $\lambda$  in the elastic-net approach, it is often advised to use a penalty value located at one standard error ($\lambda_{1se}$). This helps to obtain a more parsimonious model. The prediction scores of such a model are displayed below.

```{r}
#| echo: true
#| cache: false
#| message: FALSE
#| warning: FALSE
#| code-fold: true
#| label: tbl-Pscore_GLMnet2
#| tbl-cap: Prediction scores for the GLM-net model (alpha=1)

Result_  <- data.frame()
Result2_  <- data.frame()
for(i in 1:nb.fold) {
    learn <- train2[train2$fold != i,]
    valid <- train2[train2$fold == i,]
    
    matrix.x <- model.matrix(glm.score, data=learn)[,-1]
    y <- learn$NB_Claim
    offset <- log(learn$expo)

    lambda.min <- 3.981072e-05
    lambda.1se <- 0.001258925
    
    lambda.select <- lambda.1se
    #fit <- glmnet(matrix.x, y, family = "poisson", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)
    fit <- glmnet(matrix.x, y, family = "poisson", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)
  
    matrix.x <- model.matrix(glm.score, data=valid)[,-1]
    y <- valid$NB_Claim
    offset <- log(valid$expo)

    valid$pred <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)
    
    Result_ <- rbind(Result_, c(i, Score.pred(valid$pred, valid$NB_Claim)/nrow(valid)))
    Result2_ <- rbind(Result2_, c(i, Score.pred(valid$pred, valid$NB_Claim)))
}


## Show results
colnames(Result_) <- c('Fold', "Sc.log", "Sc.MSE", "Sc.quad", "Sc.sph", "Sc.DSS", "Sc.CRPS")
colnames(Result2_) <- c('Fold', "Sc.log", "Sc.MSE", "Sc.quad", "Sc.sph", "Sc.DSS", "Sc.CRPS")
tot <- colSums(Result2_)/nrow(train2)
tot$Fold <- 'Total'
Result_ <- rbind(Result_ , tot)
Result_ <- rbind(Result_, Base)

Result_[nb.fold+2,1] <- 'Improvement'

for(i in 2:7){
  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]
}


rownames(Result_) <- NULL
knitr::kable(Result_, align = "ccccccc", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = ","))%>%   
  kable_styling(bootstrap_options = "striped", full_width = T)  


```

On the *test* set, we obtain:

```{r}
#| echo: true
#| cache: false
#| message: FALSE
#| warning: FALSE
#| code-fold: true
#| label: tbl-Pscore_basetest4
#| tbl-cap: Prediction scores for the GLM-net model  (testing set) 

glm.score <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3)
                        + Dayformax + Dayformin +
                        + Miles.per.day + log(Miles.per.day)
                        + Avgdays.week + I(Avgdays.week^2)
                        + Pct.drive.mon + I(Pct.drive.mon^2)
                        + Pct.drive.tue + I(Pct.drive.tue^2)
                        + Pct.drive.wed + I(Pct.drive.wed^2)
                        + Pct.drive.thr + I(Pct.drive.thr^2)
                        + Pct.drive.fri + I(Pct.drive.fri^2)
                        + Pct.drive.sat + I(Pct.drive.sat^2)
                        + Pct.drive.sun + I(Pct.drive.sun^2)
                        + Pct.drive.wkend + I(Pct.drive.wkend^2)
                        + max.day + log(max.day) 
                        + min.day + I(min.day^2)
                        + max.min + I(max.min^2)
                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) 
                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   
                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) 
                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) 
                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) 
                        + Accel.06miles + I(Accel.06miles^2) + I(Accel.06miles^3)
                        + Accel.08miles + I(Accel.08miles^2) + I(Accel.08miles^3)
                        + Accel.09miles + I(Accel.09miles^2) + I(Accel.09miles^3)
                        + Accel.11miles + I(Accel.11miles^2) + I(Accel.11miles^3)
                        + Accel.12miles + I(Accel.12miles^2) + I(Accel.12miles^3)
                        + Accel.14miles + I(Accel.14miles^2) + I(Accel.14miles^3)
                        + Brake.06miles + I(Brake.06miles^2) + I(Brake.06miles^3)
                        + Brake.08miles + I(Brake.08miles^2) + I(Brake.08miles^3)
                        + Brake.09miles + I(Brake.09miles^2) + I(Brake.09miles^3)
                        + Brake.11miles + I(Brake.11miles^2) + I(Brake.11miles^3)
                        + Brake.12miles + I(Brake.12miles^2) + I(Brake.12miles^3)
                        + Brake.14miles + I(Brake.14miles^2) + I(Brake.14miles^3)
                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)
                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)
                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)
                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)
                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)
                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)
                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)
                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)
                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)
                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))


matrix.x <- model.matrix(glm.score, data=train2)[,-1]
y <- train2$NB_Claim
offset <- log(train2$expo)

lambda.min <- 3.981072e-05
lambda.1se <- 0.001258925
    
lambda.select <- lambda.1se
#fit <- glmnet(matrix.x, y, family = "poisson", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)
fit <- glmnet(matrix.x, y, family = "poisson", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)
  
matrix.x <- model.matrix(glm.score, data=test2)[,-1]
y <- test2$NB_Claim
offset <- log(test2$expo)

test2$pred.base <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)

Result_ <- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))
Result_ <- cbind('LASSO (parsimonious)', Result_)
colnames(Result_) <- c("Model", "Sc.log", "Sc.MSE", "Sc.quad", "Sc.sph", "Sc.DSS", "Sc.CRPS")

Result_all <- rbind(Result_all, Result_)

knitr::kable(Result_all, align = "ccccccc", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = ","))%>%   
  kable_styling(bootstrap_options = "striped", full_width = T)  

```

### RESIDUALS AND PROTECTED VARIABLES

Comparing the results obtained with the pricing model based solely on traditional variables, we observe that the addition of telematic variables improves prediction quality. However, we can verify whether the protected variables we excluded from the analysis still retain predictive capability. Thus, we fit a GLM-net model on telematic data and predict the expected frequency of the model on the training dataset.  

Using the prediction as an offset variable, we can effectively assess whether the protected variables still capture a trend. The reliability of this approach is further bolstered by the comparison of the results obtained with the graphs we had in Chapter 2. If a curve is horizontal and close to the value of 1 for all possible values of a covariate, it indicates that telematic variables have indeed captured the predictive capability of the variable to predict claim frequency, as demonstrated in the graphs below.

In the graphs below, we observe that the addition of telematic variables:  

- diminishes the effect of credit score, but it still appears to be predictive;  
- almost eliminates the effect of driver age;  
- the addition of telematic variables has led to a significant shift in the effect of driver gender. Originally, a premium reduction for males seemed necessary, but adding telematic variables suggest that males should now have a surcharge;
- significantly reduces the effect of marital status;  
- slightly diminishes the effect of territory without eliminating it.

```{r}
#| echo: true
#| message: FALSE
#| warning: FALSE
#| code-fold: true
#| layout-ncol: 2
#| label: fig-CreditScore_sev
#| fig-cap: "Observed Relativity vs. Residuals Relativity"
#| fig-subcap: 
#|   - "Credit Score"
#|   - "Age of the Insured"
#|   - "Sex of the Insured"
#|   - "Marital Status of the Insured"
#|   - "Territory"
#| fig-width: 9
#| fig-height: 4


meaninv  <- sum(train2$expo)/sum(train2$NB_Claim)

temp2 <- train2 %>%
  dplyr::mutate(Duration.y = Duration/365.25, 
                Insured.age = pmin(Insured.age, 80),
                Group = ceiling(Credit.score/25) * 25) %>%
  group_by(Group) %>% 
  summarize(NB_Claim=sum(NB_Claim),
            expo=sum(expo),
            expo2=sum(pred.tele)) %>% 
  mutate(freq = meaninv*NB_Claim/expo, 
         freq2 =  NB_Claim/expo2)

ggplot() + 
  geom_smooth(aes(x=Group, y=freq, weight = expo, color='Observed'),se=F, size=1, data=temp2) + 
  geom_smooth(aes(x=Group, y=freq2, weight = expo, color='Residuals'),se=F, size=1, data=temp2) + 
  labs(x = 'Credit Score',
       y = 'Relativity') +
  geom_hline(yintercept = 1, linetype='dashed')+
  guides(color = guide_legend(title = "")) +
      theme_classic()+    theme(legend.position = 'bottom', legend.direction = "horizontal")


### Age of the insured

temp2 <- train2 %>%
  dplyr::mutate(Duration.y = Duration/365.25, 
                Insured.age = pmin(Insured.age, 80),
                Group = ceiling(Insured.age/5) * 5) %>%
  group_by(Group) %>% 
  summarize(NB_Claim=sum(NB_Claim),
            expo=sum(expo),
            expo2=sum(pred.tele)) %>% 
  mutate(freq = meaninv*NB_Claim/expo, 
         freq2 =  NB_Claim/expo2)

ggplot() + 
  geom_smooth(aes(x=Group, y=freq, weight = expo, color='Observed'),se=F, size=1, data=temp2) + 
  geom_smooth(aes(x=Group, y=freq2, weight = expo, color='Residuals'),se=F, size=1, data=temp2) + 
  labs(x = 'Age of the insured',
       y = 'Relativity') +
  geom_hline(yintercept = 1, linetype='dashed')+
  guides(color = guide_legend(title = "")) +
        theme_classic()+    theme(legend.position = 'bottom', legend.direction = "horizontal")


### Sex of the insured

temp <- train2 %>%
  mutate(Var_ = Insured.sex) %>%
  group_by(Var_) %>%
  summarize(nbclaim = sum(NB_Claim),
            expo = sum(expo),
            expo2 = sum(pred.tele)) %>%
  mutate(freq = nbclaim/expo,
         freq2 = nbclaim/expo2)

temp$freq <- temp$freq/temp$freq[1]
temp$freq2 <- temp$freq2/temp$freq2[1]

ggplot() + #start plot by by plotting bars
  geom_point(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=3) +
  geom_line(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=0.7) +
  geom_point(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=3) +
  geom_line(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=0.7) +
  labs(x = 'Sex of the insured', y = 'Relativity') +
  geom_hline(yintercept = 1, linetype='dashed')+
  ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+
  guides(color=guide_legend(title="")) +
      theme_classic()+    theme(legend.position = 'bottom', legend.direction = "horizontal")

### Marital Status

temp <- train2 %>%
  mutate(Var_ = Marital) %>%
  group_by(Var_) %>%
  summarize(nbclaim = sum(NB_Claim),
            expo = sum(expo),
            expo2 = sum(pred.tele)) %>%
  mutate(freq = nbclaim/expo, 
         freq2 = nbclaim/expo2)

temp$freq <- temp$freq/temp$freq[1]
temp$freq2 <- temp$freq2/temp$freq2[1]

ggplot() + #start plot by by plotting bars
  geom_point(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=3) +
  geom_line(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=0.7) +
  geom_point(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=3) +
  geom_line(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=0.7) +
  labs(x = 'Marital status of the insured', y = 'Relativity') +
  geom_hline(yintercept = 1, linetype='dashed')+
  ylim(0.9, max(temp$freq, temp$freq2)*1.2)+
  guides(color=guide_legend(title="")) +
      theme_classic()+    theme(legend.position = 'bottom', legend.direction = "horizontal")

### Insured's territory

temp <- train2 %>%
  mutate(Var_ = Territory) %>%
  group_by(Var_) %>%
  summarize(nbclaim = sum(NB_Claim),
            expo = sum(expo),
            expo2 = sum(pred.tele)) %>%
  mutate(freq1 = nbclaim/expo2, 
         freq2 = meaninv*nbclaim/expo)

ggplot() + 
  geom_line(data = temp, aes(x = Var_, y = freq1, group = 1, color='Residuals'), size=0.7) +
  geom_point(data = temp, aes(x = Var_, y = freq1, group = 1, color='Residuals'), size=0.7) +
  geom_line(data = temp, aes(x = Var_, y = freq2, group = 1, color='Observed'), size=0.7) +
  geom_point(data = temp, aes(x = Var_, y = freq2, group = 1, color='Observed'), size=0.7) +
  geom_hline(yintercept = 1, linetype='dashed')+
  labs(x = 'Territory',
       y = 'Relativity') +
  scale_x_discrete(labels = NULL, breaks = NULL)+
  guides(color=guide_legend(title="")) +
      theme_classic()+    theme(legend.position = 'bottom', legend.direction = "horizontal")


```

### GLM-NET ON RESIDUALS

One can directly fit a GLM-Net model by appropriately utilizing the initial predictions. By retaining only the protected variables in the GLM-Net approach while using the prediction as an offset variable, it is possible to verify whether adding protected variables enhances the model's predictive capacity.  

The tables below indicate the scores obtained for this new model (with the two approaches marked with an *). We observe a slight improvement in the model, indicating that using telematics data does not eliminate the need to use protected variables.

```{r}
#| echo: false
#| eval: false
#| message: FALSE
#| warning: FALSE
#| code-fold: true

glm.score <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3)
                        + Dayformax + Dayformin +
                        + Miles.per.day + log(Miles.per.day)
                        + Avgdays.week + I(Avgdays.week^2)
                        + Pct.drive.mon + I(Pct.drive.mon^2)
                        + Pct.drive.tue + I(Pct.drive.tue^2)
                        + Pct.drive.wed + I(Pct.drive.wed^2)
                        + Pct.drive.thr + I(Pct.drive.thr^2)
                        + Pct.drive.fri + I(Pct.drive.fri^2)
                        + Pct.drive.sat + I(Pct.drive.sat^2)
                        + Pct.drive.sun + I(Pct.drive.sun^2)
                        + Pct.drive.wkend + I(Pct.drive.wkend^2)
                        + max.day + log(max.day) 
                        + min.day + I(min.day^2)
                        + max.min + I(max.min^2)
                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) 
                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   
                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) 
                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) 
                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) 
                        + Accel.06miles + I(Accel.06miles^2) + I(Accel.06miles^3)
                        + Accel.08miles + I(Accel.08miles^2) + I(Accel.08miles^3)
                        + Accel.09miles + I(Accel.09miles^2) + I(Accel.09miles^3)
                        + Accel.11miles + I(Accel.11miles^2) + I(Accel.11miles^3)
                        + Accel.12miles + I(Accel.12miles^2) + I(Accel.12miles^3)
                        + Accel.14miles + I(Accel.14miles^2) + I(Accel.14miles^3)
                        + Brake.06miles + I(Brake.06miles^2) + I(Brake.06miles^3)
                        + Brake.08miles + I(Brake.08miles^2) + I(Brake.08miles^3)
                        + Brake.09miles + I(Brake.09miles^2) + I(Brake.09miles^3)
                        + Brake.11miles + I(Brake.11miles^2) + I(Brake.11miles^3)
                        + Brake.12miles + I(Brake.12miles^2) + I(Brake.12miles^3)
                        + Brake.14miles + I(Brake.14miles^2) + I(Brake.14miles^3)
                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)
                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)
                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)
                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)
                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)
                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)
                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)
                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)
                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)
                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))


matrix.x <- model.matrix(glm.score, data=train2)[,-1]
y <- train2$NB_Claim
offset <- log(train2$expo)
    
lambda.min <- 3.981072e-05
lasso.min <- glmnet(matrix.x, y, family = "poisson", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.min)
train2$pred.tele <- predict(lasso.min, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.min)

###

var.sens <- c("Marital", "Insured.sex", "Credit.score", "Insured.age", "Territory")     
glm.score <- as.formula(NB_Claim ~ Insured.sex + Marital 
                                  + Credit.score +  I(Credit.score^2) 
                                  + Insured.age +  log(Insured.age) + I(Insured.age^2) 
                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )

matrix.x <- model.matrix(glm.score, data=train2)[,-1]
y <- train2$NB_Claim
offset <- log(train2$pred.tele)
fold.id <- train2$fold

lambda_seq <- c(10^seq(0, -8, by = -.1), 0)
cvfit0  <- cv.glmnet(matrix.x, y, relax=FALSE, family = "poisson", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0)
cvfit.2 <- cv.glmnet(matrix.x, y, relax=FALSE, family = "poisson", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.2)
cvfit.4 <- cv.glmnet(matrix.x, y, relax=FALSE, family = "poisson", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.4)
cvfit.6 <- cv.glmnet(matrix.x, y, relax=FALSE, family = "poisson", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.6)
cvfit.8 <- cv.glmnet(matrix.x, y, relax=FALSE, family = "poisson", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.8)
cvfit1  <- cv.glmnet(matrix.x, y, relax=FALSE, family = "poisson", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 1)

c(cvfit0$lambda.min, cvfit.2$lambda.min, cvfit.4$lambda.min, cvfit.6$lambda.min, cvfit.8$lambda.min, cvfit1$lambda.min)

all.min <- data.frame(c(min(cvfit0$cvm), min(cvfit.2$cvm), min(cvfit.4$cvm), min(cvfit.6$cvm), min(cvfit.8$cvm), min(cvfit1$cvm))) %>%
  mutate(alpha = 2*(row_number()-1)/10)
colnames(all.min)[1] <- 'min' 
all.min %>% filter(min == min(min))

cvfit1$lambda.min
cvfit1$lambda.1se


```


```{r}
#| echo: true
#| cache: false
#| message: FALSE
#| warning: FALSE
#| code-fold: true
#| label: tbl-Pscore_basetest4b
#| tbl-cap: Prediction scores for the GLM-net model  (testing set) 

glm.score <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) 
                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3)
                        + Dayformax + Dayformin +
                        + Miles.per.day + log(Miles.per.day)
                        + Avgdays.week + I(Avgdays.week^2)
                        + Pct.drive.mon + I(Pct.drive.mon^2)
                        + Pct.drive.tue + I(Pct.drive.tue^2)
                        + Pct.drive.wed + I(Pct.drive.wed^2)
                        + Pct.drive.thr + I(Pct.drive.thr^2)
                        + Pct.drive.fri + I(Pct.drive.fri^2)
                        + Pct.drive.sat + I(Pct.drive.sat^2)
                        + Pct.drive.sun + I(Pct.drive.sun^2)
                        + Pct.drive.wkend + I(Pct.drive.wkend^2)
                        + max.day + log(max.day) 
                        + min.day + I(min.day^2)
                        + max.min + I(max.min^2)
                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) 
                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   
                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) 
                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) 
                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) 
                        + Accel.06miles + I(Accel.06miles^2) + I(Accel.06miles^3)
                        + Accel.08miles + I(Accel.08miles^2) + I(Accel.08miles^3)
                        + Accel.09miles + I(Accel.09miles^2) + I(Accel.09miles^3)
                        + Accel.11miles + I(Accel.11miles^2) + I(Accel.11miles^3)
                        + Accel.12miles + I(Accel.12miles^2) + I(Accel.12miles^3)
                        + Accel.14miles + I(Accel.14miles^2) + I(Accel.14miles^3)
                        + Brake.06miles + I(Brake.06miles^2) + I(Brake.06miles^3)
                        + Brake.08miles + I(Brake.08miles^2) + I(Brake.08miles^3)
                        + Brake.09miles + I(Brake.09miles^2) + I(Brake.09miles^3)
                        + Brake.11miles + I(Brake.11miles^2) + I(Brake.11miles^3)
                        + Brake.12miles + I(Brake.12miles^2) + I(Brake.12miles^3)
                        + Brake.14miles + I(Brake.14miles^2) + I(Brake.14miles^3)
                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)
                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)
                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)
                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)
                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)
                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)
                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)
                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)
                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)
                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))

matrix.x <- model.matrix(glm.score, data=train2)[,-1]
y <- train2$NB_Claim
offset <- log(train2$expo)
    
lambda.min <- 3.981072e-05
lasso.min <- glmnet(matrix.x, y, family = "poisson", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.min)
train2$pred.tele <- predict(lasso.min, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.min)

matrix.x <- model.matrix(glm.score, data=test2)[,-1]
y <- test2$NB_Claim
offset <- log(test2$expo)
test2$pred.tele <- predict(lasso.min, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.min)

glm.score <- as.formula(NB_Claim ~ Insured.sex + Marital 
                                  + Credit.score +  I(Credit.score^2) 
                                  + Insured.age +  log(Insured.age) + I(Insured.age^2) 
                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )

matrix.x <- model.matrix(glm.score, data=train2)[,-1]
y <- train2$NB_Claim
offset <- log(train2$pred.tele)

lambda.min <- 1e-08
lambda.1se <- 0.007943282
    
lambda.select <- lambda.min
#fit <- glmnet(matrix.x, y, family = "poisson", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)
fit <- glmnet(matrix.x, y, family = "poisson", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)
  
matrix.x <- model.matrix(glm.score, data=test2)[,-1]
y <- test2$NB_Claim
offset <- log(test2$pred.tele)

test2$pred.base <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)

Result_ <- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))
Result_ <- cbind('LASSO* (optimal)', Result_)
colnames(Result_) <- c("Model", "Sc.log", "Sc.MSE", "Sc.quad", "Sc.sph", "Sc.DSS", "Sc.CRPS")

Result_all <- rbind(Result_all, Result_)

###

glm.score <- as.formula(NB_Claim ~ Insured.sex + Marital 
                                  + Credit.score +  I(Credit.score^2) 
                                  + Insured.age +  log(Insured.age) + I(Insured.age^2) 
                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )

matrix.x <- model.matrix(glm.score, data=train2)[,-1]
y <- train2$NB_Claim
offset <- log(train2$pred.tele)

lambda.min <- 1e-08
lambda.1se <- 0.007943282
    
lambda.select <- lambda.1se
fit <- glmnet(matrix.x, y, family = "poisson", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)
#fit <- glmnet(matrix.x, y, family = "poisson", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)
  
matrix.x <- model.matrix(glm.score, data=test2)[,-1]
y <- test2$NB_Claim
offset <- log(test2$pred.tele)

test2$pred.base <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)

Result_ <- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))
Result_ <- cbind('LASSO* (parsimonious)', Result_)
colnames(Result_) <- c("Model", "Sc.log", "Sc.MSE", "Sc.quad", "Sc.sph", "Sc.DSS", "Sc.CRPS")

Result_all <- rbind(Result_all, Result_)

knitr::kable(Result_all, align = "ccccccc", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = ","))%>%   
  kable_styling(bootstrap_options = "striped", full_width = T)  

```


:::

## XGBoost

We now consider an XGBoost model. As with the frequency model, hyperparameter values are obtained by performing a Bayesian search on a grid of possible values.

```{r}
#| echo: false
#| eval: false
#| message: FALSE
#| warning: FALSE

library(xgboost)
library(Ckmeans.1d.dp)
library(SHAPforxgboost)
library(pacman)

# p_load automatically installs packages if needed
p_load(xgboost, ParBayesianOptimization, mlbench, dplyr, skimr, recipes, resample)

all.vars2 <- c("Car.use", "Region", "Car.age", "Years.noclaims",
               "Miles.per.day", "Avgdays.week",
               "Pct.drive.mon", "Pct.drive.tue", "Pct.drive.wed", "Pct.drive.thr", "Pct.drive.fri", "Pct.drive.sat", "Pct.drive.sun",
               "max.day", "min.day", "max.min", "Dayformax", "Dayformin",
               "Pct.drive.rush.am", "Pct.drive.rush.pm",
               "Pct.drive.wkend",
               "Pct.drive.2hrs", "Pct.drive.3hrs", "Pct.drive.4hrs",
               "Accel.06miles", "Accel.08miles", "Accel.09miles", "Accel.11miles", "Accel.12miles", "Accel.14miles", 
               "Brake.06miles", "Brake.08miles", "Brake.09miles", "Brake.11miles", "Brake.12miles", "Brake.14miles", 
               "Left.turn.intensity08", "Left.turn.intensity09", "Left.turn.intensity10", "Left.turn.intensity11", "Left.turn.intensity12",
               "Right.turn.intensity08", "Right.turn.intensity09", "Right.turn.intensity10", "Right.turn.intensity11", "Right.turn.intensity12")

dtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(all.vars2)]), label = train2$NB_Claim)
setinfo(dtrain,"base_margin",log(train2$expo))
folds <-list(fold1 = which(train2$fold == 1),
             fold2 = which(train2$fold == 2),
             fold3 = which(train2$fold == 3),
             fold4 = which(train2$fold == 4),
             fold5 = which(train2$fold == 5))

bounds <- list(eta = c(0.001, 0.5),
               max_depth = c(1L, 50L),
               subsample = c(0.1, 1),
               min_child_weight = c(1, 175))

obj_func <- function(eta, max_depth, subsample, min_child_weight) {
  param <- list(
    eta = eta,
    max_depth = max_depth,
    subsample = subsample,
    min_child_weight = min_child_weight,
    booster = "gbtree",
    objective = "count:poisson",
    eval_metric = "poisson-nloglik")
  
  set.seed(533)
  xgbcv <- xgb.cv(params = param,
                  nrounds = base.rounds,
                  data = dtrain,
                  folds = folds,
                  prediction = TRUE,
                  early_stopping_rounds = 10,
                  verbose = 0,
                  maximize = F)
  
  lst <- list(
    Score = -min(xgbcv$evaluation_log$test_poisson_nloglik_mean),
    nrounds = xgbcv$best_iteration
  )
  
  return(lst)
}

base.rounds <- 500
set.seed(254)
bayes_out <- bayesOpt(FUN = obj_func, bounds = bounds, initPoints = length(bounds) + 2, iters.n = 5)
comp <- bayes_out$scoreSummary[which(bayes_out$scoreSummary$Score== max(bayes_out$scoreSummary$Score))]
comp 

#Epoch Iteration        eta max_depth subsample min_child_weight gpUtility acqOptimum inBounds Elapsed      Score nrounds errorMessage
#1:     0         4 0.02337437        26 0.8097923         123.6033        NA      FALSE     TRUE  735.83 -0.1325545     367           NA

############
## Verif ###
############
obj_func(eta=comp$eta, max_depth=comp$max_depth, subsample=comp$subsample, min_child_weight=comp$min_child_weight)

param <- list(
  eta = 0.02337437,
  max_depth = 26,
  subsample = 0.8097923,
  min_child_weight = 1,
  booster = "gbtree",
  objective = "count:poisson",
  eval_metric = "poisson-nloglik")

set.seed(133)
xgbcv <- xgb.cv(params = param,
                nrounds = 367,
                data = dtrain,
                folds = folds,
                prediction = TRUE,
                early_stopping_rounds = 10,
                verbose = 0,
                maximize = F)

-min(xgbcv$evaluation_log$test_poisson_nloglik_mean)

```


::: {.panel-tabset}

### Prediction Scores

We can calculate the models prediction scores based on all classical and telematics covariates. The XGBoost approach is particularly effective in capturing the effect of all available telematic covariates. Indeed, the scores obtained are significantly improved compared to other tested approaches.

```{r}
#| echo: false
#| eval: true
#| message: FALSE
#| warning: FALSE
#| output: false

library(xgboost)
library(Ckmeans.1d.dp)
library(SHAPforxgboost)

all.vars2 <- c("Car.use", "Region", "Car.age", "Years.noclaims",
               "Miles.per.day", "Avgdays.week",
               "Pct.drive.mon", "Pct.drive.tue", "Pct.drive.wed", "Pct.drive.thr", "Pct.drive.fri", "Pct.drive.sat", "Pct.drive.sun",
               "max.day", "min.day", "max.min", "Dayformax", "Dayformin",
               "Pct.drive.rush.am", "Pct.drive.rush.pm",
               "Pct.drive.wkend",
               "Pct.drive.2hrs", "Pct.drive.3hrs", "Pct.drive.4hrs",
               "Accel.06miles", "Accel.08miles", "Accel.09miles", "Accel.11miles", "Accel.12miles", "Accel.14miles", 
               "Brake.06miles", "Brake.08miles", "Brake.09miles", "Brake.11miles", "Brake.12miles", "Brake.14miles", 
               "Left.turn.intensity08", "Left.turn.intensity09", "Left.turn.intensity10", "Left.turn.intensity11", "Left.turn.intensity12",
               "Right.turn.intensity08", "Right.turn.intensity09", "Right.turn.intensity10", "Right.turn.intensity11", "Right.turn.intensity12")

dtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(all.vars2)]), label = train2$NB_Claim)
setinfo(dtrain,"base_margin",log(train2$expo))
folds <-list(fold1 = which(train2$fold == 1),
             fold2 = which(train2$fold == 2),
             fold3 = which(train2$fold == 3),
             fold4 = which(train2$fold == 4),
             fold5 = which(train2$fold == 5))


```


```{r}
#| echo: true
#| cache: false
#| message: FALSE
#| warning: FALSE
#| code-fold: true
#| label: tbl-Pscore_XGBoosttele
#| tbl-cap: Prediction scores for the XGBoost model with telematics

param <- list(
  eta = 0.02337437,
  max_depth = 26,
  subsample = 0.8097923,
  min_child_weight = 1,
  booster = "gbtree",
  objective = "count:poisson",
  eval_metric = "poisson-nloglik")

set.seed(133)
xgbcv <- xgb.cv(params = param,
                nrounds = 367,
                data = dtrain,
                folds = folds,
                prediction = TRUE,
                early_stopping_rounds = 10,
                verbose = 0,
                maximize = F)
  
Sc.log <- sapply(xgbcv$folds, function(x){-dpois(train2$NB_Claim[x], unlist(xgbcv$pred[x]), log=TRUE)})
Sc.MSE <- sapply(xgbcv$folds, function(x){(train2$NB_Claim[x]-unlist(xgbcv$pred[x]))^2})
Sc.quad <- sapply(xgbcv$folds, function(x){
  nb <- train2$NB_Claim[x]
  mu <- unlist(xgbcv$pred[x])
  -2*dpois(nb,lambda=mu) + dpois(0,lambda=mu)^2 + dpois(1,lambda=mu)^2 + dpois(2,lambda=mu)^2+ dpois(3,lambda=mu)^2 + dpois(4,lambda=mu)^2 + dpois(5,lambda=mu)^2 
})  
Sc.sph <- sapply(xgbcv$folds, function(x){
  nb <- train2$NB_Claim[x]
  mu <- unlist(xgbcv$pred[x])
  -dpois(nb,lambda=mu) / sqrt(dpois(0,lambda=mu)^2 + dpois(1,lambda=mu)^2 + dpois(2,lambda=mu)^2+ dpois(3,lambda=mu)^2 + dpois(4,lambda=mu)^2 + dpois(5,lambda=mu)^2 ) 
})
Sc.DSS <- sapply(xgbcv$folds, function(x){dss_pois(train2$NB_Claim[x], unlist(xgbcv$pred[x]))})
Sc.CRPS <- sapply(xgbcv$folds, function(x){crps_pois(train2$NB_Claim[x], unlist(xgbcv$pred[x]))})

Result_  <- rbind(
c(1,mean(Sc.log[1]$fold1), mean(Sc.MSE[1]$fold1), mean(Sc.quad[1]$fold1), mean(Sc.sph[1]$fold1), mean(Sc.DSS[1]$fold1), mean(Sc.CRPS[1]$fold1)),
c(2,mean(Sc.log[2]$fold2), mean(Sc.MSE[2]$fold2), mean(Sc.quad[2]$fold2), mean(Sc.sph[2]$fold2), mean(Sc.DSS[2]$fold2), mean(Sc.CRPS[2]$fold2)),
c(3,mean(Sc.log[3]$fold3), mean(Sc.MSE[3]$fold3), mean(Sc.quad[3]$fold3), mean(Sc.sph[3]$fold3), mean(Sc.DSS[3]$fold3), mean(Sc.CRPS[3]$fold3)),
c(4,mean(Sc.log[4]$fold4), mean(Sc.MSE[4]$fold4), mean(Sc.quad[4]$fold4), mean(Sc.sph[4]$fold4), mean(Sc.DSS[4]$fold4), mean(Sc.CRPS[4]$fold4)),
c(5,mean(Sc.log[5]$fold5), mean(Sc.MSE[5]$fold5), mean(Sc.quad[5]$fold5), mean(Sc.sph[5]$fold5), mean(Sc.DSS[5]$fold5), mean(Sc.CRPS[5]$fold5))
)

Res.sum  <- rbind(
c(sum(Sc.log[1]$fold1), sum(Sc.MSE[1]$fold1), sum(Sc.quad[1]$fold1), sum(Sc.sph[1]$fold1), sum(Sc.DSS[1]$fold1), sum(Sc.CRPS[1]$fold1)),
c(sum(Sc.log[2]$fold2), sum(Sc.MSE[2]$fold2), sum(Sc.quad[2]$fold2), sum(Sc.sph[2]$fold2), sum(Sc.DSS[2]$fold2), sum(Sc.CRPS[2]$fold2)),
c(sum(Sc.log[3]$fold3), sum(Sc.MSE[3]$fold3), sum(Sc.quad[3]$fold3), sum(Sc.sph[3]$fold3), sum(Sc.DSS[3]$fold3), sum(Sc.CRPS[3]$fold3)),
c(sum(Sc.log[4]$fold4), sum(Sc.MSE[4]$fold4), sum(Sc.quad[4]$fold4), sum(Sc.sph[4]$fold4), sum(Sc.DSS[4]$fold4), sum(Sc.CRPS[4]$fold4)),
c(sum(Sc.log[5]$fold5), sum(Sc.MSE[5]$fold5), sum(Sc.quad[5]$fold5), sum(Sc.sph[5]$fold5), sum(Sc.DSS[5]$fold5), sum(Sc.CRPS[5]$fold5))
)
sum <- c('Total', colSums(Res.sum)/nrow(train2))

Result_  <- data.frame(rbind(Result_, sum)) 

## Show results
colnames(Result_) <- c('Fold', "Sc.log", "Sc.MSE", "Sc.quad", "Sc.sph", "Sc.DSS", "Sc.CRPS")
Result_ <- rbind(Result_, Base)

Result_[nb.fold+2,1] <- 'Improvement'

for(i in 2:7){
  Result_[,i] <- as.numeric(Result_[,i])  
  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]
}

rownames(Result_) <- NULL
knitr::kable(Result_, align = "ccccccc", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = ","))%>%   
  kable_styling(bootstrap_options = "striped", full_width = T)  

```

On the *test* set, we obtain:

```{r}
#| echo: true
#| cache: false
#| message: FALSE
#| warning: FALSE
#| output: false
#| code-fold: true

dtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(all.vars2)]), label = train2$NB_Claim)
setinfo(dtrain,"base_margin",log(train2$expo))
dtest <- xgb.DMatrix(data = data.matrix(test2[, paste(all.vars2)]), label = test2$NB_Claim)
setinfo(dtest,"base_margin",log(test2$expo))
folds <-list(fold1 = which(train2$fold == 1),
             fold2 = which(train2$fold == 2),
             fold3 = which(train2$fold == 3),
             fold4 = which(train2$fold == 4),
             fold5 = which(train2$fold == 5))
```


```{r}
#| echo: true
#| cache: false
#| message: FALSE
#| warning: FALSE
#| code-fold: true
#| label: tbl-Pscore_XGBoost_correction333
#| tbl-cap: Prediction scores for the XGBoost model with telematics

param <- list(
  eta = 0.02337437,
  max_depth = 26,
  subsample = 0.8097923,
  min_child_weight = 1,
  booster = "gbtree",
  objective = "count:poisson",
  eval_metric = "poisson-nloglik")

set.seed(133)
fit.xgb <- xgb.train(params = param,
                     nrounds = 367,
                     data = dtrain)

train2$pred.xgb <- predict(fit.xgb, dtrain, type='response')


test2$pred.xgb <- predict(fit.xgb, dtest, type='response')

test2$pred.base <- test2$pred.xgb

Result_ <- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))
Result_ <- cbind('XGBoost', Result_)
colnames(Result_) <- c("Model", "Sc.log", "Sc.MSE", "Sc.quad", "Sc.sph", "Sc.DSS", "Sc.CRPS")

Result_all <- rbind(Result_all, Result_)

knitr::kable(Result_all, align = "ccccccc", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = ","))%>%   
  kable_styling(bootstrap_options = "striped", full_width = T)  


```

### Variables Importance

The graph below illustrates the most critical variables in the XGBoost model for claim frequency. As indicated in the literature, the level of vehicle usage, represented here by the daily mileage, is the most crucial variable in the model. Driving experience, measured by the number of claim-free years, follows. To a lesser extent, a series of telematic variables also has predictive capability.

```{r}
#| echo: true
#| cache: false
#| message: FALSE
#| warning: FALSE
#| code-fold: true

importance_matrix <- xgb.importance(dimnames(dtrain)[[2]], model = fit.xgb)
xgb.ggplot.importance(importance_matrix,top_n=15) + theme(text = element_text(size=15))


```

### RESIDUALS AND PROTECTED VARIABLES

As we did for the GLM-Net approach, we can again analyze the residuals of the approach to see if the addition of telematic data eliminates the need to use protected variables. The graphs below show that XGBoost is even more effective than GLM-Net. Indeed, although the credit score still appears useful in modeling frequency, its effect is greatly diminished. The effects of age, gender, and marital status of the insured are also significantly reduced. Finally, we can even see that the effect of territory is also greatly minimized.


```{r}
#| echo: true
#| message: FALSE
#| warning: FALSE
#| code-fold: true
#| layout-ncol: 2
#| label: fig-CreditScore_sev2
#| fig-cap: "Observed Relativity vs. Residuals Relativity"
#| fig-subcap: 
#|   - "Credit Score"
#|   - "Age of the Insured"
#|   - "Sex of the Insured"
#|   - "Marital Status of the Insured"
#|   - "Territory"
#| fig-width: 9
#| fig-height: 4

meaninv  <- sum(train2$expo)/sum(train2$NB_Claim)
moy.xgb <- sum(train2$pred.xgb)/sum(train2$NB_Claim)

temp2 <- train2 %>%
  dplyr::mutate(Duration.y = Duration/365.25, 
                Insured.age = pmin(Insured.age, 80),
                Group = ceiling(Credit.score/25) * 25) %>%
  group_by(Group) %>% 
  summarize(NB_Claim=sum(NB_Claim),
            expo=sum(expo),
            expo2=sum(pred.xgb)) %>% 
  mutate(freq = meaninv*NB_Claim/expo, 
         freq2 =  moy.xgb*NB_Claim/expo2)

Graph_resCS <- ggplot() + 
  geom_smooth(aes(x=Group, y=freq, weight = expo, color='Observed'),se=F, size=1, data=temp2) + 
  geom_smooth(aes(x=Group, y=freq2, weight = expo, color='Residuals'),se=F, size=1, data=temp2) + 
  labs(x = 'Credit Score',
       y = 'Relativity') +
  geom_hline(yintercept = 1, linetype='dashed')+
  guides(color = guide_legend(title = "")) +
    theme_classic()+    theme(legend.position = 'bottom', legend.direction = "horizontal")

print(Graph_resCS)
save(Graph_resCS, file = "Data/Graph_resCS.rdata")

### Age of the insured

temp2 <- train2 %>%
  dplyr::mutate(Duration.y = Duration/365.25, 
                Insured.age = pmin(Insured.age, 80), 
                Group = ceiling(Insured.age/5) * 5) %>%
  group_by(Group) %>% 
  summarize(NB_Claim=sum(NB_Claim),
            expo=sum(expo),
            expo2=sum(pred.xgb)) %>% 
  mutate(freq = meaninv*NB_Claim/expo, 
         freq2 =  moy.xgb*NB_Claim/expo2)

Graph_resAge <- ggplot() + 
  geom_smooth(aes(x=Group, y=freq, weight = expo, color='Observed'),se=F, size=1, data=temp2) + 
  geom_smooth(aes(x=Group, y=freq2, weight = expo, color='Residuals'),se=F, size=1, data=temp2) + 
  labs(x = 'Age of the insured',
       y = 'Relativity') +
  geom_hline(yintercept = 1, linetype='dashed')+
  guides(color = guide_legend(title = "")) +
    theme_classic()+    theme(legend.position = 'bottom', legend.direction = "horizontal")

print(Graph_resAge)
save(Graph_resAge, file = "Data/Graph_resAge.rdata")

### Sex of the insured


temp <- train2 %>%
  mutate(Var_ = Insured.sex) %>%
  group_by(Var_) %>%
  summarize(nbclaim = sum(NB_Claim),
            expo = sum(expo),
            expo2 = sum(pred.xgb)) %>%
  mutate(freq = nbclaim/expo,
         freq2 = nbclaim/expo2)

temp$freq <- temp$freq/temp$freq[1]
temp$freq2 <- temp$freq2/temp$freq2[1]

ggplot() + #start plot by by plotting bars
  geom_point(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=3) +
  geom_line(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=0.7) +
  geom_point(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=3) +
  geom_line(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=0.7) +
  labs(x = 'Sex of the insured', y = 'Relativity') +
  geom_hline(yintercept = 1, linetype='dashed')+
  ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+
  guides(color=guide_legend(title="")) +
    theme_classic()+    theme(legend.position = 'bottom', legend.direction = "horizontal")


### Marital Status


temp <- train2 %>%
  mutate(Var_ = Marital) %>%
  group_by(Var_) %>%
  summarize(nbclaim = sum(NB_Claim),
            expo = sum(expo),
            expo2 = sum(pred.xgb)) %>%
  mutate(freq = nbclaim/expo, 
         freq2 = nbclaim/expo2)

temp$freq <- temp$freq/temp$freq[1]
temp$freq2 <- temp$freq2/temp$freq2[1]

ggplot() + #start plot by by plotting bars
  geom_point(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=3) +
  geom_line(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=0.7) +
  geom_point(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=3) +
  geom_line(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=0.7) +
  labs(x = 'Marital status of the insured', y = 'Relativity') +
  geom_hline(yintercept = 1, linetype='dashed')+
  ylim(0.9, max(temp$freq, temp$freq2)*1.2)+
  guides(color=guide_legend(title="")) +
    theme_classic()+    theme(legend.position = 'bottom', legend.direction = "horizontal")

### Insured's territory

temp <- train2 %>%
  mutate(Var_ = Territory) %>%
  group_by(Var_) %>%
  summarize(nbclaim = sum(NB_Claim),
            expo = sum(expo),
            expo2 = sum(pred.xgb)) %>%
  mutate(freq = moy.xgb*nbclaim/expo2)

temp2 <- train2 %>%
  mutate(Var_ = Territory) %>%
  group_by(Var_) %>%
  summarize(nbclaim = sum(NB_Claim),
            expo = sum(expo)) %>%
  mutate(freq = meaninv*nbclaim/expo)

Graph_resTerr <- ggplot() + 
  geom_line(data = temp, aes(x = Var_, y = freq, group = 1, color='Residuals'), size=0.7) +
  geom_point(data = temp, aes(x = Var_, y = freq, group = 1, color='Residuals'), size=0.7) +
  geom_line(data = temp2, aes(x = Var_, y = freq, group = 1, color='Observed'), size=0.7) +
  geom_point(data = temp2, aes(x = Var_, y = freq, group = 1, color='Observed'), size=0.7) +
  geom_hline(yintercept = 1, linetype='dashed')+
  labs(x = 'Territory',
       y = 'Relativity') +
  scale_x_discrete(labels = NULL, breaks = NULL)+
  guides(color=guide_legend(title="")) +
    theme_classic()+    theme(legend.position = 'bottom', legend.direction = "horizontal")

print(Graph_resTerr)
save(Graph_resTerr, file = "Data/Graph_resTerr.rdata")


```




### XGBOOST ON RESIDUALS

We repeat the same exercise we did with the GLM-Net approach: fitting an XGBoost model on the residuals of the first XGBoost model. This will allow us to see if protected variables are capable of capturing trends in the approach's residuals.

The table below shows the different scores for the XGBoost* model. There is only a gain on some of the indicated scores. Hence, telematics data can substitute protected variables based on the studied dataset.

```{r}
#| echo: false
#| eval: false
#| message: FALSE
#| warning: FALSE

library(xgboost)
library(Ckmeans.1d.dp)
library(SHAPforxgboost)
library(pacman)

# p_load automatically installs packages if needed
p_load(xgboost, ParBayesianOptimization, mlbench, dplyr, skimr, recipes, resample)

var.sens <- c("Marital", "Insured.sex", "Credit.score", "Insured.age", "Territory")    

dtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(var.sens)]), label = train2$NB_Claim)
setinfo(dtrain,"base_margin",log(train2$pred.xgb))
folds <-list(fold1 = which(train2$fold == 1),
             fold2 = which(train2$fold == 2),
             fold3 = which(train2$fold == 3),
             fold4 = which(train2$fold == 4),
             fold5 = which(train2$fold == 5))

bounds <- list(eta = c(0.001, 0.5),
               max_depth = c(1L, 50L),
               subsample = c(0.1, 1),
               min_child_weight = c(1, 175))

obj_func <- function(eta, max_depth, subsample, min_child_weight) {
  param <- list(
    eta = eta,
    max_depth = max_depth,
    subsample = subsample,
    min_child_weight = min_child_weight,
    booster = "gbtree",
    objective = "count:poisson",
    eval_metric = "poisson-nloglik")
  
  set.seed(533)
  xgbcv <- xgb.cv(params = param,
                  nrounds = base.rounds,
                  data = dtrain,
                  folds = folds,
                  prediction = TRUE,
                  early_stopping_rounds = 10,
                  verbose = 0,
                  maximize = F)
  
  lst <- list(
    Score = -min(xgbcv$evaluation_log$test_poisson_nloglik_mean),
    nrounds = xgbcv$best_iteration
  )
  
  return(lst)
}

base.rounds <- 500
set.seed(254)
bayes_out <- bayesOpt(FUN = obj_func, bounds = bounds, initPoints = length(bounds) + 2, iters.n = 5)
comp <- bayes_out$scoreSummary[which(bayes_out$scoreSummary$Score== max(bayes_out$scoreSummary$Score))]
comp 

#   Epoch Iteration       eta max_depth subsample min_child_weight gpUtility acqOptimum inBounds Elapsed       Score nrounds errorMessage
#1:    10        16 0.2215803        50 0.7623535                1 0.7185566       TRUE     TRUE    7.14 -0.06200109       5           NA

############
## Verif ###
############
obj_func(eta=comp$eta, max_depth=comp$max_depth, subsample=comp$subsample, min_child_weight=comp$min_child_weight)

param <- list(
  eta = 0.2215803,
  max_depth = 50,
  subsample = 0.7623535,
  min_child_weight = 1,
  booster = "gbtree",
  objective = "count:poisson",
  eval_metric = "poisson-nloglik")

set.seed(533)
xgbcv <- xgb.cv(params = param,
                nrounds = 5,
                data = dtrain,
                folds = folds,
                prediction = TRUE,
                early_stopping_rounds = 10,
                verbose = 0,
                maximize = F)

-min(xgbcv$evaluation_log$test_poisson_nloglik_mean)

```




```{r}
#| echo: true
#| cache: false
#| message: FALSE
#| warning: FALSE
#| code-fold: true
#| output: false

library(xgboost)
library(Ckmeans.1d.dp)
library(SHAPforxgboost)
library(pacman)

var.sens <- c("Marital", "Insured.sex", "Credit.score", "Insured.age", "Territory")    

dtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(var.sens)]), label = train2$NB_Claim)
setinfo(dtrain,"base_margin",log(train2$pred.xgb))
folds <-list(fold1 = which(train2$fold == 1),
             fold2 = which(train2$fold == 2),
             fold3 = which(train2$fold == 3),
             fold4 = which(train2$fold == 4),
             fold5 = which(train2$fold == 5))

param <- list(
  eta = 0.2215803,
  max_depth = 50,
  subsample = 0.7623535,
  min_child_weight = 1,
  booster = "gbtree",
  objective = "count:poisson",
  eval_metric = "poisson-nloglik")

set.seed(533)
fit.xgb2 <- xgb.train(params = param,
                      nrounds = 5,
                      data = dtrain)

dtest <- xgb.DMatrix(data = data.matrix(test2[, paste(var.sens)]), label = test2$NB_Claim)
setinfo(dtest,"base_margin",log(test2$pred.xgb))

```


```{r}
#| echo: true
#| cache: false
#| message: FALSE
#| warning: FALSE
#| code-fold: true
#| label: tbl-Pscore_XGBoost_correction33
#| tbl-cap: Prediction scores for the XGBoost model with telematics

test2$pred.base <- predict(fit.xgb2, dtest, type='response')

Result_ <- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))
Result_ <- cbind('XGBoost*', Result_)
colnames(Result_) <- c("Model", "Sc.log", "Sc.MSE", "Sc.quad", "Sc.sph", "Sc.DSS", "Sc.CRPS")

Result_all <- rbind(Result_all, Result_)

save(Result_all, file='Data/ResultsSynth.Rda')

knitr::kable(Result_all, align = "ccccccc", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = ","))%>%   
  kable_styling(bootstrap_options = "striped", full_width = T)  

```

Although the XGBoost model on residuals does not yield significant gains, the graph below illustrates the most critical protected variables in the XGBoost model. It shows that the insured's credit score and territory are the most critical variables in this model.

```{r}
#| echo: true
#| cache: false
#| message: FALSE
#| warning: FALSE
#| code-fold: true

importance_matrix <- xgb.importance(dimnames(dtrain)[[2]], model = fit.xgb2)
xgb.ggplot.importance(importance_matrix,top_n=15) + theme(text = element_text(size=15))


```



:::




