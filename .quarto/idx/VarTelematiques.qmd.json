{"title":"Telematic Covariates","markdown":{"headingText":"Telematic Covariates","containsRefs":false,"markdown":"\n## Preamble\n\n::: {.panel-tabset}\n\n### Chapter Objective\n\nWe continue the analysis of claim frequency by adding telematic variables to the same three approaches of the last chapter. However, we will remove protected traditional variables from our analysis. Specifically, we will not use the following five covariates in our models:  \n\n  1) Credit.score,    \n  2) Insured.age,  \n  3) Insured.sex,  \n  4) Marital,  \n  5) Territory.\n\nThe objective is to assess the performance of ratemaking approaches incorporating telematics information without protected covariates. By analyzing the residuals of the approach, we will gauge the relevance of those protected covariates.\n\nTo compare models, we will employ the following functions that calculate predictive scores:\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\nScore.pred <- function(mu, x) {\n  Sc.log  <- -sum(dpois(x, mu, log=TRUE))\n  Sc.MSE  <- sum((x - mu)^2)\n  Sc.quad <- sum(-2*dpois(x,lambda=mu) + sapply(mu, function(x){ sum(dpois(0:10,lambda=x)^2) }))\n  Sc.sph <- sum(- dpois(x,mu) / sqrt(sapply(mu, function(x){ sum(dpois(0:10,lambda=x)^2) })))\n  Sc.DSS <- sum(dss_pois(x, mu))\n  Sc.CRPS <- sum(crps_pois(x, mu))\n    \n  return(c(Sc.log, Sc.MSE, Sc.quad, Sc.sph, Sc.DSS, Sc.CRPS))\n}\n\n\n```\n\n### Packages\n\nHere is the list of packages that will be used:\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\nlibrary(tidyverse)\nlibrary(vtable)\nlibrary(rpart)\nlibrary(repr)\nlibrary(rpart.plot)\nlibrary(gam)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(glmnet)\nlibrary(scoringRules)\nlibrary(sjPlot)\n\n```\n\n### Data\n\nThe same data is used.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\ndataS <- read.csv('Data/Synthetic.csv')\n\n# Modifications \ndataS <- dataS %>%\n  mutate(Territory = as.factor(Territory)) %>%\n  select(-c('Annual.pct.driven', 'Annual.miles.drive'))\ndata.select <- dataS\n\n# Train-test \nset.seed(123)\ntrain <- data.select %>% sample_frac(0.8, replace = FALSE)\ntest <- data.select %>% anti_join(train)\n```\n\n### Data Transformation\n\nAs we concluded at the end of our overview of the data, a transformation of certain variables is also necessary.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\n# Modif data\ntrain2 <- train %>%\n  mutate(Miles.per.day = Total.miles.driven/Duration,\n         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),\n         expo = Duration/365.25)\n\ntransform.fct <- function(var){\n  df <- train2 %>% mutate(var_ = get(var)*Total.miles.driven/(1000*Duration))\n  q99 <- quantile(df$var_, 0.99)\n  df <- df %>% mutate(var_ = ifelse(var_ > q99, q99, var_))\n  #colnames(df)[ncol(df)] <- paste0(var, '_')\n  return(df)\n}\n\ntrain2 <- transform.fct(\"Brake.06miles\")\ntrain2 <- transform.fct(\"Brake.08miles\")\ntrain2 <- transform.fct(\"Brake.09miles\")\ntrain2 <- transform.fct(\"Brake.11miles\")\ntrain2 <- transform.fct(\"Brake.14miles\")\ntrain2 <- transform.fct(\"Accel.06miles\")\ntrain2 <- transform.fct(\"Accel.08miles\")\ntrain2 <- transform.fct(\"Accel.09miles\")\ntrain2 <- transform.fct(\"Accel.11miles\")\ntrain2 <- transform.fct(\"Accel.12miles\")\ntrain2 <- transform.fct(\"Accel.14miles\")\ntrain2 <- transform.fct(\"Left.turn.intensity08\")\ntrain2 <- transform.fct(\"Left.turn.intensity09\")\ntrain2 <- transform.fct(\"Left.turn.intensity10\")\ntrain2 <- transform.fct(\"Left.turn.intensity11\")\ntrain2 <- transform.fct(\"Left.turn.intensity12\")\ntrain2 <- transform.fct(\"Right.turn.intensity08\")\ntrain2 <- transform.fct(\"Right.turn.intensity09\")\ntrain2 <- transform.fct(\"Right.turn.intensity10\")\ntrain2 <- transform.fct(\"Right.turn.intensity11\")\ntrain2 <- transform.fct(\"Right.turn.intensity12\")\n\n# Create folds\nnb.fold <- 5\nfold <- sample(1:nb.fold, nrow(train2), replace = TRUE)\ntrain2$fold <- fold\n\n##\n\ntest2 <- test %>%\n  mutate(Miles.per.day = Total.miles.driven/Duration,\n         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),\n         expo = Duration/365.25)\n\ntransform.fct <- function(var){\n  df <- test2 %>% mutate(var_ = get(var)*Total.miles.driven/(1000*Duration))\n  q99 <- quantile(df$var_, 0.99)\n  df <- df %>% mutate(var_ = ifelse(var_ > q99, q99, var_))\n  #colnames(df)[ncol(df)] <- paste0(var, '_')\n  return(df)\n}\n\ntest2 <- transform.fct(\"Brake.06miles\")\ntest2 <- transform.fct(\"Brake.08miles\")\ntest2 <- transform.fct(\"Brake.09miles\")\ntest2 <- transform.fct(\"Brake.11miles\")\ntest2 <- transform.fct(\"Brake.14miles\")\ntest2 <- transform.fct(\"Accel.06miles\")\ntest2 <- transform.fct(\"Accel.08miles\")\ntest2 <- transform.fct(\"Accel.09miles\")\ntest2 <- transform.fct(\"Accel.11miles\")\ntest2 <- transform.fct(\"Accel.12miles\")\ntest2 <- transform.fct(\"Accel.14miles\")\ntest2 <- transform.fct(\"Left.turn.intensity08\")\ntest2 <- transform.fct(\"Left.turn.intensity09\")\ntest2 <- transform.fct(\"Left.turn.intensity10\")\ntest2 <- transform.fct(\"Left.turn.intensity11\")\ntest2 <- transform.fct(\"Left.turn.intensity12\")\ntest2 <- transform.fct(\"Right.turn.intensity08\")\ntest2 <- transform.fct(\"Right.turn.intensity09\")\ntest2 <- transform.fct(\"Right.turn.intensity10\")\ntest2 <- transform.fct(\"Right.turn.intensity11\")\ntest2 <- transform.fct(\"Right.turn.intensity12\")\n\n# Mean Encoding with White Noise pour les territoires\ncardi <- length(unique(train$Territory))\n\nenc.terr <- train2 %>%\n  group_by(Territory) %>%\n  summarize(freq = sum(NB_Claim)/sum(expo)) %>%\n  arrange(freq) %>%\n  mutate(terr.code= row_number()/(cardi+1)) %>%\n  select(Territory, terr.code)\n\ntrain2 <- train2 %>%\n  group_by(Territory) %>%\n  left_join(enc.terr, by='Territory') %>%\n  ungroup()\n\ntest2 <- test2 %>%\n  group_by(Territory) %>%\n  left_join(enc.terr, by='Territory') %>%\n  ungroup()\n\n\n```\n\n:::\n\n## Basic GLM Models\n\n::: {.panel-tabset}\n\n### Single intercept\n\nA baseline model corresponding to a Generalized Linear Model (GLM) with intercept and predicting for each contract only the observed mean multiplied by the exposure is used as a point of comparison.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_base\n#| tbl-cap: Prediction scores for the base model\n\n## Model on each fold\nResult_  <- data.frame()\nResult2_  <- data.frame()\nfor(i in 1:nb.fold) {\n    learn <- train2[train2$fold != i,]\n    valid <- train2[train2$fold == i,]\n\n    mean <- sum(learn$NB_Claim)/sum(learn$expo) \n    learn$pred.base <- mean*learn$expo\n    valid$pred.base <- mean*valid$expo\n\n    Result_ <- rbind(Result_, c(i, Score.pred(valid$pred.base, valid$NB_Claim)/nrow(valid)))\n    Result2_ <- rbind(Result2_, c(i, Score.pred(valid$pred.base, valid$NB_Claim)))\n}\n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ncolnames(Result2_) <- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ntot <- colSums(Result2_)/nrow(train2)\ntot$Fold <- 'Total'\nResult_ <- rbind(Result_ , tot)\n\nResult.base <- Result_  \nBase <- Result.base[nb.fold+1,]\n\nknitr::kable(Result_, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n```\n\nThe model is trained on the entire training dataset and subsequently tested on the untouched test dataset, ensuring that the parameter calibration process remains independent from the test data.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_basetest\n#| tbl-cap: Prediction scores for the base model (testing set) \n\nmean <- sum(train2$NB_Claim)/sum(train2$expo) \ntest2$pred.base <- mean*test2$expo\n\nResult_ <- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ <- cbind('Base', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all <- Result_\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n```\n\n### Traditional covariates already used (without protected variables)\n\nFor comparison, we start with a simple GLM model considering only the traditional covariates (excluding the protected variables). Therefore, we include:  \n\n  - Car.use,  \n  - Region,  \n  - Car.age, \n  - Years.noclaims.  \n\nThe model's prediction scores are displayed below. As expected, adding segmentation variables improves the prediction scores compared to the simple baseline model with only an intercept.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-TeleGLM2\n#| tbl-cap: Prediction scores for the GLM model with traditional covariates (without protected)\n\n## Model \nscore.base <- as.formula(NB_Claim ~ 1 + offset(log(expo)))\nscore.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + offset(log(expo)))\n## Model on each fold\nResult_  <- data.frame()\nResult2_  <- data.frame()\nfor(i in 1:nb.fold) {\n    learn <- train2[train2$fold != i,]\n    valid <- train2[train2$fold == i,]\n    glm.fit <- glm(score.glm, family = poisson(), data = learn)\n\n    learn$pred.base <- predict(glm.fit, newdata=learn, type='response')\n    valid$pred.base <- predict(glm.fit, newdata=valid, type='response')\n\n    Result_ <- rbind(Result_, c(i, Score.pred(valid$pred.base, valid$NB_Claim)/nrow(valid)))\n    Result2_ <- rbind(Result2_, c(i, Score.pred(valid$pred.base, valid$NB_Claim)))\n}\n\n## Model on all data from train\nglm.base <- glm(score.base, family = poisson(), data = train2)\nglm.fit <- glm(score.glm, family = poisson(), data = train2)\ntrain2$pred.glm1 <- predict(glm.fit, newdata=train2, type='response')\nResult.glm1 <- Result_  \n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ncolnames(Result2_) <- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ntot <- colSums(Result2_)/nrow(train2)\ntot$Fold <- 'Total'\nResult_ <- rbind(Result_ , tot)\nResult_ <- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] <- 'Improvement'\n\nfor(i in 2:7){\n  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) <- NULL\nknitr::kable(Result_, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n```\n\nThe comparison with the test dataset is also depicted in the table below. \n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_basetest2\n#| tbl-cap: Prediction scores for the GLM model with traditional covariates (testing set) \n\nscore.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + offset(log(expo)))\n\nglm.fit <- glm(score.glm, family = poisson(), data = train2)\ntest2$pred.base <- predict(glm.fit, newdata=test2, type='response')\n\nResult_ <- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ <- cbind('GLM (trad.)', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n```\n\n### Estimated Parameters\n\nThe table below shows the estimators obtained for the GLM-Poisson approach and compares them with the baseline model, which has only an intercept.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-TeleGLM1\n#| tbl-cap: Estimated parameters for the GLM model with traditional covariates (without protected)\n\n## Model \nscore.base <- as.formula(NB_Claim ~ 1 + offset(log(expo)))\n\nscore.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + offset(log(expo)))\n\n## Model on all data from train\nglm.base <- glm(score.base, family = poisson(), data = train2)\nglm.fit <- glm(score.glm, family = poisson(), data = train2)\n\ntab_model(glm.base, glm.fit, transform = NULL)\n\n```\n\n\n\n:::\n\n## GLM-Net\n\n### Parametric transformation of continuous covariates\n\nNow, we add all available telematic variables to the model. Similar to the approach taken in the previous chapter, we will introduce a method utilizing the Generalized Additive Models (GAM) theory for all these continuous variables. This will enable us to observe the general form of the covariate to explain the number of claims. Subsequently, a parametric form will be proposed to achieve the best possible correspondence with the spline obtained by the GAM.\n\n::: {.panel-tabset}\n\n### VEHICLE USAGE LEVEL\n\nFor the two covariates related to usage level, the proposed parametric forms are as follows:\n\n\\begin{align*}\ns(Miles.per.day) &\\approx Miles.per.day + log(Miles.per.day)\\\\\ns(Avgdays.week) &\\approx Avgdays.week + Avgdays.week^2\n\\end{align*}\n\nThe graphs below compare the fit of the parametric approach with that of the GAM model.\n\n\n```{r}\n#| echo: true\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| layout-ncol: 2\n#| label: fig-MpD_GAM_sev_tel\n#| fig-cap: \"Smoothing of Usage level covariates\"\n#| fig-subcap: \n#|   - \"Miles.per.day\"\n#|   - \"Avgdays.week\"\n\nmin_ <- min(train2$Miles.per.day) \nmax_ <- max(train2$Miles.per.day) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'Miles.per.day'\n\nq99 <- quantile(train2$Miles.per.day, 0.99)\n\ndb <- train2 %>%\n  select(-'Miles.per.day') %>%\n  slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + s(Miles.per.day))\n\nscore.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + Miles.per.day + log(Miles.per.day) )\n\ngam.fit <- gam(score.gam, family = poisson(), data = train2)\nglm.fit <- glm(score.glm, family = poisson(), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')\nbase <- db %>%\n  mutate(diff = abs(Miles.per.day - mean(train2$Miles.per.day))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=Miles.per.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=Miles.per.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Miles per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n### Avgdays.week\n\nmin_ <- min(train2$Avgdays.week) \nmax_ <- max(train2$Avgdays.week) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'Avgdays.week'\n\nq99 <- quantile(train2$Avgdays.week, 0.99)\n\ndb <- train2 %>%\n  select(-'Avgdays.week') %>%\n  slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + s(Avgdays.week))\n\nscore.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + Avgdays.week + I(Avgdays.week^2) )\n\ngam.fit <- gam(score.gam, family = poisson(), data = train2)\nglm.fit <- glm(score.glm, family = poisson(), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')\nbase <- db %>%\n  mutate(diff = abs(Avgdays.week - mean(train2$Avgdays.week))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=Avgdays.week, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=Avgdays.week, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Avgdays.week',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n```\n\n### Type of Vehicle Usage\n\n\nSeveral covariates are available in the category *Type of vehicle usage*:  \n\n- We propose the same parametric form for all variants of the variable *Pct.drive.day* (Monday to Sunday);  \n- The same parametric form will also be proposed for *Pct.drive.rush.am*, *Pct.drive.rush.pm*, *Pct.drive.2hrs*, *Pct.drive.3hrs*, and *Pct.drive.4hrs*;  \n- The other three covariates have their own parametric form.\n\nWe then have:\n\n\\begin{align*}\ns(Pct.drive.day) &\\approx Pct.drive.day + Pct.drive.day^2 \\\\\ns(Pct.drive) &\\approx Pct.drive + \\sqrt{Pct.drive} \\\\\ns(max.day) &\\approx max.day + \\log(max.day) \\\\\ns(min.day) &\\approx min.day + min.day^2 \\\\\ns(max.min) &\\approx max.min + max.min^2 \n\\end{align*}\n\n\n```{r}\n#| echo: true\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| layout-ncol: 2\n#| layout-nrow: 3\n#| label: fig-MpD_GAM_sev_tel3\n#| fig-cap: \"Smoothing of Type of vehicle usage covariates (severity)\"\n#| fig-subcap: \n#|   - \"Pct.drive.mon\"\n#|   - \"Pct.drive.rush.am\"\n#|   - \"max.day\"\n#|   - \"min.day\"\n#|   - \"max.min\"\n\ntrain2$Pct.drive <- train2$Pct.drive.sun\n\nmin_ <- min(train2$Pct.drive) \nmax_ <- max(train2$Pct.drive) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'Pct.drive'\n\nq99 <- quantile(train2$Pct.drive, 0.99)\n\ndb <- train2 %>%\n  select(-'Pct.drive') %>%\n  slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + s(Pct.drive))\n\nscore.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + Pct.drive + I(Pct.drive^2) )\n\ngam.fit <- gam(score.gam, family = poisson(), data = train2)\nglm.fit <- glm(score.glm, family = poisson(), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')\nbase <- db %>%\n  mutate(diff = abs(Pct.drive - mean(train2$Pct.drive))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=Pct.drive, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=Pct.drive, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Pct.drive',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n### Pct.drive \n\n\ntrain2$use.day <- train2$Pct.drive.rush.am\n\nmin_ <- min(train2$use.day) \nmax_ <- max(train2$use.day) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'use.day'\n\nq99 <- quantile(train2$use.day, 0.99)\n\ndb <- train2 %>%\n  select(-'use.day') %>%\n  slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + s(use.day))\n\nscore.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + use.day + I(use.day^0.5))\n\ngam.fit <- gam(score.gam, family = poisson(), data = train2)\nglm.fit <- glm(score.glm, family = poisson(), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')\nbase <- db %>%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n### Max day\n\ntrain2$use.day <- train2$max.day\n\nmin_ <- min(train2$use.day) \nmax_ <- max(train2$use.day) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'use.day'\n\nq99 <- quantile(train2$use.day, 0.99)\n\ndb <- train2 %>%\n  select(-'use.day') %>%\n  slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + s(use.day))\n\nscore.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + use.day + log(use.day) )\n\ngam.fit <- gam(score.gam, family = poisson(), data = train2)\nglm.fit <- glm(score.glm, family = poisson(), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')\nbase <- db %>%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Min day\n\n\ntrain2$use.day <- train2$min.day\n\nmin_ <- min(train2$use.day) \nmax_ <- max(train2$use.day) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'use.day'\n\nq99 <- quantile(train2$use.day, 0.99)\n\ndb <- train2 %>%\n  select(-'use.day') %>%\n  slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + s(use.day))\n\nscore.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + use.day + I(use.day^2) )\n\ngam.fit <- gam(score.gam, family = poisson(), data = train2)\nglm.fit <- glm(score.glm, family = poisson(), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')\nbase <- db %>%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Max min\n\ntrain2$use.day <- train2$max.min\n\nmin_ <- min(train2$use.day) \nmax_ <- max(train2$use.day) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'use.day'\n\nq99 <- quantile(train2$use.day, 0.99)\n\ndb <- train2 %>%\n  select(-'use.day') %>%\n  slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + s(use.day))\n\nscore.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + use.day + I(use.day^2) )\n\ngam.fit <- gam(score.gam, family = poisson(), data = train2)\nglm.fit <- glm(score.glm, family = poisson(), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')\nbase <- db %>%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n\n```\n\n\n### Driving Behavior\n\nThe same parametric form is proposed for the different variants of the *Accel* and *Brake* variables, i.e., *Accel.06miles* to *Accel.14miles*, and *Brake.06miles* to *Brake.14miles*.  For the different variants of the *turn* variable, a single parametric form is also used: \n\n\\begin{align*}\ns(Brake.Accel) &\\approx Brake.Accel + Brake.Accel^2 + Brake.Accel^3\\\\\ns(Turn) &\\approx Turn + log(Turn)\n\\end{align*}\n\nThe graphs below compare the fit of the parametric approach for *Accel.06miles* and *Right.turn.intensity08* with that of the GAM model. \n\n```{r}\n#| echo: true\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| layout-ncol: 2\n#| label: fig-MpD_GAM_sev_tel2\n#| fig-cap: \"Smoothing of Driving behavior covariates (severity)\"\n#| fig-subcap: \n#|   - \"Accel.06miles\"\n#|   - \"Right.turn.intensity08\"\n\ntrain2$use.day <- train2$Accel.06miles\n\nmin_ <- min(train2$use.day) \nmax_ <- max(train2$use.day) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'use.day'\n\nq99 <- quantile(train2$use.day, 0.99)\n\ndb <- train2 %>%\n  select(-'use.day') %>%\n  slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + s(use.day))\n\nscore.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + use.day + I(use.day^2) + I(use.day^3) )\n\ngam.fit <- gam(score.gam, family = poisson(), data = train2)\nglm.fit <- glm(score.glm, family = poisson(), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')\nbase <- db %>%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n### Right and left turns\n\n\ntrain2$use.day <- train2$Right.turn.intensity08\n\nq99 <- quantile(train2$use.day, 0.99)\n\nmin_ <- min(train2$use.day) \nmax_ <- q99\nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'use.day'\n\ndb <- train2 %>%\n  select(-'use.day') %>%\n  slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\ntemp <- train2 %>%\n  mutate(use.day = pmin(q99, use.day))\n\nscore.gam <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                        + s(use.day))\n\nscore.glm <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) + offset(log(expo))\n                         + use.day + log1p(use.day))\n\ngam.fit <- gam(score.gam, family = poisson(), data = temp)\nglm.fit <- glm(score.glm, family = poisson(), data = temp)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')\nbase <- db %>%\n  mutate(diff = abs(use.day - mean(temp$use.day))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n  \nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n # xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n\n```\n\n:::\n\n### Fitting the GLM-Net model\n\n```{r}\n#| echo: false\n#| eval: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\nglm.score <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3)\n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + log(max.day) \n                        + min.day + I(min.day^2)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Accel.06miles + I(Accel.06miles^2) + I(Accel.06miles^3)\n                        + Accel.08miles + I(Accel.08miles^2) + I(Accel.08miles^3)\n                        + Accel.09miles + I(Accel.09miles^2) + I(Accel.09miles^3)\n                        + Accel.11miles + I(Accel.11miles^2) + I(Accel.11miles^3)\n                        + Accel.12miles + I(Accel.12miles^2) + I(Accel.12miles^3)\n                        + Accel.14miles + I(Accel.14miles^2) + I(Accel.14miles^3)\n                        + Brake.06miles + I(Brake.06miles^2) + I(Brake.06miles^3)\n                        + Brake.08miles + I(Brake.08miles^2) + I(Brake.08miles^3)\n                        + Brake.09miles + I(Brake.09miles^2) + I(Brake.09miles^3)\n                        + Brake.11miles + I(Brake.11miles^2) + I(Brake.11miles^3)\n                        + Brake.12miles + I(Brake.12miles^2) + I(Brake.12miles^3)\n                        + Brake.14miles + I(Brake.14miles^2) + I(Brake.14miles^3)\n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n                        \n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$NB_Claim\noffset <- log(train2$expo)\nfold.id <- train2$fold\n\nlambda_seq <- c(10^seq(0, -8, by = -.1), 0)\ncvfit0  <- cv.glmnet(matrix.x, y, relax=FALSE, family = \"poisson\", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0)\ncvfit.2 <- cv.glmnet(matrix.x, y, relax=FALSE, family = \"poisson\", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.2)\ncvfit.4 <- cv.glmnet(matrix.x, y, relax=FALSE, family = \"poisson\", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.4)\ncvfit.6 <- cv.glmnet(matrix.x, y, relax=FALSE, family = \"poisson\", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.6)\ncvfit.8 <- cv.glmnet(matrix.x, y, relax=FALSE, family = \"poisson\", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.8)\ncvfit1  <- cv.glmnet(matrix.x, y, relax=FALSE, family = \"poisson\", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 1)\n\nc(cvfit0$lambda.min, cvfit.2$lambda.min, cvfit.4$lambda.min, cvfit.6$lambda.min, cvfit.8$lambda.min, cvfit1$lambda.min)\n\nall.min <- data.frame(c(min(cvfit0$cvm), min(cvfit.2$cvm), min(cvfit.4$cvm), min(cvfit.6$cvm), min(cvfit.8$cvm), min(cvfit1$cvm))) %>%\n  mutate(alpha = 2*(row_number()-1)/10)\ncolnames(all.min)[1] <- 'min' \nall.min %>% filter(min == min(min))\n\ncvfit1$lambda.min\ncvfit1$lambda.1se\n\n```\n\n::: {.panel-tabset}\n\n### Optimal value\n\nThe parameters of the GLM-net were calibrated using cross-validation to obtain the model's hyperparameters. Using these values, we can calculate the prediction scores of the model based on all covariates.\n\n```{r}\n#| echo: true\n#| cache: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_teleGLMnet1\n#| tbl-cap: Prediction scores for the GLM-net model (alpha=1)\n\nglm.score <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3)\n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + log(max.day) \n                        + min.day + I(min.day^2)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Accel.06miles + I(Accel.06miles^2) + I(Accel.06miles^3)\n                        + Accel.08miles + I(Accel.08miles^2) + I(Accel.08miles^3)\n                        + Accel.09miles + I(Accel.09miles^2) + I(Accel.09miles^3)\n                        + Accel.11miles + I(Accel.11miles^2) + I(Accel.11miles^3)\n                        + Accel.12miles + I(Accel.12miles^2) + I(Accel.12miles^3)\n                        + Accel.14miles + I(Accel.14miles^2) + I(Accel.14miles^3)\n                        + Brake.06miles + I(Brake.06miles^2) + I(Brake.06miles^3)\n                        + Brake.08miles + I(Brake.08miles^2) + I(Brake.08miles^3)\n                        + Brake.09miles + I(Brake.09miles^2) + I(Brake.09miles^3)\n                        + Brake.11miles + I(Brake.11miles^2) + I(Brake.11miles^3)\n                        + Brake.12miles + I(Brake.12miles^2) + I(Brake.12miles^3)\n                        + Brake.14miles + I(Brake.14miles^2) + I(Brake.14miles^3)\n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\nResult_  <- data.frame()\nResult2_  <- data.frame()\nfor(i in 1:nb.fold) {\n    learn <- train2[train2$fold != i,]\n    valid <- train2[train2$fold == i,]\n    \n    matrix.x <- model.matrix(glm.score, data=learn)[,-1]\n    y <- learn$NB_Claim\n    offset <- log(learn$expo)\n\n    lambda.min <- 3.981072e-05\n    lambda.1se <- 0.001258925\n    \n    lambda.select <- lambda.min\n    fit <- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\n    #fit <- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n  \n    matrix.x <- model.matrix(glm.score, data=valid)[,-1]\n    y <- valid$NB_Claim\n    offset <- log(valid$expo)\n\n    valid$pred <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)\n    \n    Result_ <- rbind(Result_, c(i, Score.pred(valid$pred, valid$NB_Claim)/nrow(valid)))\n    Result2_ <- rbind(Result2_, c(i, Score.pred(valid$pred, valid$NB_Claim)))\n}\n\n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ncolnames(Result2_) <- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ntot <- colSums(Result2_)/nrow(train2)\ntot$Fold <- 'Total'\nResult_ <- rbind(Result_ , tot)\nResult_ <- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] <- 'Improvement'\n\nfor(i in 2:7){\n  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) <- NULL\nknitr::kable(Result_, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n```\n\nOn the *test* set, we obtain:\n\n```{r}\n#| echo: true\n#| cache: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_basetest3\n#| tbl-cap: Prediction scores for the GLM-net model  (testing set) \n\nglm.score <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3)\n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + log(max.day) \n                        + min.day + I(min.day^2)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Accel.06miles + I(Accel.06miles^2) + I(Accel.06miles^3)\n                        + Accel.08miles + I(Accel.08miles^2) + I(Accel.08miles^3)\n                        + Accel.09miles + I(Accel.09miles^2) + I(Accel.09miles^3)\n                        + Accel.11miles + I(Accel.11miles^2) + I(Accel.11miles^3)\n                        + Accel.12miles + I(Accel.12miles^2) + I(Accel.12miles^3)\n                        + Accel.14miles + I(Accel.14miles^2) + I(Accel.14miles^3)\n                        + Brake.06miles + I(Brake.06miles^2) + I(Brake.06miles^3)\n                        + Brake.08miles + I(Brake.08miles^2) + I(Brake.08miles^3)\n                        + Brake.09miles + I(Brake.09miles^2) + I(Brake.09miles^3)\n                        + Brake.11miles + I(Brake.11miles^2) + I(Brake.11miles^3)\n                        + Brake.12miles + I(Brake.12miles^2) + I(Brake.12miles^3)\n                        + Brake.14miles + I(Brake.14miles^2) + I(Brake.14miles^3)\n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$NB_Claim\noffset <- log(train2$expo)\n\nlambda.min <- 3.981072e-05\nlambda.1se <- 0.001258925\n    \nlambda.select <- lambda.min\nfit <- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\n#fit <- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n\ntrain2$pred.tele <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)\n\nmatrix.x <- model.matrix(glm.score, data=test2)[,-1]\ny <- test2$NB_Claim\noffset <- log(test2$expo)\n\ntest2$pred.base <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)\n\nResult_ <- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ <- cbind('LASSO (optimal)', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n```\n\n\n### Parsimonious model\n\nInstead of using the optimal value of the penalty $\\lambda$  in the elastic-net approach, it is often advised to use a penalty value located at one standard error ($\\lambda_{1se}$). This helps to obtain a more parsimonious model. The prediction scores of such a model are displayed below.\n\n```{r}\n#| echo: true\n#| cache: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_GLMnet2\n#| tbl-cap: Prediction scores for the GLM-net model (alpha=1)\n\nResult_  <- data.frame()\nResult2_  <- data.frame()\nfor(i in 1:nb.fold) {\n    learn <- train2[train2$fold != i,]\n    valid <- train2[train2$fold == i,]\n    \n    matrix.x <- model.matrix(glm.score, data=learn)[,-1]\n    y <- learn$NB_Claim\n    offset <- log(learn$expo)\n\n    lambda.min <- 3.981072e-05\n    lambda.1se <- 0.001258925\n    \n    lambda.select <- lambda.1se\n    #fit <- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\n    fit <- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n  \n    matrix.x <- model.matrix(glm.score, data=valid)[,-1]\n    y <- valid$NB_Claim\n    offset <- log(valid$expo)\n\n    valid$pred <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)\n    \n    Result_ <- rbind(Result_, c(i, Score.pred(valid$pred, valid$NB_Claim)/nrow(valid)))\n    Result2_ <- rbind(Result2_, c(i, Score.pred(valid$pred, valid$NB_Claim)))\n}\n\n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ncolnames(Result2_) <- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ntot <- colSums(Result2_)/nrow(train2)\ntot$Fold <- 'Total'\nResult_ <- rbind(Result_ , tot)\nResult_ <- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] <- 'Improvement'\n\nfor(i in 2:7){\n  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) <- NULL\nknitr::kable(Result_, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n```\n\nOn the *test* set, we obtain:\n\n```{r}\n#| echo: true\n#| cache: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_basetest4\n#| tbl-cap: Prediction scores for the GLM-net model  (testing set) \n\nglm.score <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3)\n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + log(max.day) \n                        + min.day + I(min.day^2)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Accel.06miles + I(Accel.06miles^2) + I(Accel.06miles^3)\n                        + Accel.08miles + I(Accel.08miles^2) + I(Accel.08miles^3)\n                        + Accel.09miles + I(Accel.09miles^2) + I(Accel.09miles^3)\n                        + Accel.11miles + I(Accel.11miles^2) + I(Accel.11miles^3)\n                        + Accel.12miles + I(Accel.12miles^2) + I(Accel.12miles^3)\n                        + Accel.14miles + I(Accel.14miles^2) + I(Accel.14miles^3)\n                        + Brake.06miles + I(Brake.06miles^2) + I(Brake.06miles^3)\n                        + Brake.08miles + I(Brake.08miles^2) + I(Brake.08miles^3)\n                        + Brake.09miles + I(Brake.09miles^2) + I(Brake.09miles^3)\n                        + Brake.11miles + I(Brake.11miles^2) + I(Brake.11miles^3)\n                        + Brake.12miles + I(Brake.12miles^2) + I(Brake.12miles^3)\n                        + Brake.14miles + I(Brake.14miles^2) + I(Brake.14miles^3)\n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$NB_Claim\noffset <- log(train2$expo)\n\nlambda.min <- 3.981072e-05\nlambda.1se <- 0.001258925\n    \nlambda.select <- lambda.1se\n#fit <- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\nfit <- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n  \nmatrix.x <- model.matrix(glm.score, data=test2)[,-1]\ny <- test2$NB_Claim\noffset <- log(test2$expo)\n\ntest2$pred.base <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)\n\nResult_ <- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ <- cbind('LASSO (parsimonious)', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n```\n\n### RESIDUALS AND PROTECTED VARIABLES\n\nComparing the results obtained with the pricing model based solely on traditional variables, we observe that the addition of telematic variables improves prediction quality. However, we can verify whether the protected variables we excluded from the analysis still retain predictive capability. Thus, we fit a GLM-net model on telematic data and predict the expected frequency of the model on the training dataset.  \n\nUsing the prediction as an offset variable, we can effectively assess whether the protected variables still capture a trend. The reliability of this approach is further bolstered by the comparison of the results obtained with the graphs we had in Chapter 2. If a curve is horizontal and close to the value of 1 for all possible values of a covariate, it indicates that telematic variables have indeed captured the predictive capability of the variable to predict claim frequency, as demonstrated in the graphs below.\n\nIn the graphs below, we observe that the addition of telematic variables:  \n\n- diminishes the effect of credit score, but it still appears to be predictive;  \n- almost eliminates the effect of driver age;  \n- the addition of telematic variables has led to a significant shift in the effect of driver gender. Originally, a premium reduction for males seemed necessary, but adding telematic variables suggest that males should now have a surcharge;\n- significantly reduces the effect of marital status;  \n- slightly diminishes the effect of territory without eliminating it.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| layout-ncol: 2\n#| label: fig-CreditScore_sev\n#| fig-cap: \"Observed Relativity vs. Residuals Relativity\"\n#| fig-subcap: \n#|   - \"Credit Score\"\n#|   - \"Age of the Insured\"\n#|   - \"Sex of the Insured\"\n#|   - \"Marital Status of the Insured\"\n#|   - \"Territory\"\n#| fig-width: 9\n#| fig-height: 4\n\n\nmeaninv  <- sum(train2$expo)/sum(train2$NB_Claim)\n\ntemp2 <- train2 %>%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Insured.age = pmin(Insured.age, 80),\n                Group = ceiling(Credit.score/25) * 25) %>%\n  group_by(Group) %>% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(expo),\n            expo2=sum(pred.tele)) %>% \n  mutate(freq = meaninv*NB_Claim/expo, \n         freq2 =  NB_Claim/expo2)\n\nggplot() + \n  geom_smooth(aes(x=Group, y=freq, weight = expo, color='Observed'),se=F, size=1, data=temp2) + \n  geom_smooth(aes(x=Group, y=freq2, weight = expo, color='Residuals'),se=F, size=1, data=temp2) + \n  labs(x = 'Credit Score',\n       y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  guides(color = guide_legend(title = \"\")) +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n### Age of the insured\n\ntemp2 <- train2 %>%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Insured.age = pmin(Insured.age, 80),\n                Group = ceiling(Insured.age/5) * 5) %>%\n  group_by(Group) %>% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(expo),\n            expo2=sum(pred.tele)) %>% \n  mutate(freq = meaninv*NB_Claim/expo, \n         freq2 =  NB_Claim/expo2)\n\nggplot() + \n  geom_smooth(aes(x=Group, y=freq, weight = expo, color='Observed'),se=F, size=1, data=temp2) + \n  geom_smooth(aes(x=Group, y=freq2, weight = expo, color='Residuals'),se=F, size=1, data=temp2) + \n  labs(x = 'Age of the insured',\n       y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  guides(color = guide_legend(title = \"\")) +\n        theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n### Sex of the insured\n\ntemp <- train2 %>%\n  mutate(Var_ = Insured.sex) %>%\n  group_by(Var_) %>%\n  summarize(nbclaim = sum(NB_Claim),\n            expo = sum(expo),\n            expo2 = sum(pred.tele)) %>%\n  mutate(freq = nbclaim/expo,\n         freq2 = nbclaim/expo2)\n\ntemp$freq <- temp$freq/temp$freq[1]\ntemp$freq2 <- temp$freq2/temp$freq2[1]\n\nggplot() + #start plot by by plotting bars\n  geom_point(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=0.7) +\n  geom_point(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=0.7) +\n  labs(x = 'Sex of the insured', y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+\n  guides(color=guide_legend(title=\"\")) +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Marital Status\n\ntemp <- train2 %>%\n  mutate(Var_ = Marital) %>%\n  group_by(Var_) %>%\n  summarize(nbclaim = sum(NB_Claim),\n            expo = sum(expo),\n            expo2 = sum(pred.tele)) %>%\n  mutate(freq = nbclaim/expo, \n         freq2 = nbclaim/expo2)\n\ntemp$freq <- temp$freq/temp$freq[1]\ntemp$freq2 <- temp$freq2/temp$freq2[1]\n\nggplot() + #start plot by by plotting bars\n  geom_point(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=0.7) +\n  geom_point(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=0.7) +\n  labs(x = 'Marital status of the insured', y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  ylim(0.9, max(temp$freq, temp$freq2)*1.2)+\n  guides(color=guide_legend(title=\"\")) +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Insured's territory\n\ntemp <- train2 %>%\n  mutate(Var_ = Territory) %>%\n  group_by(Var_) %>%\n  summarize(nbclaim = sum(NB_Claim),\n            expo = sum(expo),\n            expo2 = sum(pred.tele)) %>%\n  mutate(freq1 = nbclaim/expo2, \n         freq2 = meaninv*nbclaim/expo)\n\nggplot() + \n  geom_line(data = temp, aes(x = Var_, y = freq1, group = 1, color='Residuals'), size=0.7) +\n  geom_point(data = temp, aes(x = Var_, y = freq1, group = 1, color='Residuals'), size=0.7) +\n  geom_line(data = temp, aes(x = Var_, y = freq2, group = 1, color='Observed'), size=0.7) +\n  geom_point(data = temp, aes(x = Var_, y = freq2, group = 1, color='Observed'), size=0.7) +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  labs(x = 'Territory',\n       y = 'Relativity') +\n  scale_x_discrete(labels = NULL, breaks = NULL)+\n  guides(color=guide_legend(title=\"\")) +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n```\n\n### GLM-NET ON RESIDUALS\n\nOne can directly fit a GLM-Net model by appropriately utilizing the initial predictions. By retaining only the protected variables in the GLM-Net approach while using the prediction as an offset variable, it is possible to verify whether adding protected variables enhances the model's predictive capacity.  \n\nThe tables below indicate the scores obtained for this new model (with the two approaches marked with an *). We observe a slight improvement in the model, indicating that using telematics data does not eliminate the need to use protected variables.\n\n```{r}\n#| echo: false\n#| eval: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\nglm.score <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3)\n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + log(max.day) \n                        + min.day + I(min.day^2)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Accel.06miles + I(Accel.06miles^2) + I(Accel.06miles^3)\n                        + Accel.08miles + I(Accel.08miles^2) + I(Accel.08miles^3)\n                        + Accel.09miles + I(Accel.09miles^2) + I(Accel.09miles^3)\n                        + Accel.11miles + I(Accel.11miles^2) + I(Accel.11miles^3)\n                        + Accel.12miles + I(Accel.12miles^2) + I(Accel.12miles^3)\n                        + Accel.14miles + I(Accel.14miles^2) + I(Accel.14miles^3)\n                        + Brake.06miles + I(Brake.06miles^2) + I(Brake.06miles^3)\n                        + Brake.08miles + I(Brake.08miles^2) + I(Brake.08miles^3)\n                        + Brake.09miles + I(Brake.09miles^2) + I(Brake.09miles^3)\n                        + Brake.11miles + I(Brake.11miles^2) + I(Brake.11miles^3)\n                        + Brake.12miles + I(Brake.12miles^2) + I(Brake.12miles^3)\n                        + Brake.14miles + I(Brake.14miles^2) + I(Brake.14miles^3)\n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$NB_Claim\noffset <- log(train2$expo)\n    \nlambda.min <- 3.981072e-05\nlasso.min <- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.min)\ntrain2$pred.tele <- predict(lasso.min, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.min)\n\n###\n\nvar.sens <- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\")     \nglm.score <- as.formula(NB_Claim ~ Insured.sex + Marital \n                                  + Credit.score +  I(Credit.score^2) \n                                  + Insured.age +  log(Insured.age) + I(Insured.age^2) \n                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$NB_Claim\noffset <- log(train2$pred.tele)\nfold.id <- train2$fold\n\nlambda_seq <- c(10^seq(0, -8, by = -.1), 0)\ncvfit0  <- cv.glmnet(matrix.x, y, relax=FALSE, family = \"poisson\", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0)\ncvfit.2 <- cv.glmnet(matrix.x, y, relax=FALSE, family = \"poisson\", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.2)\ncvfit.4 <- cv.glmnet(matrix.x, y, relax=FALSE, family = \"poisson\", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.4)\ncvfit.6 <- cv.glmnet(matrix.x, y, relax=FALSE, family = \"poisson\", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.6)\ncvfit.8 <- cv.glmnet(matrix.x, y, relax=FALSE, family = \"poisson\", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.8)\ncvfit1  <- cv.glmnet(matrix.x, y, relax=FALSE, family = \"poisson\", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 1)\n\nc(cvfit0$lambda.min, cvfit.2$lambda.min, cvfit.4$lambda.min, cvfit.6$lambda.min, cvfit.8$lambda.min, cvfit1$lambda.min)\n\nall.min <- data.frame(c(min(cvfit0$cvm), min(cvfit.2$cvm), min(cvfit.4$cvm), min(cvfit.6$cvm), min(cvfit.8$cvm), min(cvfit1$cvm))) %>%\n  mutate(alpha = 2*(row_number()-1)/10)\ncolnames(all.min)[1] <- 'min' \nall.min %>% filter(min == min(min))\n\ncvfit1$lambda.min\ncvfit1$lambda.1se\n\n\n```\n\n\n```{r}\n#| echo: true\n#| cache: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_basetest4b\n#| tbl-cap: Prediction scores for the GLM-net model  (testing set) \n\nglm.score <- as.formula(NB_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3)\n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + log(max.day) \n                        + min.day + I(min.day^2)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Accel.06miles + I(Accel.06miles^2) + I(Accel.06miles^3)\n                        + Accel.08miles + I(Accel.08miles^2) + I(Accel.08miles^3)\n                        + Accel.09miles + I(Accel.09miles^2) + I(Accel.09miles^3)\n                        + Accel.11miles + I(Accel.11miles^2) + I(Accel.11miles^3)\n                        + Accel.12miles + I(Accel.12miles^2) + I(Accel.12miles^3)\n                        + Accel.14miles + I(Accel.14miles^2) + I(Accel.14miles^3)\n                        + Brake.06miles + I(Brake.06miles^2) + I(Brake.06miles^3)\n                        + Brake.08miles + I(Brake.08miles^2) + I(Brake.08miles^3)\n                        + Brake.09miles + I(Brake.09miles^2) + I(Brake.09miles^3)\n                        + Brake.11miles + I(Brake.11miles^2) + I(Brake.11miles^3)\n                        + Brake.12miles + I(Brake.12miles^2) + I(Brake.12miles^3)\n                        + Brake.14miles + I(Brake.14miles^2) + I(Brake.14miles^3)\n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$NB_Claim\noffset <- log(train2$expo)\n    \nlambda.min <- 3.981072e-05\nlasso.min <- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.min)\ntrain2$pred.tele <- predict(lasso.min, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.min)\n\nmatrix.x <- model.matrix(glm.score, data=test2)[,-1]\ny <- test2$NB_Claim\noffset <- log(test2$expo)\ntest2$pred.tele <- predict(lasso.min, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.min)\n\nglm.score <- as.formula(NB_Claim ~ Insured.sex + Marital \n                                  + Credit.score +  I(Credit.score^2) \n                                  + Insured.age +  log(Insured.age) + I(Insured.age^2) \n                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$NB_Claim\noffset <- log(train2$pred.tele)\n\nlambda.min <- 1e-08\nlambda.1se <- 0.007943282\n    \nlambda.select <- lambda.min\n#fit <- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\nfit <- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n  \nmatrix.x <- model.matrix(glm.score, data=test2)[,-1]\ny <- test2$NB_Claim\noffset <- log(test2$pred.tele)\n\ntest2$pred.base <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)\n\nResult_ <- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ <- cbind('LASSO* (optimal)', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all <- rbind(Result_all, Result_)\n\n###\n\nglm.score <- as.formula(NB_Claim ~ Insured.sex + Marital \n                                  + Credit.score +  I(Credit.score^2) \n                                  + Insured.age +  log(Insured.age) + I(Insured.age^2) \n                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$NB_Claim\noffset <- log(train2$pred.tele)\n\nlambda.min <- 1e-08\nlambda.1se <- 0.007943282\n    \nlambda.select <- lambda.1se\nfit <- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\n#fit <- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n  \nmatrix.x <- model.matrix(glm.score, data=test2)[,-1]\ny <- test2$NB_Claim\noffset <- log(test2$pred.tele)\n\ntest2$pred.base <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)\n\nResult_ <- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ <- cbind('LASSO* (parsimonious)', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n```\n\n\n:::\n\n## XGBoost\n\nWe now consider an XGBoost model. As with the frequency model, hyperparameter values are obtained by performing a Bayesian search on a grid of possible values.\n\n```{r}\n#| echo: false\n#| eval: false\n#| message: FALSE\n#| warning: FALSE\n\nlibrary(xgboost)\nlibrary(Ckmeans.1d.dp)\nlibrary(SHAPforxgboost)\nlibrary(pacman)\n\n# p_load automatically installs packages if needed\np_load(xgboost, ParBayesianOptimization, mlbench, dplyr, skimr, recipes, resample)\n\nall.vars2 <- c(\"Car.use\", \"Region\", \"Car.age\", \"Years.noclaims\",\n               \"Miles.per.day\", \"Avgdays.week\",\n               \"Pct.drive.mon\", \"Pct.drive.tue\", \"Pct.drive.wed\", \"Pct.drive.thr\", \"Pct.drive.fri\", \"Pct.drive.sat\", \"Pct.drive.sun\",\n               \"max.day\", \"min.day\", \"max.min\", \"Dayformax\", \"Dayformin\",\n               \"Pct.drive.rush.am\", \"Pct.drive.rush.pm\",\n               \"Pct.drive.wkend\",\n               \"Pct.drive.2hrs\", \"Pct.drive.3hrs\", \"Pct.drive.4hrs\",\n               \"Accel.06miles\", \"Accel.08miles\", \"Accel.09miles\", \"Accel.11miles\", \"Accel.12miles\", \"Accel.14miles\", \n               \"Brake.06miles\", \"Brake.08miles\", \"Brake.09miles\", \"Brake.11miles\", \"Brake.12miles\", \"Brake.14miles\", \n               \"Left.turn.intensity08\", \"Left.turn.intensity09\", \"Left.turn.intensity10\", \"Left.turn.intensity11\", \"Left.turn.intensity12\",\n               \"Right.turn.intensity08\", \"Right.turn.intensity09\", \"Right.turn.intensity10\", \"Right.turn.intensity11\", \"Right.turn.intensity12\")\n\ndtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(all.vars2)]), label = train2$NB_Claim)\nsetinfo(dtrain,\"base_margin\",log(train2$expo))\nfolds <-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n\nbounds <- list(eta = c(0.001, 0.5),\n               max_depth = c(1L, 50L),\n               subsample = c(0.1, 1),\n               min_child_weight = c(1, 175))\n\nobj_func <- function(eta, max_depth, subsample, min_child_weight) {\n  param <- list(\n    eta = eta,\n    max_depth = max_depth,\n    subsample = subsample,\n    min_child_weight = min_child_weight,\n    booster = \"gbtree\",\n    objective = \"count:poisson\",\n    eval_metric = \"poisson-nloglik\")\n  \n  set.seed(533)\n  xgbcv <- xgb.cv(params = param,\n                  nrounds = base.rounds,\n                  data = dtrain,\n                  folds = folds,\n                  prediction = TRUE,\n                  early_stopping_rounds = 10,\n                  verbose = 0,\n                  maximize = F)\n  \n  lst <- list(\n    Score = -min(xgbcv$evaluation_log$test_poisson_nloglik_mean),\n    nrounds = xgbcv$best_iteration\n  )\n  \n  return(lst)\n}\n\nbase.rounds <- 500\nset.seed(254)\nbayes_out <- bayesOpt(FUN = obj_func, bounds = bounds, initPoints = length(bounds) + 2, iters.n = 5)\ncomp <- bayes_out$scoreSummary[which(bayes_out$scoreSummary$Score== max(bayes_out$scoreSummary$Score))]\ncomp \n\n#Epoch Iteration        eta max_depth subsample min_child_weight gpUtility acqOptimum inBounds Elapsed      Score nrounds errorMessage\n#1:     0         4 0.02337437        26 0.8097923         123.6033        NA      FALSE     TRUE  735.83 -0.1325545     367           NA\n\n############\n## Verif ###\n############\nobj_func(eta=comp$eta, max_depth=comp$max_depth, subsample=comp$subsample, min_child_weight=comp$min_child_weight)\n\nparam <- list(\n  eta = 0.02337437,\n  max_depth = 26,\n  subsample = 0.8097923,\n  min_child_weight = 1,\n  booster = \"gbtree\",\n  objective = \"count:poisson\",\n  eval_metric = \"poisson-nloglik\")\n\nset.seed(133)\nxgbcv <- xgb.cv(params = param,\n                nrounds = 367,\n                data = dtrain,\n                folds = folds,\n                prediction = TRUE,\n                early_stopping_rounds = 10,\n                verbose = 0,\n                maximize = F)\n\n-min(xgbcv$evaluation_log$test_poisson_nloglik_mean)\n\n```\n\n\n::: {.panel-tabset}\n\n### Prediction Scores\n\nWe can calculate the models prediction scores based on all classical and telematics covariates. The XGBoost approach is particularly effective in capturing the effect of all available telematic covariates. Indeed, the scores obtained are significantly improved compared to other tested approaches.\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| output: false\n\nlibrary(xgboost)\nlibrary(Ckmeans.1d.dp)\nlibrary(SHAPforxgboost)\n\nall.vars2 <- c(\"Car.use\", \"Region\", \"Car.age\", \"Years.noclaims\",\n               \"Miles.per.day\", \"Avgdays.week\",\n               \"Pct.drive.mon\", \"Pct.drive.tue\", \"Pct.drive.wed\", \"Pct.drive.thr\", \"Pct.drive.fri\", \"Pct.drive.sat\", \"Pct.drive.sun\",\n               \"max.day\", \"min.day\", \"max.min\", \"Dayformax\", \"Dayformin\",\n               \"Pct.drive.rush.am\", \"Pct.drive.rush.pm\",\n               \"Pct.drive.wkend\",\n               \"Pct.drive.2hrs\", \"Pct.drive.3hrs\", \"Pct.drive.4hrs\",\n               \"Accel.06miles\", \"Accel.08miles\", \"Accel.09miles\", \"Accel.11miles\", \"Accel.12miles\", \"Accel.14miles\", \n               \"Brake.06miles\", \"Brake.08miles\", \"Brake.09miles\", \"Brake.11miles\", \"Brake.12miles\", \"Brake.14miles\", \n               \"Left.turn.intensity08\", \"Left.turn.intensity09\", \"Left.turn.intensity10\", \"Left.turn.intensity11\", \"Left.turn.intensity12\",\n               \"Right.turn.intensity08\", \"Right.turn.intensity09\", \"Right.turn.intensity10\", \"Right.turn.intensity11\", \"Right.turn.intensity12\")\n\ndtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(all.vars2)]), label = train2$NB_Claim)\nsetinfo(dtrain,\"base_margin\",log(train2$expo))\nfolds <-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n\n\n```\n\n\n```{r}\n#| echo: true\n#| cache: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_XGBoosttele\n#| tbl-cap: Prediction scores for the XGBoost model with telematics\n\nparam <- list(\n  eta = 0.02337437,\n  max_depth = 26,\n  subsample = 0.8097923,\n  min_child_weight = 1,\n  booster = \"gbtree\",\n  objective = \"count:poisson\",\n  eval_metric = \"poisson-nloglik\")\n\nset.seed(133)\nxgbcv <- xgb.cv(params = param,\n                nrounds = 367,\n                data = dtrain,\n                folds = folds,\n                prediction = TRUE,\n                early_stopping_rounds = 10,\n                verbose = 0,\n                maximize = F)\n  \nSc.log <- sapply(xgbcv$folds, function(x){-dpois(train2$NB_Claim[x], unlist(xgbcv$pred[x]), log=TRUE)})\nSc.MSE <- sapply(xgbcv$folds, function(x){(train2$NB_Claim[x]-unlist(xgbcv$pred[x]))^2})\nSc.quad <- sapply(xgbcv$folds, function(x){\n  nb <- train2$NB_Claim[x]\n  mu <- unlist(xgbcv$pred[x])\n  -2*dpois(nb,lambda=mu) + dpois(0,lambda=mu)^2 + dpois(1,lambda=mu)^2 + dpois(2,lambda=mu)^2+ dpois(3,lambda=mu)^2 + dpois(4,lambda=mu)^2 + dpois(5,lambda=mu)^2 \n})  \nSc.sph <- sapply(xgbcv$folds, function(x){\n  nb <- train2$NB_Claim[x]\n  mu <- unlist(xgbcv$pred[x])\n  -dpois(nb,lambda=mu) / sqrt(dpois(0,lambda=mu)^2 + dpois(1,lambda=mu)^2 + dpois(2,lambda=mu)^2+ dpois(3,lambda=mu)^2 + dpois(4,lambda=mu)^2 + dpois(5,lambda=mu)^2 ) \n})\nSc.DSS <- sapply(xgbcv$folds, function(x){dss_pois(train2$NB_Claim[x], unlist(xgbcv$pred[x]))})\nSc.CRPS <- sapply(xgbcv$folds, function(x){crps_pois(train2$NB_Claim[x], unlist(xgbcv$pred[x]))})\n\nResult_  <- rbind(\nc(1,mean(Sc.log[1]$fold1), mean(Sc.MSE[1]$fold1), mean(Sc.quad[1]$fold1), mean(Sc.sph[1]$fold1), mean(Sc.DSS[1]$fold1), mean(Sc.CRPS[1]$fold1)),\nc(2,mean(Sc.log[2]$fold2), mean(Sc.MSE[2]$fold2), mean(Sc.quad[2]$fold2), mean(Sc.sph[2]$fold2), mean(Sc.DSS[2]$fold2), mean(Sc.CRPS[2]$fold2)),\nc(3,mean(Sc.log[3]$fold3), mean(Sc.MSE[3]$fold3), mean(Sc.quad[3]$fold3), mean(Sc.sph[3]$fold3), mean(Sc.DSS[3]$fold3), mean(Sc.CRPS[3]$fold3)),\nc(4,mean(Sc.log[4]$fold4), mean(Sc.MSE[4]$fold4), mean(Sc.quad[4]$fold4), mean(Sc.sph[4]$fold4), mean(Sc.DSS[4]$fold4), mean(Sc.CRPS[4]$fold4)),\nc(5,mean(Sc.log[5]$fold5), mean(Sc.MSE[5]$fold5), mean(Sc.quad[5]$fold5), mean(Sc.sph[5]$fold5), mean(Sc.DSS[5]$fold5), mean(Sc.CRPS[5]$fold5))\n)\n\nRes.sum  <- rbind(\nc(sum(Sc.log[1]$fold1), sum(Sc.MSE[1]$fold1), sum(Sc.quad[1]$fold1), sum(Sc.sph[1]$fold1), sum(Sc.DSS[1]$fold1), sum(Sc.CRPS[1]$fold1)),\nc(sum(Sc.log[2]$fold2), sum(Sc.MSE[2]$fold2), sum(Sc.quad[2]$fold2), sum(Sc.sph[2]$fold2), sum(Sc.DSS[2]$fold2), sum(Sc.CRPS[2]$fold2)),\nc(sum(Sc.log[3]$fold3), sum(Sc.MSE[3]$fold3), sum(Sc.quad[3]$fold3), sum(Sc.sph[3]$fold3), sum(Sc.DSS[3]$fold3), sum(Sc.CRPS[3]$fold3)),\nc(sum(Sc.log[4]$fold4), sum(Sc.MSE[4]$fold4), sum(Sc.quad[4]$fold4), sum(Sc.sph[4]$fold4), sum(Sc.DSS[4]$fold4), sum(Sc.CRPS[4]$fold4)),\nc(sum(Sc.log[5]$fold5), sum(Sc.MSE[5]$fold5), sum(Sc.quad[5]$fold5), sum(Sc.sph[5]$fold5), sum(Sc.DSS[5]$fold5), sum(Sc.CRPS[5]$fold5))\n)\nsum <- c('Total', colSums(Res.sum)/nrow(train2))\n\nResult_  <- data.frame(rbind(Result_, sum)) \n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\nResult_ <- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] <- 'Improvement'\n\nfor(i in 2:7){\n  Result_[,i] <- as.numeric(Result_[,i])  \n  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\nrownames(Result_) <- NULL\nknitr::kable(Result_, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n```\n\nOn the *test* set, we obtain:\n\n```{r}\n#| echo: true\n#| cache: false\n#| message: FALSE\n#| warning: FALSE\n#| output: false\n#| code-fold: true\n\ndtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(all.vars2)]), label = train2$NB_Claim)\nsetinfo(dtrain,\"base_margin\",log(train2$expo))\ndtest <- xgb.DMatrix(data = data.matrix(test2[, paste(all.vars2)]), label = test2$NB_Claim)\nsetinfo(dtest,\"base_margin\",log(test2$expo))\nfolds <-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n```\n\n\n```{r}\n#| echo: true\n#| cache: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_XGBoost_correction333\n#| tbl-cap: Prediction scores for the XGBoost model with telematics\n\nparam <- list(\n  eta = 0.02337437,\n  max_depth = 26,\n  subsample = 0.8097923,\n  min_child_weight = 1,\n  booster = \"gbtree\",\n  objective = \"count:poisson\",\n  eval_metric = \"poisson-nloglik\")\n\nset.seed(133)\nfit.xgb <- xgb.train(params = param,\n                     nrounds = 367,\n                     data = dtrain)\n\ntrain2$pred.xgb <- predict(fit.xgb, dtrain, type='response')\n\n\ntest2$pred.xgb <- predict(fit.xgb, dtest, type='response')\n\ntest2$pred.base <- test2$pred.xgb\n\nResult_ <- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ <- cbind('XGBoost', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n```\n\n### Variables Importance\n\nThe graph below illustrates the most critical variables in the XGBoost model for claim frequency. As indicated in the literature, the level of vehicle usage, represented here by the daily mileage, is the most crucial variable in the model. Driving experience, measured by the number of claim-free years, follows. To a lesser extent, a series of telematic variables also has predictive capability.\n\n```{r}\n#| echo: true\n#| cache: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\nimportance_matrix <- xgb.importance(dimnames(dtrain)[[2]], model = fit.xgb)\nxgb.ggplot.importance(importance_matrix,top_n=15) + theme(text = element_text(size=15))\n\n\n```\n\n### RESIDUALS AND PROTECTED VARIABLES\n\nAs we did for the GLM-Net approach, we can again analyze the residuals of the approach to see if the addition of telematic data eliminates the need to use protected variables. The graphs below show that XGBoost is even more effective than GLM-Net. Indeed, although the credit score still appears useful in modeling frequency, its effect is greatly diminished. The effects of age, gender, and marital status of the insured are also significantly reduced. Finally, we can even see that the effect of territory is also greatly minimized.\n\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| layout-ncol: 2\n#| label: fig-CreditScore_sev2\n#| fig-cap: \"Observed Relativity vs. Residuals Relativity\"\n#| fig-subcap: \n#|   - \"Credit Score\"\n#|   - \"Age of the Insured\"\n#|   - \"Sex of the Insured\"\n#|   - \"Marital Status of the Insured\"\n#|   - \"Territory\"\n#| fig-width: 9\n#| fig-height: 4\n\nmeaninv  <- sum(train2$expo)/sum(train2$NB_Claim)\nmoy.xgb <- sum(train2$pred.xgb)/sum(train2$NB_Claim)\n\ntemp2 <- train2 %>%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Insured.age = pmin(Insured.age, 80),\n                Group = ceiling(Credit.score/25) * 25) %>%\n  group_by(Group) %>% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(expo),\n            expo2=sum(pred.xgb)) %>% \n  mutate(freq = meaninv*NB_Claim/expo, \n         freq2 =  moy.xgb*NB_Claim/expo2)\n\nGraph_resCS <- ggplot() + \n  geom_smooth(aes(x=Group, y=freq, weight = expo, color='Observed'),se=F, size=1, data=temp2) + \n  geom_smooth(aes(x=Group, y=freq2, weight = expo, color='Residuals'),se=F, size=1, data=temp2) + \n  labs(x = 'Credit Score',\n       y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  guides(color = guide_legend(title = \"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\nprint(Graph_resCS)\nsave(Graph_resCS, file = \"Data/Graph_resCS.rdata\")\n\n### Age of the insured\n\ntemp2 <- train2 %>%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Insured.age = pmin(Insured.age, 80), \n                Group = ceiling(Insured.age/5) * 5) %>%\n  group_by(Group) %>% \n  summarize(NB_Claim=sum(NB_Claim),\n            expo=sum(expo),\n            expo2=sum(pred.xgb)) %>% \n  mutate(freq = meaninv*NB_Claim/expo, \n         freq2 =  moy.xgb*NB_Claim/expo2)\n\nGraph_resAge <- ggplot() + \n  geom_smooth(aes(x=Group, y=freq, weight = expo, color='Observed'),se=F, size=1, data=temp2) + \n  geom_smooth(aes(x=Group, y=freq2, weight = expo, color='Residuals'),se=F, size=1, data=temp2) + \n  labs(x = 'Age of the insured',\n       y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  guides(color = guide_legend(title = \"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\nprint(Graph_resAge)\nsave(Graph_resAge, file = \"Data/Graph_resAge.rdata\")\n\n### Sex of the insured\n\n\ntemp <- train2 %>%\n  mutate(Var_ = Insured.sex) %>%\n  group_by(Var_) %>%\n  summarize(nbclaim = sum(NB_Claim),\n            expo = sum(expo),\n            expo2 = sum(pred.xgb)) %>%\n  mutate(freq = nbclaim/expo,\n         freq2 = nbclaim/expo2)\n\ntemp$freq <- temp$freq/temp$freq[1]\ntemp$freq2 <- temp$freq2/temp$freq2[1]\n\nggplot() + #start plot by by plotting bars\n  geom_point(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=0.7) +\n  geom_point(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=0.7) +\n  labs(x = 'Sex of the insured', y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+\n  guides(color=guide_legend(title=\"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n### Marital Status\n\n\ntemp <- train2 %>%\n  mutate(Var_ = Marital) %>%\n  group_by(Var_) %>%\n  summarize(nbclaim = sum(NB_Claim),\n            expo = sum(expo),\n            expo2 = sum(pred.xgb)) %>%\n  mutate(freq = nbclaim/expo, \n         freq2 = nbclaim/expo2)\n\ntemp$freq <- temp$freq/temp$freq[1]\ntemp$freq2 <- temp$freq2/temp$freq2[1]\n\nggplot() + #start plot by by plotting bars\n  geom_point(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (freq2), group = 1, color='Residuals'), size=0.7) +\n  geom_point(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (freq), group = 1, color='Observed'), size=0.7) +\n  labs(x = 'Marital status of the insured', y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  ylim(0.9, max(temp$freq, temp$freq2)*1.2)+\n  guides(color=guide_legend(title=\"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Insured's territory\n\ntemp <- train2 %>%\n  mutate(Var_ = Territory) %>%\n  group_by(Var_) %>%\n  summarize(nbclaim = sum(NB_Claim),\n            expo = sum(expo),\n            expo2 = sum(pred.xgb)) %>%\n  mutate(freq = moy.xgb*nbclaim/expo2)\n\ntemp2 <- train2 %>%\n  mutate(Var_ = Territory) %>%\n  group_by(Var_) %>%\n  summarize(nbclaim = sum(NB_Claim),\n            expo = sum(expo)) %>%\n  mutate(freq = meaninv*nbclaim/expo)\n\nGraph_resTerr <- ggplot() + \n  geom_line(data = temp, aes(x = Var_, y = freq, group = 1, color='Residuals'), size=0.7) +\n  geom_point(data = temp, aes(x = Var_, y = freq, group = 1, color='Residuals'), size=0.7) +\n  geom_line(data = temp2, aes(x = Var_, y = freq, group = 1, color='Observed'), size=0.7) +\n  geom_point(data = temp2, aes(x = Var_, y = freq, group = 1, color='Observed'), size=0.7) +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  labs(x = 'Territory',\n       y = 'Relativity') +\n  scale_x_discrete(labels = NULL, breaks = NULL)+\n  guides(color=guide_legend(title=\"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\nprint(Graph_resTerr)\nsave(Graph_resTerr, file = \"Data/Graph_resTerr.rdata\")\n\n\n```\n\n\n\n\n### XGBOOST ON RESIDUALS\n\nWe repeat the same exercise we did with the GLM-Net approach: fitting an XGBoost model on the residuals of the first XGBoost model. This will allow us to see if protected variables are capable of capturing trends in the approach's residuals.\n\nThe table below shows the different scores for the XGBoost* model. There is only a gain on some of the indicated scores. Hence, telematics data can substitute protected variables based on the studied dataset.\n\n```{r}\n#| echo: false\n#| eval: false\n#| message: FALSE\n#| warning: FALSE\n\nlibrary(xgboost)\nlibrary(Ckmeans.1d.dp)\nlibrary(SHAPforxgboost)\nlibrary(pacman)\n\n# p_load automatically installs packages if needed\np_load(xgboost, ParBayesianOptimization, mlbench, dplyr, skimr, recipes, resample)\n\nvar.sens <- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\")    \n\ndtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(var.sens)]), label = train2$NB_Claim)\nsetinfo(dtrain,\"base_margin\",log(train2$pred.xgb))\nfolds <-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n\nbounds <- list(eta = c(0.001, 0.5),\n               max_depth = c(1L, 50L),\n               subsample = c(0.1, 1),\n               min_child_weight = c(1, 175))\n\nobj_func <- function(eta, max_depth, subsample, min_child_weight) {\n  param <- list(\n    eta = eta,\n    max_depth = max_depth,\n    subsample = subsample,\n    min_child_weight = min_child_weight,\n    booster = \"gbtree\",\n    objective = \"count:poisson\",\n    eval_metric = \"poisson-nloglik\")\n  \n  set.seed(533)\n  xgbcv <- xgb.cv(params = param,\n                  nrounds = base.rounds,\n                  data = dtrain,\n                  folds = folds,\n                  prediction = TRUE,\n                  early_stopping_rounds = 10,\n                  verbose = 0,\n                  maximize = F)\n  \n  lst <- list(\n    Score = -min(xgbcv$evaluation_log$test_poisson_nloglik_mean),\n    nrounds = xgbcv$best_iteration\n  )\n  \n  return(lst)\n}\n\nbase.rounds <- 500\nset.seed(254)\nbayes_out <- bayesOpt(FUN = obj_func, bounds = bounds, initPoints = length(bounds) + 2, iters.n = 5)\ncomp <- bayes_out$scoreSummary[which(bayes_out$scoreSummary$Score== max(bayes_out$scoreSummary$Score))]\ncomp \n\n#   Epoch Iteration       eta max_depth subsample min_child_weight gpUtility acqOptimum inBounds Elapsed       Score nrounds errorMessage\n#1:    10        16 0.2215803        50 0.7623535                1 0.7185566       TRUE     TRUE    7.14 -0.06200109       5           NA\n\n############\n## Verif ###\n############\nobj_func(eta=comp$eta, max_depth=comp$max_depth, subsample=comp$subsample, min_child_weight=comp$min_child_weight)\n\nparam <- list(\n  eta = 0.2215803,\n  max_depth = 50,\n  subsample = 0.7623535,\n  min_child_weight = 1,\n  booster = \"gbtree\",\n  objective = \"count:poisson\",\n  eval_metric = \"poisson-nloglik\")\n\nset.seed(533)\nxgbcv <- xgb.cv(params = param,\n                nrounds = 5,\n                data = dtrain,\n                folds = folds,\n                prediction = TRUE,\n                early_stopping_rounds = 10,\n                verbose = 0,\n                maximize = F)\n\n-min(xgbcv$evaluation_log$test_poisson_nloglik_mean)\n\n```\n\n\n\n\n```{r}\n#| echo: true\n#| cache: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| output: false\n\nlibrary(xgboost)\nlibrary(Ckmeans.1d.dp)\nlibrary(SHAPforxgboost)\nlibrary(pacman)\n\nvar.sens <- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\")    \n\ndtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(var.sens)]), label = train2$NB_Claim)\nsetinfo(dtrain,\"base_margin\",log(train2$pred.xgb))\nfolds <-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n\nparam <- list(\n  eta = 0.2215803,\n  max_depth = 50,\n  subsample = 0.7623535,\n  min_child_weight = 1,\n  booster = \"gbtree\",\n  objective = \"count:poisson\",\n  eval_metric = \"poisson-nloglik\")\n\nset.seed(533)\nfit.xgb2 <- xgb.train(params = param,\n                      nrounds = 5,\n                      data = dtrain)\n\ndtest <- xgb.DMatrix(data = data.matrix(test2[, paste(var.sens)]), label = test2$NB_Claim)\nsetinfo(dtest,\"base_margin\",log(test2$pred.xgb))\n\n```\n\n\n```{r}\n#| echo: true\n#| cache: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_XGBoost_correction33\n#| tbl-cap: Prediction scores for the XGBoost model with telematics\n\ntest2$pred.base <- predict(fit.xgb2, dtest, type='response')\n\nResult_ <- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ <- cbind('XGBoost*', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all <- rbind(Result_all, Result_)\n\nsave(Result_all, file='Data/ResultsSynth.Rda')\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n```\n\nAlthough the XGBoost model on residuals does not yield significant gains, the graph below illustrates the most critical protected variables in the XGBoost model. It shows that the insured's credit score and territory are the most critical variables in this model.\n\n```{r}\n#| echo: true\n#| cache: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\nimportance_matrix <- xgb.importance(dimnames(dtrain)[[2]], model = fit.xgb2)\nxgb.ggplot.importance(importance_matrix,top_n=15) + theme(text = element_text(size=15))\n\n\n```\n\n\n\n:::\n\n\n\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","embed-resources":false,"css":["custom.css"],"toc":true,"output-file":"VarTelematiques.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","bibliography":["bibtex.bib"],"editor":"source","theme":"sandstone","fontsize":"1.0em","linestretch":1.4,"grid":{"sidebar-width":"250px","body-width":"1000px","margin-width":"250px","gutter-width":"1.5rem"}},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"VarTelematiques.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"block-headings":true,"bibliography":["bibtex.bib"],"editor":"source","documentclass":"scrreprt"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}