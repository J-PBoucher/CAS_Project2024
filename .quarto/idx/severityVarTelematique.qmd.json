{"title":"Telematic Covariates","markdown":{"headingText":"Telematic Covariates","containsRefs":false,"markdown":"\n## Preamble\n\n::: {.panel-tabset}\n\n### Chapter Objective\n\nWe continue our analysis of claim severity based on the available covariates in the database.  Compared to the previous chapter, we are now adding telematics variables to the exercise while removing protected variables. Thus, the following five covariates are excluded, for the moment, from the analysis:  \n\n   1) Credit.score,  \n   2) Insured age,  \n   3) Insured.sex,  \n   4) Marital, and  \n   5) Territory.\n\nWe use the same two main models as in the previous chapters, namely Generalized Linear Model (GLM) family, including elastic-net and XGBoost. For each model, the response variable is the average cost of a claim, given that at least one claim has occurred.  To analyze severities, we still use the same two scores used previously.   \n\n```{r}\n#| echo: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\ntrad.vars2 <- c(\"Car.use\", \"Region\", \"Car.age\", \"Years.noclaims\")\ntele.var <- c(\"Miles.per.day\", \"Avgdays.week\",\n              \"Pct.drive.mon\", \"Pct.drive.tue\", \"Pct.drive.wed\", \"Pct.drive.thr\", \"Pct.drive.fri\", \"Pct.drive.sat\", \"Pct.drive.sun\",\n              \"max.day\", \"min.day\", \"max.min\", \"Dayformax\", \"Dayformin\",\n              \"Pct.drive.rush.am\", \"Pct.drive.rush.pm\",\n              \"Pct.drive.wkend\",\n              \"Pct.drive.2hrs\", \"Pct.drive.3hrs\", \"Pct.drive.4hrs\",\n              \"Accel.06miles\", \"Accel.08miles\", \"Accel.09miles\", \"Accel.11miles\", \"Accel.12miles\", \"Accel.14miles\", \n              \"Brake.06miles\", \"Brake.08miles\", \"Brake.09miles\", \"Brake.11miles\", \"Brake.12miles\", \"Brake.14miles\", \n              \"Left.turn.intensity08\", \"Left.turn.intensity09\", \"Left.turn.intensity10\", \"Left.turn.intensity11\", \"Left.turn.intensity12\",\n              \"Right.turn.intensity08\", \"Right.turn.intensity09\", \"Right.turn.intensity10\", \"Right.turn.intensity11\", \"Right.turn.intensity12\")\n\n```\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\nScore.pred.sev <- function(mu, x, phi) {\n  Sc.log  <- -sum(dgamma(x, shape = 1/phi, scale = mu*phi, log=TRUE))\n  Sc.MSE  <- sum((x - mu)^2)/1000000\n  return(c(Sc.log, Sc.MSE))\n}\n\n\n```\n\n\n\n### Packages\n\nFor this chapter, we need the following packages:\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\nlibrary(tidyverse)\nlibrary(vtable)\nlibrary(rpart)\nlibrary(repr)\nlibrary(rpart.plot)\nlibrary(gam)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(glmnet)\nlibrary(scoringRules)\nlibrary(sjPlot)\n\n```\n\n### Data\n\nIn this chapter, we conduct analyses using the same data as in the previous chapters amd we use the same train/test division.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\ndataS <- read.csv('Data/Synthetic.csv')\n\ndata <- dataS[dataS$AMT_Claim > 0,]\ndata$M_Claim <- data$AMT_Claim/data$NB_Claim\n\n# Modifications \ndata <- data %>%\n  mutate(Territory = as.factor(Territory)) %>%\n  select(-c('Annual.pct.driven', 'Annual.miles.drive'))\n\ndata.select <- data\n\n# Train-test \nset.seed(123)\ntrain <- data.select %>% sample_frac(0.8, replace = FALSE)\ntest <- data.select %>% anti_join(train)\n\ntest <- test[-640,]\n\n```\n\n### Data Transformation\n\nAs we concluded at the end of our overview of the data, certain variables also need to be transformed.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\n# Modif data\ntrain2 <- train %>%\n  mutate(Miles.per.day = Total.miles.driven/Duration,\n         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),\n         expo = Duration/365.25)\n\ntransform.fct <- function(var){\n  df <- train2 %>% mutate(var_ = get(var)*Total.miles.driven/(1000*Duration))\n  q99 <- quantile(df$var_, 0.99)\n  df <- df %>% mutate(var_ = ifelse(var_ > q99, q99, var_))\n  #colnames(df)[ncol(df)] <- paste0(var, '_')\n  return(df)\n}\n\ntrain2 <- transform.fct(\"Brake.06miles\")\ntrain2 <- transform.fct(\"Brake.08miles\")\ntrain2 <- transform.fct(\"Brake.09miles\")\ntrain2 <- transform.fct(\"Brake.11miles\")\ntrain2 <- transform.fct(\"Brake.14miles\")\ntrain2 <- transform.fct(\"Accel.06miles\")\ntrain2 <- transform.fct(\"Accel.08miles\")\ntrain2 <- transform.fct(\"Accel.09miles\")\ntrain2 <- transform.fct(\"Accel.11miles\")\ntrain2 <- transform.fct(\"Accel.12miles\")\ntrain2 <- transform.fct(\"Accel.14miles\")\ntrain2 <- transform.fct(\"Left.turn.intensity08\")\ntrain2 <- transform.fct(\"Left.turn.intensity09\")\ntrain2 <- transform.fct(\"Left.turn.intensity10\")\ntrain2 <- transform.fct(\"Left.turn.intensity11\")\ntrain2 <- transform.fct(\"Left.turn.intensity12\")\ntrain2 <- transform.fct(\"Right.turn.intensity08\")\ntrain2 <- transform.fct(\"Right.turn.intensity09\")\ntrain2 <- transform.fct(\"Right.turn.intensity10\")\ntrain2 <- transform.fct(\"Right.turn.intensity11\")\ntrain2 <- transform.fct(\"Right.turn.intensity12\")\n\n# Create folds\nnb.fold <- 5\nfold <- sample(1:nb.fold, nrow(train2), replace = TRUE)\ntrain2$fold <- fold\n\n##\n\ntest2 <- test %>%\n  mutate(Miles.per.day = Total.miles.driven/Duration,\n         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),\n         expo = Duration/365.25)\n\ntransform.fct <- function(var){\n  df <- test2 %>% mutate(var_ = get(var)*Total.miles.driven/(1000*Duration))\n  q99 <- quantile(df$var_, 0.99)\n  df <- df %>% mutate(var_ = ifelse(var_ > q99, q99, var_))\n  #colnames(df)[ncol(df)] <- paste0(var, '_')\n  return(df)\n}\n\ntest2 <- transform.fct(\"Brake.06miles\")\ntest2 <- transform.fct(\"Brake.08miles\")\ntest2 <- transform.fct(\"Brake.09miles\")\ntest2 <- transform.fct(\"Brake.11miles\")\ntest2 <- transform.fct(\"Brake.14miles\")\ntest2 <- transform.fct(\"Accel.06miles\")\ntest2 <- transform.fct(\"Accel.08miles\")\ntest2 <- transform.fct(\"Accel.09miles\")\ntest2 <- transform.fct(\"Accel.11miles\")\ntest2 <- transform.fct(\"Accel.12miles\")\ntest2 <- transform.fct(\"Accel.14miles\")\ntest2 <- transform.fct(\"Left.turn.intensity08\")\ntest2 <- transform.fct(\"Left.turn.intensity09\")\ntest2 <- transform.fct(\"Left.turn.intensity10\")\ntest2 <- transform.fct(\"Left.turn.intensity11\")\ntest2 <- transform.fct(\"Left.turn.intensity12\")\ntest2 <- transform.fct(\"Right.turn.intensity08\")\ntest2 <- transform.fct(\"Right.turn.intensity09\")\ntest2 <- transform.fct(\"Right.turn.intensity10\")\ntest2 <- transform.fct(\"Right.turn.intensity11\")\ntest2 <- transform.fct(\"Right.turn.intensity12\")\n\n# Mean Encoding with White Noise pour les territoires\ncardi <- length(unique(train$Territory))\n\nenc.terr <- train2 %>%\n  group_by(Territory) %>%\n  summarize(freq = sum(NB_Claim)/sum(expo)) %>%\n  arrange(freq) %>%\n  mutate(terr.code= row_number()/(cardi+1)) %>%\n  select(Territory, terr.code)\n\ntrain2 <- train2 %>%\n  group_by(Territory) %>%\n  left_join(enc.terr, by='Territory') %>%\n  ungroup()\n\ntest2 <- test2 %>%\n  group_by(Territory) %>%\n  left_join(enc.terr, by='Territory') %>%\n  ungroup()\n\n\n```\n\n\n:::\n\n\n\n\n## Basic GLM Models\n\n::: {.panel-tabset}\n\n### Single intercept\n\nFor comparison, we use a baseline model corresponding to a GLM with an intercept that predicts only the mean severity multiplied by the observed frequency for each contract. We do not present the results based on the *train* dataset to avoid burdening the report unnecessarily. However, if necessary, uncomment the last portion of the code to produce the results table.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_base_sev_tel\n#| tbl-cap: Prediction scores for the base model (severity)\n\n## Model on each fold\nResult_  <- data.frame()\nResult2_  <- data.frame()\nfor(i in 1:nb.fold) {\n  learn <- train2[train2$fold != i,]\n  valid <- train2[train2$fold == i,]\n  \n  mean <- sum(learn$AMT_Claim)/sum(learn$NB_Claim) \n  variance <- sd(learn$AMT_Claim)^2\n  phi <- variance/mean(learn$AMT_Claim)^2\n  \n  learn$pred.base <- mean*learn$NB_Claim\n  valid$pred.base <- mean*valid$NB_Claim\n  \n  Result_ <- rbind(Result_, c(i, Score.pred.sev(valid$pred.base, valid$AMT_Claim, phi)/nrow(valid)))\n  Result2_ <- rbind(Result2_, c(i, Score.pred.sev(valid$pred.base, valid$AMT_Claim, phi)))\n}\n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ncolnames(Result2_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ntot <- colSums(Result2_)/nrow(train2)\ntot$Fold <- 'Total'\nResult_ <- rbind(Result_ , tot)\n\nResult.base <- Result_  \nBase <- Result.base[nb.fold+1,]\n\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n```\n\nThe model is, therefore, estimated on the entire *train* database, and the predictions are made on the *test* database, which was not used during the calibration phase.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_basetest_sev_tel\n#| tbl-cap: Prediction scores for the base model (testing set) (severity)\n\nmean <- sum(train2$AMT_Claim)/sum(train2$NB_Claim) \nvariance <- sd(train2$AMT_Claim)^2\nphi <- variance/mean(train2$AMT_Claim)^2 \n  \ntest2$pred.base <- mean*test2$NB_Claim\n\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('Base', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- Result_\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n```\n\n### Traditional covariates already used (without protected variables)\n\nWe construct a first GLM with only the following covariates:  \n\n  - Car.use,  \n  - Region,  \n  - Car.age, and\n  - Years.noclaims.  \n\nWe calculate the prediction scores of the model with all categorical covariates on the *train* and the *test* dataset. As expected, adding segmentation variables improves the prediction scores compared to the simple baseline model with only an intercept.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-TeleGLM2_sev\n#| tbl-cap: Prediction scores for the GLM model with traditional covariates (without protected variables) (severity)\n\n## Model \nscore.base <- as.formula(M_Claim ~ 1)\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2))\n## Model on each fold\nResult_  <- data.frame()\nResult2_  <- data.frame()\nfor(i in 1:nb.fold) {\n  learn <- train2[train2$fold != i,]\n  valid <- train2[train2$fold == i,]\n  glm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = learn)\n  \n  learn$pred.base <- predict(glm.fit, newdata=learn, type='response')*learn$NB_Claim\n  valid$pred.base <- predict(glm.fit, newdata=valid, type='response')*valid$NB_Claim\n  phi <- summary(glm.fit)$dispersion\n  \n  Result_ <- rbind(Result_, c(i, Score.pred.sev(valid$pred.base, valid$AMT_Claim, phi)/nrow(valid)))\n  Result2_ <- rbind(Result2_, c(i, Score.pred.sev(valid$pred.base, valid$AMT_Claim, phi)))\n}\n\n## Model on all data from train\nglm.base <- glm(score.base, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\ntrain2$pred.glm1 <- predict(glm.fit, newdata=train2, type='response')*train2$NB_Claim\nphi <- summary(glm.fit)$dispersion\nResult.glm1 <- Result_  \n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ncolnames(Result2_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ntot <- colSums(Result2_)/nrow(train2)\ntot$Fold <- 'Total'\nResult_ <- rbind(Result_ , tot)\nResult_ <- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] <- 'Improvement'\n\nfor(i in 2:3){\n  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) <- NULL\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n```\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_basetest2_sev\n#| tbl-cap: Prediction scores for the GLM model with traditional covariates (testing set) (severity)\n\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2))\n\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\ntest2$pred.base <- predict(glm.fit, newdata=test2, type='response')*test2$NB_Claim\nphi <- summary(glm.fit)$dispersion\n\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('GLM (trad.)', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n```\n\n\n### Estimated parameters\n\nThe table below shows the estimators obtained for the GLM-Gamma approach and compares them with the baseline model, which has only an intercept. We note that except for the variable \"Car.use\", the traditional variables do not seem significant in the model.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-TeleGLM1_sev\n#| tbl-cap: Estimated parameters for the GLM model with traditional covariates (without protected variables) (severity)\n\n## Model \nscore.base <- as.formula(M_Claim ~ 1)\n\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2))\n\n## Model on all data from train\nglm.base <- glm(score.base, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ntab_model(glm.base, glm.fit, transform = NULL)\n\n```\n\n\n:::\n\n  \n\n\n## GLM-Net\n\nFirst, we consider the GLM-net model. To make the approach as effective as possible, we need to adjust the continuous segmentation variables.\n\n### Parametric transformation of telematic covariates\n  \nAs we did in the other chapter, we first introduce an approach using the Generalized Additive Models (GAM) theory for all continuous variables. \nThis approach allows us to observe the general form of the covariate to explain the severity. A parametric form will then be proposed to achieve the best possible correspondence with the spline obtained by the GAM.\n\n::: {.panel-tabset}\n\n### Vehicle Usage level\n\nFor the two covariates related to usage level, the proposed parametric forms are as follows:\n\n\\begin{align*}\ns(Miles.per.day) \\approx& Miles.per.day + log(Miles.per.day)\\\\\ns(Avgdays.week) \\approx& Avgdays.week + Avgdays.week^2 + Avgdays.week^3\n\\end{align*}\n\nThe graphs below compare the fit of the parametric approach with that of the GAM model.\n\n```{r}\n#| echo: true\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| layout-ncol: 2\n#| label: fig-MpD_GAM_sev_tel\n#| fig-cap: \"Smoothing of Usage level covariates (severity)\"\n#| fig-subcap: \n#|   - \"Miles.per.day\"\n#|   - \"Avgdays.week\"\n\nmin_ <- min(train2$Miles.per.day) \nmax_ <- max(train2$Miles.per.day) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'Miles.per.day'\n\nq99 <- quantile(train2$Miles.per.day, 0.99)\n\ndb <- train2 %>%\n  select(-'Miles.per.day') %>%\n  dplyr::slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)\n                        + s(Miles.per.day))\n\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)\n                        + Miles.per.day + log(Miles.per.day))\n\ngam.fit <- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase <- db %>%\n  mutate(diff = abs(Miles.per.day - mean(train2$Miles.per.day))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=Miles.per.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=Miles.per.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Miles per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n##\n\nmin_ <- quantile(train2$Avgdays.week, 0.01) \nmax_ <- max(train2$Avgdays.week) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'Avgdays.week'\n\nq99 <- quantile(train2$Avgdays.week, 0.99)\n\ndb <- train2 %>%\n  select(-'Avgdays.week') %>%\n  dplyr::slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)\n                        + s(Avgdays.week))\n\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)\n                        + Avgdays.week + I(Avgdays.week^2) + I(Avgdays.week^3))\n\ngam.fit <- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase <- db %>%\n  mutate(diff = abs(Avgdays.week - mean(train2$Avgdays.week))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=Avgdays.week, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=Avgdays.week, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Avgdays.week',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n```\n\n### Type of vehicle usage\n\nSeveral covariates are available in the category *Type of vehicle usage*:  \n\n- We propose the same parametric form for all variants of the variable *Pct.drive.day* (Monday to Sunday);  \n- The same parametric form will also be proposed For *Pct.drive.rush.am*, *Pct.drive.rush.pm*, *Pct.drive.2hrs*, *Pct.drive.3hrs*, and *Pct.drive.4hrs*;  \n- The other 3 covariates have their own parametric form.\n\nWe then have:\n\n\\begin{align*}\ns(Pct.drive.day) &\\approx Pct.drive.day + Pct.drive.day^2 \\\\\ns(Pct.drive) &\\approx Pct.drive + \\sqrt{Pct.drive} \\\\\ns(max.day) &\\approx max.day + max.day^2 + max.day^3 \\\\\ns(min.day) &\\approx min.day + min.day^2 + min.day^3 \\\\\ns(max.min) &\\approx max.min + max.min^2 \n\\end{align*}\n\n\n```{r}\n#| echo: true\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| layout-ncol: 2\n#| layout-nrow: 3\n#| label: fig-MpD_GAM_sev_tel3\n#| fig-cap: \"Smoothing of Type of vehicle usage covariates (severity)\"\n#| fig-subcap: \n#|   - \"Pct.drive.mon\"\n#|   - \"Pct.drive.rush.am\"\n#|   - \"max.day\"\n#|   - \"min.day\"\n#|   - \"max.min\"\n\ntrain2$Pct.drive <- train2$Pct.drive.mon\n\nmin_ <- min(train2$Pct.drive) \nmax_ <- max(train2$Pct.drive) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'Pct.drive'\n\nq99 <- quantile(train2$Pct.drive, 0.99)\n\ndb <- train2 %>%\n  select(-'Pct.drive') %>%\n  dplyr::slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)\n                        + s(Pct.drive))\n\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)\n                        + Pct.drive + I(Pct.drive^2))\n\ngam.fit <- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase <- db %>%\n  mutate(diff = abs(Pct.drive - mean(train2$Pct.drive))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=Pct.drive, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=Pct.drive, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Pct.drive',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Rush\n\ntrain2$use.day <- train2$Pct.drive.rush.am\n\nmin_ <- min(train2$use.day) \nmax_ <- max(train2$use.day) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'use.day'\n\nq99 <- quantile(train2$use.day, 0.99)\n\ndb <- train2 %>%\n  select(-'use.day') %>%\n  dplyr::slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2) \n                        + s(use.day))\n\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  \n                        + use.day + I(use.day^0.5))\n\ngam.fit <- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase <- db %>%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Max.day\n\ntrain2$use.day <- train2$max.day\n\nmin_ <- min(train2$use.day) \nmax_ <- max(train2$use.day) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'use.day'\n\nq99 <- quantile(train2$use.day, 0.99)\n\ndb <- train2 %>%\n  select(-'use.day') %>%\n  dplyr::slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2) \n                        + s(use.day))\n\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  \n                        + use.day + I(use.day^2) + I(use.day^3) )\n\ngam.fit <- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase <- db %>%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, 1) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Min.day\n\ntrain2$use.day <- train2$min.day\n\nmin_ <- min(train2$use.day) \nmax_ <- max(train2$use.day) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'use.day'\n\nq99 <- quantile(train2$use.day, 0.99)\n\ndb <- train2 %>%\n  select(-'use.day') %>%\n  dplyr::slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  \n                        + s(use.day))\n\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  \n                        + use.day + I(use.day^2)+ I(use.day^3) )\n\ngam.fit <- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase <- db %>%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Max.min\n\ntrain2$use.day <- train2$max.min\n\nmin_ <- min(train2$use.day) \nmax_ <- max(train2$use.day) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'use.day'\n\nq99 <- quantile(train2$use.day, 0.99)\n\ndb <- train2 %>%\n  select(-'use.day') %>%\n  dplyr::slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  \n                        + s(use.day))\n\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)  \n                        + use.day + I(use.day^2) )\n\ngam.fit <- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase <- db %>%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n```\n\n\n\n\n\n\n### Driving behavior\n\nThe same parametric form is proposed for the different variants of the *Accel* and *Brake* variables, i.e., *Accel.06miles* to *Accel.14miles*, and *Brake.06miles* to *Brake.14miles*.  For the different variants of the *turn* variable, a single parametric form is also used: \n\n\\begin{align*}\ns(Brake.Accel) &\\approx Brake.Accel + Brake.Accel^2 + Brake.Accel^3\\\\\ns(Turn) &\\approx Turn + log(Turn)\n\\end{align*}\n\nThe graphs below compare the fit of the parametric approach for *Accel.06miles* and *Right.turn.intensity08* with that of the GAM model. \n  \n```{r}\n#| echo: true\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| layout-ncol: 2\n#| label: fig-MpD_GAM_sev_tel2\n#| fig-cap: \"Smoothing of Driving behavior covariates (severity)\"\n#| fig-subcap: \n#|   - \"Accel.06miles\"\n#|   - \"Right.turn.intensity08\"\n\ntrain2$use.day <- train2$Accel.06miles\n\nmin_ <- min(train2$use.day) \nmax_ <- max(train2$use.day) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'use.day'\n\nq99 <- quantile(train2$use.day, 0.99)\n\ndb <- train2 %>%\n  select(-'use.day') %>%\n  dplyr::slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2) \n                        + s(use.day))\n\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2) \n                        + use.day + I(use.day^2) + I(use.day^3) )\n\ngam.fit <- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase <- db %>%\n  mutate(diff = abs(use.day - mean(train2$use.day))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\ntrain2$use.day <- train2$Right.turn.intensity08\n\nq99 <- quantile(train2$use.day, 0.99)\n\nmin_ <- min(train2$use.day) \nmax_ <- q99\nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'use.day'\n\ndb <- train2 %>%\n  select(-'use.day') %>%\n  dplyr::slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\ntemp <- train2 %>%\n  mutate(use.day = pmin(q99, use.day))\n\nscore.gam <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2) \n                        + s(use.day))\n\nscore.glm <- as.formula(M_Claim ~ Car.use + Region + Car.age + I(Car.age^2) \n                        + Years.noclaims + I(Years.noclaims^2)\n                        + use.day + log1p(use.day))\n\ngam.fit <- gam(score.gam, family = Gamma(link = \"log\"), data = temp)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = temp)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase <- db %>%\n  mutate(diff = abs(use.day - mean(temp$use.day))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=use.day, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=use.day, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Use per day',\n       y = 'Relativity') +\n  # xlim(0, q99) +\n  theme_classic()+\n   theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n```\n\n\n\n\n:::\n  \n  \n  \n  \n```{r}\n#| echo: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\ntele.var <- c(\"Miles.per.day\", \"Avgdays.week\",\n              \"Pct.drive.mon\", \"Pct.drive.tue\", \n              \"Pct.drive.wed\", \"Pct.drive.thr\", \"Pct.drive.fri\", \"Pct.drive.sat\", \"Pct.drive.sun\",\n              \"max.day\", \"min.day\", \"max.min\", \"Dayformax\", \"Dayformin\",\n              \"Pct.drive.rush.am\", \"Pct.drive.rush.pm\",\n              \"Pct.drive.wkend\",\n              \"Pct.drive.2hrs\", \"Pct.drive.3hrs\", \"Pct.drive.4hrs\",\n              \"Accel.06miles\", \"Accel.08miles\", \"Accel.09miles\", \"Accel.11miles\", \"Accel.12miles\", \"Accel.14miles\", \n              \"Brake.06miles\", \"Brake.08miles\", \"Brake.09miles\", \"Brake.11miles\", \"Brake.12miles\", \"Brake.14miles\", \n              \"Left.turn.intensity08\", \"Left.turn.intensity09\", \"Left.turn.intensity10\", \"Left.turn.intensity11\", \"Left.turn.intensity12\",\n              \"Right.turn.intensity08\", \"Right.turn.intensity09\", \"Right.turn.intensity10\", \"Right.turn.intensity11\", \"Right.turn.intensity12\")\n\n```\n\n\n### Fitting the GLM-Net model\n\nTo solve some convergence issues, we remove from all GML-Net models *Accel.* and *Brake.* terms.  \n\n```{r}\n#| echo: false\n#| eval: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\nglm.score <- as.formula(M_Claim ~ \n                          Car.use + Region + Car.age + I(Car.age^2) + I(Car.age^3) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2) + I(Avgdays.week^3)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + I(max.day^2) + I(max.day^3) \n                        + min.day + I(min.day^2) + I(min.day^3)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs)\n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$M_Claim\n\nfold.id <- train2$fold\n\nlambda_seq <- c(10^seq(0, -8, by = -.1), 0)\ncvfit0  <- cv.glmnet(matrix.x, y, relax=FALSE, family = Gamma(link = \"log\"), lambda = lambda_seq, foldid = fold, alpha = 0)\ncvfit.2 <- cv.glmnet(matrix.x, y, relax=FALSE, family = Gamma(link = \"log\"), lambda = lambda_seq, foldid = fold, alpha = 0.2)\ncvfit.4 <- cv.glmnet(matrix.x, y, relax=FALSE, family = Gamma(link = \"log\"), lambda = lambda_seq, foldid = fold, alpha = 0.4)\ncvfit.6 <- cv.glmnet(matrix.x, y, relax=FALSE, family = Gamma(link = \"log\"), lambda = lambda_seq, foldid = fold, alpha = 0.6)\ncvfit.8 <- cv.glmnet(matrix.x, y, relax=FALSE, family = Gamma(link = \"log\"), lambda = lambda_seq, foldid = fold, alpha = 0.8)\ncvfit1  <- cv.glmnet(matrix.x, y, relax=FALSE, family = Gamma(link = \"log\"), lambda = lambda_seq, foldid = fold, alpha = 1)\n\nc(cvfit0$lambda.min, cvfit.2$lambda.min, cvfit.4$lambda.min, cvfit.6$lambda.min, cvfit.8$lambda.min, cvfit1$lambda.min)\n\nall.min <- data.frame(c(min(cvfit0$cvm), min(cvfit.2$cvm), min(cvfit.4$cvm), min(cvfit.6$cvm), min(cvfit.8$cvm), min(cvfit1$cvm))) %>%\n  mutate(alpha = 2*(row_number()-1)/10)\ncolnames(all.min)[1] <- 'min' \nall.min %>% filter(min == min(min))\n\ncvfit1$lambda.min\ncvfit1$lambda.1se\n\n```\n\n::: {.panel-tabset}\n\n\n### Optimal value\n\nThe parameters of the GLM-net were calibrated using cross-validation to obtain the model hyperparameters. It leads to a LASSO approach ($\\alpha = 1$). Using the optimal value of the penalty $\\lambda$, we can calculate the model's prediction scores based on all covariates.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_teleGLMnet1_sev_tel\n#| tbl-cap: Prediction scores for the GLM-net model\n\nglm.score <- as.formula(M_Claim ~ \n                          Car.use + Region + Car.age + I(Car.age^2) + I(Car.age^3) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2) + I(Avgdays.week^3)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + I(max.day^2) + I(max.day^3) \n                        + min.day + I(min.day^2) + I(min.day^3)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\nResult_  <- data.frame()\nResult2_  <- data.frame()\nfor(i in 1:nb.fold) {\n    learn <- train2[train2$fold != i,]\n    valid <- train2[train2$fold == i,]\n    \n    matrix.x <- model.matrix(glm.score, data=learn)[,-1]\n    y <- learn$M_Claim\n\n    lambda.min <- 0.003162278\n    lambda.1se <- 0.05011872\n    \n    lambda.select <- lambda.min\n    fit <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, alpha = 1, lambda = lambda.select)\n    #fit <- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n    learn$pred <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*learn$NB_Claim\n    \n  \n    matrix.x <- model.matrix(glm.score, data=valid)[,-1]\n    y <- valid$M_Claim\n\n    valid$pred <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*valid$NB_Claim\n    variance <- (sum((learn$AMT_Claim - learn$pred)^2)/(nrow(learn) - length(fit$beta)))\n    phi <- variance/mean(learn$AMT_Claim)^2\n    \n    \n    Result_ <- rbind(Result_, c(i, Score.pred.sev(valid$pred, valid$AMT_Claim, phi)/nrow(valid)))\n    Result2_ <- rbind(Result2_, c(i, Score.pred.sev(valid$pred, valid$AMT_Claim, phi)))\n}\n\n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ncolnames(Result2_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ntot <- colSums(Result2_)/nrow(train2)\ntot$Fold <- 'Total'\nResult_ <- rbind(Result_ , tot)\nResult_ <- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] <- 'Improvement'\n\nfor(i in 2:3){\n  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\nrownames(Result_) <- NULL\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n```\n\nThe same model can be used to compute the scores on the *test* set.\n  \n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_basetest3_sev_tel\n#| tbl-cap: Prediction scores for the GLM-net model  (testing set) \n\nglm.score <- as.formula(M_Claim ~ \n                          Car.use + Region + Car.age + I(Car.age^2) + I(Car.age^3) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2) + I(Avgdays.week^3)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + I(max.day^2) + I(max.day^3) \n                        + min.day + I(min.day^2) + I(min.day^3)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$M_Claim\n\nlambda.min <- 0.003162278\nlambda.1se <- 0.05011872\n\nlambda.select <- lambda.min\nfit <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, alpha = 1, lambda = lambda.select)\n#fit <- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n\ntrain2$pred <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*train2$NB_Claim\ntrain2$pred.tele <- train2$pred\n\nmatrix.x <- model.matrix(glm.score, data=test2)[,-1]\ny <- test2$M_Claim\n\ntest2$pred.base <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*test2$NB_Claim\nvariance <- (sum((train2$AMT_Claim - train2$pred)^2)/(nrow(train2) - length(fit$beta)))\nphi <- variance/mean(train2$AMT_Claim)^2\n\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('LASSO (optimal)', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n```\n\n\n### Parsimonious model\n\nInstead of using the optimal value of the penalty $\\lambda$  in the elastic-net approach, it is often advised to use a penalty value located at one standard error ($\\lambda_{1se}$). This approach helps to obtain a more parsimonious model. The prediction scores of such a model are displayed below.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_GLMnet2_sev_tel\n#| tbl-cap: Prediction scores for the GLM-net model\n\nResult_  <- data.frame()\nResult2_  <- data.frame()\nfor(i in 1:nb.fold) {\n  learn <- train2[train2$fold != i,]\n  valid <- train2[train2$fold == i,]\n  \n  matrix.x <- model.matrix(glm.score, data=learn)[,-1]\n  y <- learn$M_Claim\n  \n  lambda.min <- 0.003162278\n  lambda.1se <- 0.05011872\n  \n  lambda.select <- lambda.1se\n  #fit <- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\n  fit <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=TRUE, alpha = 1, lambda = lambda.select)\n  learn$pred <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*learn$NB_Claim\n  \n  \n  matrix.x <- model.matrix(glm.score, data=valid)[,-1]\n  y <- valid$M_Claim\n\n  \n  valid$pred <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*valid$NB_Claim\n  variance <- (sum((learn$AMT_Claim - learn$pred)^2)/(nrow(learn) - length(fit$beta)))\n  phi <-  variance/mean(learn$AMT_Claim)^2\n\n  Result_ <- rbind(Result_, c(i, Score.pred.sev(valid$pred, valid$AMT_Claim, phi)/nrow(valid)))\n  Result2_ <- rbind(Result2_, c(i, Score.pred.sev(valid$pred, valid$AMT_Claim, phi)))\n}\n\n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ncolnames(Result2_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ntot <- colSums(Result2_)/nrow(train2)\ntot$Fold <- 'Total'\nResult_ <- rbind(Result_ , tot)\nResult_ <- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] <- 'Improvement'\n\nfor(i in 2:3){\n  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) <- NULL\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n```\n\nThe value of $\\lambda_{1se}$ is also used to compute the scores on the *test* set.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_basetest4_sev_tel\n#| tbl-cap: Prediction scores for the GLM-net model  (testing set) \n\nglm.score <- as.formula(M_Claim ~ \n                          Car.use + Region + Car.age + I(Car.age^2) + I(Car.age^3) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2) + I(Avgdays.week^3)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + I(max.day^2) + I(max.day^3) \n                        + min.day + I(min.day^2) + I(min.day^3)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$M_Claim\n\nlambda.min <- 0.003162278\nlambda.1se <- 0.05011872\n\nlambda.select <- lambda.1se\n#fit <- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\nfit <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=TRUE, alpha = 1, lambda = lambda.select)\n\ntrain2$pred <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*train2$NB_Claim\n\nmatrix.x <- model.matrix(glm.score, data=test2)[,-1]\ny <- test2$M_Claim\n\n\ntest2$pred.base <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*test2$NB_Claim\nvariance <- (sum((train2$AMT_Claim - train2$pred)^2)/(nrow(train2) - length(fit$beta)))\nphi <- variance/mean(train2$AMT_Claim)^2\n\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('LASSO (parsimonious)', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n```\n\n### Residuals and protected variables\n\nBy comparing the results obtained with pricing models based solely on traditional variables, we observe an enhancement in prediction quality upon incorporating telematics variables. Nonetheless, we aim to assess whether there is an added advantage in retaining protected segmentation variables even with the availability of telematics data. To gauge this benefit, we compute the residuals of the GLM-Net approach. Specifically, we proceed as follows:  \n\n1) We fit a GLM-Net model using the available covariates, as previously outlined.    \n2) We predict the expected severity of the model based on the *train* data.    \n3) We utilize these predictions as an *offset* variable.   \n\nWith this modeling approach, we can now assess whether telematics data effectively eliminates the predictive power of protected variables.\n\nThe graphs below depict the extent to which protected variables contribute to the residuals from a model utilizing telematics covariates:  \n\n- Credit score and territory still exhibit a (slight) impact on severity.  \n- The impact of insured age appears to have been absorbed by telematic covariates.  \n- Insured age and marital status still appear to explain severity.  \n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| layout-ncol: 2\n#| label: fig-CreditScore_sev\n#| fig-cap: \"Observed Relativity vs. Residuals Relativity\"\n#| fig-subcap: \n#|   - \"Credit Score\"\n#|   - \"Age of the Insured\"\n#|   - \"Sex of the Insured\"\n#|   - \"Marital Status of the Insured\"\n#|   - \"Territory\"\n#| fig-width: 9\n#| fig-height: 4\n\n## Credit Score\n\nmeansev.inv <- sum(train2$NB_Claim)/sum(train2$M_Claim)\nmeanpred.inv <- sum(train2$pred.tele)/sum(train2$M_Claim)\n\ntemp2 <- train2 %>%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Group = ceiling(Credit.score/25) * 25) %>%\n  group_by(Group) %>% \n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.tele),\n            nbclaim = n()) %>% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*M_Claim/pred)\n\n\nggplot() + \n  geom_smooth(aes(x=Group, y=sev, weight = nbclaim, color='Observed'),se=F, size=1, data=temp2) + \n  geom_smooth(aes(x=Group, y=sev2, weight = nbclaim, color='Residuals'),se=F, size=1, data=temp2) + \n  labs(x = 'Credit Score',\n       y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  guides(color = guide_legend(title = \"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n## Insured Age\n\ntemp2 <- train2 %>%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Insured.age = pmin(Insured.age, 80),\n                Group = ceiling(Insured.age/5) * 5) %>%\n  group_by(Group) %>% \n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.tele),\n            nbclaim = n()) %>% \n   mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*M_Claim/pred)\n\nggplot() + \n  geom_smooth(aes(x=Group, y=sev, weight = nbclaim, color='Observed'),se=F, size=1, data=temp2) + \n  geom_smooth(aes(x=Group, y=sev2, weight = nbclaim, color='Residuals'),se=F, size=1, data=temp2) + \n  labs(x = 'Age of the insured',\n       y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  guides(color = guide_legend(title = \"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n## Sex of the insured\n\ntemp <- train2 %>%\n  mutate(Var_ = Insured.sex) %>%\n  group_by(Var_) %>%\n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.tele),\n            nbclaim = n()) %>% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n          sev2 = meanpred.inv*M_Claim/pred)\n\ntemp$sev <- temp$sev/temp$sev[1]\ntemp$sev2 <- temp$sev2/temp$sev2[1]\n\nggplot() + #start plot by by plotting bars\n  geom_point(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=0.7) +\n  geom_point(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=0.7) +\n  labs(x = 'Sex of the insured', y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  #ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+\n  guides(color=guide_legend(title=\"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n## Marital\n\ntemp <- train2 %>%\n  mutate(Var_ = Marital) %>%\n  group_by(Var_) %>%\n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.tele),\n            nbclaim = n()) %>% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*pred/nbclaim)\n\ntemp$sev <- temp$sev/temp$sev[1]\ntemp$sev2 <- temp$sev2/temp$sev2[1]\n\nggplot() + #start plot by by plotting bars\n  geom_point(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=0.7) +\n  geom_point(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=0.7) +\n  labs(x = 'Marital status of the insured', y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  #ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+\n  guides(color=guide_legend(title=\"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n## Territory\n\ntemp <- train2 %>%\n  mutate(Var_ = Territory) %>%\n  group_by(Var_) %>%\n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.tele),\n            nbclaim = n()) %>% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*M_Claim/pred)\n\n#temp$sev <- temp$sev/temp$sev[1]\n#temp$sev2 <- temp$sev2/temp$sev2[1]\n\nggplot() + #start plot by by plotting bars\n  #geom_point(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=0.7) +\n  #geom_point(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=0.7) +\n  labs(x = 'Territory', y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  scale_x_discrete(labels = NULL, breaks = NULL)+\n  #ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+\n  guides(color=guide_legend(title=\"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n```\n\n### GLM-net on residuals\n\nBy utilizing the GLM-Net prediction as an offset variable, we can fit another GLM-Net model, this time exclusively employing the protected variables. The table below presents the predicted scores of the two new models: the LASSO* with optimal $\\lambda$, and the LASSO* with a penalty value situated at one standard error. Overall, we observe a slight improvement in the various scores with the addition of protected variables.\n\n```{r}\n#| echo: false\n#| eval: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\nglm.score <- as.formula(M_Claim ~ \n                          Car.use + Region + Car.age + I(Car.age^2) + I(Car.age^3) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2) + I(Avgdays.week^3)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + I(max.day^2) + I(max.day^3) \n                        + min.day + I(min.day^2) + I(min.day^3)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$M_Claim\n\n\nlambda.min <- 0.003162278\nlasso.min <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, alpha = 1, lambda = lambda.min)\ntrain2$pred.tele <- predict(lasso.min, newx = matrix.x, type='response', lambda = lambda.min)\n\n###\n\nvar.sens <- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\")     \nglm.score <- as.formula(M_Claim ~ Insured.sex + Marital \n                        + Credit.score +  I(Credit.score^2) \n                        + Insured.age + I(Insured.age^2) \n                        + terr.code + I(terr.code^2)  + I(terr.code^3))\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$M_Claim\noffset <- log(train2$pred.tele)\nfold.id <- train2$fold\n\nlambda_seq <- c(10^seq(0, -8, by = -.1), 0)\ncvfit0  <- cv.glmnet(matrix.x, y, relax=FALSE, family = Gamma(link = \"log\"), offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0)\ncvfit.2 <- cv.glmnet(matrix.x, y, relax=FALSE, family = Gamma(link = \"log\"), offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.2)\ncvfit.4 <- cv.glmnet(matrix.x, y, relax=FALSE, family = Gamma(link = \"log\"), offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.4)\ncvfit.6 <- cv.glmnet(matrix.x, y, relax=FALSE, family = Gamma(link = \"log\"), offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.6)\ncvfit.8 <- cv.glmnet(matrix.x, y, relax=FALSE, family = Gamma(link = \"log\"), offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.8)\ncvfit1  <- cv.glmnet(matrix.x, y, relax=FALSE, family = Gamma(link = \"log\"), offset = offset, lambda = lambda_seq, foldid = fold, alpha = 1)\n\nc(cvfit0$lambda.min, cvfit.2$lambda.min, cvfit.4$lambda.min, cvfit.6$lambda.min, cvfit.8$lambda.min, cvfit1$lambda.min)\n\nall.min <- data.frame(c(min(cvfit0$cvm), min(cvfit.2$cvm), min(cvfit.4$cvm), min(cvfit.6$cvm), min(cvfit.8$cvm), min(cvfit1$cvm))) %>%\n  mutate(alpha = 2*(row_number()-1)/10)\ncolnames(all.min)[1] <- 'min' \nall.min %>% filter(min == min(min))\n\ncvfit1$lambda.min\ncvfit1$lambda.1se\n\n\n```\n\n\n\nWe thus fit a GLM-net model using telematics and traditional covariates, and we predict the expected severity of the model on the *train* database. Using the prediction as an *offset* variable, we analyze the impact of each variable.\n\n\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_basetest4b_sev_tel\n#| tbl-cap: Prediction scores for the GLM-net model  (testing set) \n\nglm.score <- as.formula(M_Claim ~ \n                          Car.use + Region + Car.age + I(Car.age^2) + I(Car.age^3) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2) + I(Avgdays.week^3)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + I(max.day^2) + I(max.day^3) \n                        + min.day + I(min.day^2) + I(min.day^3)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$M_Claim\n\n\nlambda.min <- 0.003162278\nlasso.min <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, alpha = 1, lambda = lambda.min)\ntrain2$pred.tele <- predict(lasso.min, newx = matrix.x, type='response', lambda = lambda.min)\n\n\nmatrix.x <- model.matrix(glm.score, data=test2)[,-1]\ny <- test2$M_Claim\n\ntest2$pred.tele <- predict(lasso.min, newx = matrix.x, type='response', lambda = lambda.min)\n\nglm.score <- as.formula(M_Claim ~ Insured.sex + Marital \n                        + Credit.score +  I(Credit.score^2) \n                        + Insured.age +  I(Insured.age^2) \n                        + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$M_Claim\noffset <- log(train2$pred.tele)\n\nlambda.min <- 0.003162278\nlambda.1se <- 0.1258925\n\nlambda.select <- lambda.min\n#fit <- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\nfit <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n\n\ntrain2$pred <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)*train2$NB_Claim\n\nmatrix.x <- model.matrix(glm.score, data=test2)[,-1]\ny <- test2$M_Claim\noffset <- log(test2$pred.tele)\n\ntest2$pred.base <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)*test2$NB_Claim\nvariance <- (sum((train2$AMT_Claim - train2$pred)^2)/(nrow(train2) - length(fit$beta)))\nphi <- variance/mean(train2$AMT_Claim)^2\n\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('LASSO* (optimal)', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- rbind(Result_all, Result_)\n\n###\n\nglm.score <- as.formula(M_Claim ~ Insured.sex + Marital \n                        + Credit.score +  I(Credit.score^2) \n                        + Insured.age + I(Insured.age^2) \n                        + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$M_Claim\noffset <- log(train2$pred.tele)\n\nlambda.min <- 0.003162278\nlambda.1se <- 0.1258925\n\nlambda.select <- lambda.1se\nfit <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\n#fit <- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n\ntrain2$pred <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)*train2$NB_Claim\n\nmatrix.x <- model.matrix(glm.score, data=test2)[,-1]\ny <- test2$M_Claim\noffset <- log(test2$pred.tele)\n\ntest2$pred.base <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)*test2$NB_Claim\nvariance <- (sum((train2$AMT_Claim - train2$pred)^2)/(nrow(train2) - length(fit$beta)))\nphi <- variance/mean(train2$AMT_Claim)^2\n\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('LASSO* (parsimonious)', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n```\n\n\n\n\n```{r}\n#| echo: FALSE\n#| eval: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_basetest4b_sev_tel_2\n#| tbl-cap: Prediction scores for the GLM-net model without Credit Score  (testing set) \n\nglm.score <- as.formula(M_Claim ~ \n                          Car.use + Region + Car.age + I(Car.age^2) + I(Car.age^3) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + Dayformax + Dayformin +\n                        + Miles.per.day + log(Miles.per.day)\n                        + Avgdays.week + I(Avgdays.week^2) + I(Avgdays.week^3)\n                        + Pct.drive.mon + I(Pct.drive.mon^2)\n                        + Pct.drive.tue + I(Pct.drive.tue^2)\n                        + Pct.drive.wed + I(Pct.drive.wed^2)\n                        + Pct.drive.thr + I(Pct.drive.thr^2)\n                        + Pct.drive.fri + I(Pct.drive.fri^2)\n                        + Pct.drive.sat + I(Pct.drive.sat^2)\n                        + Pct.drive.sun + I(Pct.drive.sun^2)\n                        + Pct.drive.wkend + I(Pct.drive.wkend^2)\n                        + max.day + I(max.day^2) + I(max.day^3) \n                        + min.day + I(min.day^2) + I(min.day^3)\n                        + max.min + I(max.min^2)\n                        + Pct.drive.rush.am + sqrt(Pct.drive.rush.am) \n                        + Pct.drive.rush.pm + sqrt(Pct.drive.rush.pm)   \n                        + Pct.drive.2hrs + sqrt(Pct.drive.2hrs) \n                        + Pct.drive.3hrs + sqrt(Pct.drive.3hrs) \n                        + Pct.drive.4hrs + sqrt(Pct.drive.4hrs) \n                        + Left.turn.intensity08 + log1p(Left.turn.intensity08)\n                        + Left.turn.intensity09 + log1p(Left.turn.intensity09)\n                        + Left.turn.intensity10 + log1p(Left.turn.intensity10)\n                        + Left.turn.intensity11 + log1p(Left.turn.intensity11)\n                        + Left.turn.intensity12 + log1p(Left.turn.intensity12)\n                        + Right.turn.intensity08 + log1p(Right.turn.intensity08)\n                        + Right.turn.intensity09 + log1p(Right.turn.intensity09)\n                        + Right.turn.intensity10 + log1p(Right.turn.intensity10)\n                        + Right.turn.intensity11 + log1p(Right.turn.intensity11)\n                        + Right.turn.intensity12 + log1p(Right.turn.intensity12))\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$M_Claim\n\n\nlambda.min <- 0.003162278\nlasso.min <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, alpha = 1, lambda = lambda.min)\ntrain2$pred.tele <- predict(lasso.min, newx = matrix.x, type='response', lambda = lambda.min)\n\n\nmatrix.x <- model.matrix(glm.score, data=test2)[,-1]\ny <- test2$M_Claim\n\ntest2$pred.tele <- predict(lasso.min, newx = matrix.x, type='response', lambda = lambda.min)\n\nglm.score <- as.formula(M_Claim ~ Insured.sex + Marital \n                        + Insured.age +  I(Insured.age^2) \n                        + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$M_Claim\noffset <- log(train2$pred.tele)\n\nlambda.min <- 0.003162278\nlambda.1se <- 0.1258925\n\nlambda.select <- lambda.min\n#fit <- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\nfit <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n\n\ntrain2$pred <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)*train2$NB_Claim\n\nmatrix.x <- model.matrix(glm.score, data=test2)[,-1]\ny <- test2$M_Claim\noffset <- log(test2$pred.tele)\n\ntest2$pred.base <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)*test2$NB_Claim\nvariance <- (sum((train2$AMT_Claim - train2$pred)^2)/(nrow(train2) - length(fit$beta)))\nphi <- variance/mean(train2$AMT_Claim)^2\n\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('LASSO** (optimal)', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- rbind(Result_all, Result_)\n\n###\n\nglm.score <- as.formula(M_Claim ~ Insured.sex + Marital \n                        + Insured.age + I(Insured.age^2) \n                        + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$M_Claim\noffset <- log(train2$pred.tele)\n\nlambda.min <- 0.003162278\nlambda.1se <- 0.1258925\n\nlambda.select <- lambda.1se\nfit <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\n#fit <- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n\ntrain2$pred <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)*train2$NB_Claim\n\nmatrix.x <- model.matrix(glm.score, data=test2)[,-1]\ny <- test2$M_Claim\noffset <- log(test2$pred.tele)\n\ntest2$pred.base <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)*test2$NB_Claim\nvariance <- (sum((train2$AMT_Claim - train2$pred)^2)/(nrow(train2) - length(fit$beta)))\nphi <- variance/mean(train2$AMT_Claim)^2\n\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('LASSO** (parsimonious)', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n```\n\n:::\n  \n\n## XGBoost\n\nWe now consider an XGBoost model. As with the frequency model, hyperparameter values are obtained by performing a Bayesian search on a grid of possible values.\n\n```{r}\n#| echo: false\n#| eval: false\n#| message: FALSE\n#| warning: FALSE\n\nlibrary(xgboost)\nlibrary(Ckmeans.1d.dp)\nlibrary(SHAPforxgboost)\nlibrary(pacman)\n\n# p_load automatically installs packages if needed\np_load(xgboost, ParBayesianOptimization, mlbench, dplyr, skimr, recipes, resample)\n\nall.vars2 <- c(\"Car.use\", \"Region\", \"Car.age\", \"Years.noclaims\",\n               \"Miles.per.day\", \"Avgdays.week\",\n               \"Pct.drive.mon\", \"Pct.drive.tue\", \"Pct.drive.wed\", \"Pct.drive.thr\", \"Pct.drive.fri\", \"Pct.drive.sat\", \"Pct.drive.sun\",\n               \"max.day\", \"min.day\", \"max.min\", \"Dayformax\", \"Dayformin\",\n               \"Pct.drive.rush.am\", \"Pct.drive.rush.pm\",\n               \"Pct.drive.wkend\",\n               \"Pct.drive.2hrs\", \"Pct.drive.3hrs\", \"Pct.drive.4hrs\",\n               \"Accel.06miles\", \"Accel.08miles\", \"Accel.09miles\", \"Accel.11miles\", \"Accel.12miles\", \"Accel.14miles\", \n               \"Brake.06miles\", \"Brake.08miles\", \"Brake.09miles\", \"Brake.11miles\", \"Brake.12miles\", \"Brake.14miles\", \n               \"Left.turn.intensity08\", \"Left.turn.intensity09\", \"Left.turn.intensity10\", \"Left.turn.intensity11\", \"Left.turn.intensity12\",\n               \"Right.turn.intensity08\", \"Right.turn.intensity09\", \"Right.turn.intensity10\", \"Right.turn.intensity11\", \"Right.turn.intensity12\")\n\ndtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(all.vars2)]), label = train2$M_Claim)\n#setinfo(dtrain,\"base_margin\",log(train2$expo))\nfolds <-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n\nbounds <- list(eta = c(0.001, 0.5),\n               max_depth = c(1L, 50L),\n               subsample = c(0.1, 1),\n               min_child_weight = c(1, 175))\n\nobj_func <- function(eta, max_depth, subsample, min_child_weight) {\n  param <- list(\n    eta = eta,\n    max_depth = max_depth,\n    subsample = subsample,\n    min_child_weight = min_child_weight,\n    booster = \"gbtree\",\n    objective = \"reg:gamma\",\n    eval_metric = \"gamma-nloglik\")\n  \n  set.seed(133)\n  xgbcv <- xgb.cv(params = param,\n                  nrounds = base.rounds,\n                  data = dtrain,\n                  folds = folds,\n                  prediction = TRUE,\n                  early_stopping_rounds = 10,\n                  verbose = 0,\n                  maximize = F)\n  \n  lst <- list(\n    Score = -min(xgbcv$evaluation_log$test_gamma_nloglik_mean),\n    nrounds = xgbcv$best_iteration\n  )\n  \n  return(lst)\n}\n\nbase.rounds <- 500\nset.seed(254)\nbayes_out <- bayesOpt(FUN = obj_func, bounds = bounds, initPoints = length(bounds) + 2, iters.n = 5)\ncomp <- bayes_out$scoreSummary[which(bayes_out$scoreSummary$Score== max(bayes_out$scoreSummary$Score))]\ncomp \n\n#   Epoch Iteration       eta max_depth subsample min_child_weight gpUtility acqOptimum inBounds Elapsed     Score nrounds errorMessage\n#1:     0         6 0.1914908         5 0.8674295         100.0327        NA      FALSE     TRUE    2.96 -8.959176     135           NA\n\n############\n## Verif ###\n############\n\n#1- Par la fonction\nobj_func(eta=comp$eta, max_depth=comp$max_depth, subsample=comp$subsample, min_child_weight=comp$min_child_weight)\n\n#2- Par le XGB.CV\nparam <- list(\n  eta = 0.1914908,\n  max_depth = 5,\n  subsample = 0.8674295,\n  min_child_weight = 100.0327,\n  booster = \"gbtree\",\n  objective = \"reg:gamma\",\n  eval_metric = \"gamma-nloglik\")\n\nset.seed(133)\nxgbcv <- xgb.cv(params = param,\n                nrounds = 145,\n                data = dtrain,\n                folds = folds,\n                prediction = TRUE,\n                early_stopping_rounds = 10,\n                verbose = 0,\n                maximize = F)\n\n-min(xgbcv$evaluation_log$test_gamma_nloglik_mean)\n\n\n```\n\n\n\n::: {.panel-tabset}\n\n### Prediction Scores\n\nUsing these values, we can calculate the models prediction scores based on all classical and telematics covariates.  One can see that the XGBoost approach is particularly effective in capturing the effect of all available telematic covariates. Indeed, the scores obtained are significantly improved compared to other tested approaches.\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n\nlibrary(xgboost)\nlibrary(Ckmeans.1d.dp)\nlibrary(SHAPforxgboost)\nlibrary(pacman)\n\nall.vars2 <- c(\"Car.use\", \"Region\", \"Car.age\", \"Years.noclaims\",\n               \"Miles.per.day\", \"Avgdays.week\",\n               \"Pct.drive.mon\", \"Pct.drive.tue\", \"Pct.drive.wed\", \"Pct.drive.thr\", \"Pct.drive.fri\", \"Pct.drive.sat\", \"Pct.drive.sun\",\n               \"max.day\", \"min.day\", \"max.min\", \"Dayformax\", \"Dayformin\",\n               \"Pct.drive.rush.am\", \"Pct.drive.rush.pm\",\n               \"Pct.drive.wkend\",\n               \"Pct.drive.2hrs\", \"Pct.drive.3hrs\", \"Pct.drive.4hrs\",\n               \"Accel.06miles\", \"Accel.08miles\", \"Accel.09miles\", \"Accel.11miles\", \"Accel.12miles\", \"Accel.14miles\", \n               \"Brake.06miles\", \"Brake.08miles\", \"Brake.09miles\", \"Brake.11miles\", \"Brake.12miles\", \"Brake.14miles\", \n               \"Left.turn.intensity08\", \"Left.turn.intensity09\", \"Left.turn.intensity10\", \"Left.turn.intensity11\", \"Left.turn.intensity12\",\n               \"Right.turn.intensity08\", \"Right.turn.intensity09\", \"Right.turn.intensity10\", \"Right.turn.intensity11\", \"Right.turn.intensity12\")\n\ndtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(all.vars2)]), label = train2$M_Claim)\n#setinfo(dtrain,\"base_margin\",log(train2$expo))\nfolds <-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n```\n\n\n```{r}\n#| echo: true\n#| cache: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_XGBoosttele_sev_tel\n#| tbl-cap: Prediction scores for the XGBoost model with telematics (severity)\n\nparam <- list(\n  eta = 0.1914908,\n  max_depth = 5,\n  subsample = 0.8674295,\n  min_child_weight = 100.0327,\n  booster = \"gbtree\",\n  objective = \"reg:gamma\",\n  eval_metric = \"gamma-nloglik\")\n\nset.seed(133)\nxgbcv <- xgb.cv(params = param,\n                nrounds = 145,\n                data = dtrain,\n                folds = folds,\n                prediction = TRUE,\n                early_stopping_rounds = 10,\n                verbose = 0,\n                maximize = F)\n  \nvariance <- sapply(xgbcv$folds, function(x){sum((train2$AMT_Claim[x]-unlist(xgbcv$pred[x])*train2$NB_Claim[x])^2)/length(train2$AMT_Claim[x])})  \nmean <- sapply(xgbcv$folds, function(x){mean(train2$AMT_Claim[x])})\nphi <- unlist(variance)/mean^2\n\nSc.log1 <- -dgamma(train2$AMT_Claim[xgbcv$folds$fold1], shape = 1/phi[1], scale = unlist(xgbcv$pred[xgbcv$folds$fold1])*train2$NB_Claim[xgbcv$folds$fold1]*phi[1], log=TRUE)\nSc.log2 <- -dgamma(train2$AMT_Claim[xgbcv$folds$fold2], shape = 1/phi[2], scale = unlist(xgbcv$pred[xgbcv$folds$fold2])*train2$NB_Claim[xgbcv$folds$fold2]*phi[2], log=TRUE)\nSc.log3 <- -dgamma(train2$AMT_Claim[xgbcv$folds$fold3], shape = 1/phi[3], scale = unlist(xgbcv$pred[xgbcv$folds$fold3])*train2$NB_Claim[xgbcv$folds$fold3]*phi[3], log=TRUE)\nSc.log4 <- -dgamma(train2$AMT_Claim[xgbcv$folds$fold4], shape = 1/phi[4], scale = unlist(xgbcv$pred[xgbcv$folds$fold4])*train2$NB_Claim[xgbcv$folds$fold4]*phi[4], log=TRUE)\nSc.log5 <- -dgamma(train2$AMT_Claim[xgbcv$folds$fold5], shape = 1/phi[5], scale = unlist(xgbcv$pred[xgbcv$folds$fold5])*train2$NB_Claim[xgbcv$folds$fold5]*phi[5], log=TRUE)\n\nSc.MSE <- sapply(xgbcv$folds, function(x){(train2$AMT_Claim[x]-unlist(xgbcv$pred[x])*train2$NB_Claim[x])^2/1000000})\n\n \nResult_  <- rbind(\nc(1,mean(Sc.log1), mean(Sc.MSE[1]$fold1)),\nc(2,mean(Sc.log2), mean(Sc.MSE[2]$fold2)),\nc(3,mean(Sc.log3), mean(Sc.MSE[3]$fold3)),\nc(4,mean(Sc.log4), mean(Sc.MSE[4]$fold4)),\nc(5,mean(Sc.log5), mean(Sc.MSE[5]$fold5))\n)\n\nRes.sum  <- rbind(\nc(sum(Sc.log1), sum(Sc.MSE[1]$fold1)),\nc(sum(Sc.log2), sum(Sc.MSE[2]$fold2)),\nc(sum(Sc.log3), sum(Sc.MSE[3]$fold3)),\nc(sum(Sc.log4), sum(Sc.MSE[4]$fold4)),\nc(sum(Sc.log5), sum(Sc.MSE[5]$fold5))\n)\nsum <- c('Total', colSums(Res.sum)/nrow(train2))\n\nResult_  <- data.frame(rbind(Result_, sum)) \n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\nResult_ <- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] <- 'Improvement'\n\nfor(i in 2:3){\n  Result_[,i] <- as.numeric(Result_[,i])  \n  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\nrownames(Result_) <- NULL\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n```\n\nThe same model can be used to compute the scores on the *test* set.  We also observe that the XGBoost approach is the most effective.\n\n```{r}\n#| echo: true\n#| cache: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_XGBoost_correction333_sev_tel\n#| tbl-cap: Prediction scores for the XGBoost model with telematics (severity)\n\ndtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(all.vars2)]), label = train2$M_Claim)\n#setinfo(dtrain,\"base_margin\",log(train2$expo))\nfolds <-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n\nparam <- list(\n  eta = 0.1914908,\n  max_depth = 5,\n  subsample = 0.8674295,\n  min_child_weight = 100.0327,\n  booster = \"gbtree\",\n  objective = \"reg:gamma\",\n  eval_metric = \"gamma-nloglik\")\n\nset.seed(133)\nfit.xgb <- xgb.train(params = param,\n                     nrounds = 145,\n                     data = dtrain)\n\ntrain2$pred.xgb <- predict(fit.xgb, dtrain, type='response')*train2$NB_Claim\ntrain2$pred.xgb.off <- predict(fit.xgb, dtrain, type='response')\n\ndtest <- xgb.DMatrix(data = data.matrix(test2[, paste(all.vars2)]), label = test2$M_Claim)\n#setinfo(dtest,\"base_margin\",log(test2$expo))\ntest2$pred.xgb <- predict(fit.xgb, dtest, type='response')*test2$NB_Claim\ntest2$pred.xgb.off <- predict(fit.xgb, dtest, type='response')\n\ntest2$pred.base <- test2$pred.xgb\n\nvariance <- (sum((train2$pred.xgb - (train2$AMT_Claim))^2)/(length(train2$AMT_Claim)))\nphi <- variance/mean(train2$AMT_Claim)^2\n\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('XGBoost', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n```\n\n\n### Variables Importance\n\nThe graph below illustrates the most important variables in the XGBoost model. We see that the most important telematic variable is related to the vehicle usage intensity, even for severity. Nevertheless, the XGBoost model is capable of identifying a few other important covariates.\n\n```{r}\n#| echo: true\n#| cache: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\nimportance_matrix <- xgb.importance(dimnames(dtrain)[[2]], model = fit.xgb)\nxgb.ggplot.importance(importance_matrix,top_n=15) + theme(text = element_text(size=15))\n\n```\n\n\n### RESIDUALS AND PROTECTED VARIABLES\n\nAs we did for the GLM-net model, we can check whether the protected variables we excluded from the analysis retain predictive capacity.\n\nThe graphs below depict the extent to which protected variables contribute to the residuals from a model utilizing telematics covariates.  The conclusions are consistent with those obtained with the GLM-net model: most of the effect seems to be captured by the traditional and telematic variables. However, we see that for the *Credit Score* variable, there is a loss of information if the variable is not used in the model and that this loss is not completely compensated by the telematics covariates.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| layout-ncol: 2\n#| label: fig-CreditScore_sev2\n#| fig-cap: \"Observed Relativity vs. Residuals Relativity\"\n#| fig-subcap: \n#|   - \"Credit Score\"\n#|   - \"Age of the Insured\"\n#|   - \"Sex of the Insured\"\n#|   - \"Marital Status of the Insured\"\n#|   - \"Territory\"\n#| fig-width: 9\n#| fig-height: 4\nmoy.xgb <- sum(train2$pred.xgb.off)/sum(train2$M_Claim)\n\nmeansev.inv <- sum(train2$NB_Claim)/sum(train2$M_Claim)\nmeanpred.inv <- sum(train2$pred.xgb.off)/sum(train2$M_Claim)\n\n\ntemp2 <- train2 %>%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Insured.age = pmin(Insured.age, 80),\n                Group = ceiling(Credit.score/25) * 25) %>%\n  group_by(Group) %>% \n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.xgb.off),\n            nbclaim = n()) %>% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*M_Claim/pred)\n\nGraph_resCS <- ggplot() + \n  geom_smooth(aes(x=Group, y=sev, weight = nbclaim, color='Observed'),se=F, size=1, data=temp2) + \n  geom_smooth(aes(x=Group, y=sev2, weight = nbclaim, color='Residuals'),se=F, size=1, data=temp2) + \n  labs(x = 'Credit Score',\n       y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  guides(color = guide_legend(title = \"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\nprint(Graph_resCS)\nsave(Graph_resCS, file = \"Data/Graph_resCS_sev.rdata\")\n\n\n### Age of the insured\n\n\nmeansev.inv <- sum(train2$NB_Claim)/sum(train2$M_Claim)\nmeanpred.inv <- sum(train2$pred.xgb.off)/sum(train2$M_Claim)\n\n\ntemp2 <- train2 %>%\n  dplyr::mutate(Duration.y = Duration/365.25, \n                Insured.age = pmin(Insured.age, 80),\n                Group = ceiling(Insured.age/5) * 5) %>%\n  group_by(Group) %>% \n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.xgb.off),\n            nbclaim = n()) %>% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*M_Claim/pred)\n\nGraph_resAge <- ggplot() + \n  geom_smooth(aes(x=Group, y=sev, weight = nbclaim, color='Observed'),se=F, size=1, data=temp2) + \n  geom_smooth(aes(x=Group, y=sev2, weight = nbclaim, color='Residuals'),se=F, size=1, data=temp2) + \n  labs(x = 'Insured.age',\n       y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  guides(color = guide_legend(title = \"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\nprint(Graph_resAge)\nsave(Graph_resAge, file = \"Data/Graph_resAge_sev.rdata\")\n\n###\n\ntemp <- train2 %>%\n  mutate(Var_ = Insured.sex) %>%\n  group_by(Var_) %>%\n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.xgb),\n            nbclaim = n()) %>% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*M_Claim/pred)\n\ntemp$sev <- temp$sev/temp$sev[1]\ntemp$sev2 <- temp$sev2/temp$sev2[1]\n\nggplot() + #start plot by by plotting bars\n  geom_point(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=0.7) +\n  geom_point(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=0.7) +\n  labs(x = 'Sex of the insured', y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  #ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+\n  guides(color=guide_legend(title=\"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n###\n\n temp <- train2 %>%\n   mutate(Var_ = Marital) %>%\n   group_by(Var_) %>%\n   summarize(M_Claim=sum(M_Claim),\n             pred=sum(pred.xgb),\n             nbclaim = n()) %>% \n   mutate(sev = meansev.inv*M_Claim/nbclaim,\n          sev2 = meanpred.inv*M_Claim/pred)\n \n temp$sev <- temp$sev/temp$sev[1]\n temp$sev2 <- temp$sev2/temp$sev2[1]\n \n ggplot() + #start plot by by plotting bars\n   geom_point(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=3) +\n   geom_line(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=0.7) +\n   geom_point(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=3) +\n   geom_line(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=0.7) +\n   labs(x = 'Marital status of the insured', y = 'Relativity') +\n   geom_hline(yintercept = 1, linetype='dashed')+\n   #ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+\n   guides(color=guide_legend(title=\"\")) +\n    theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n \n###\n\ntemp <- train2 %>%\n  mutate(Var_ = Territory) %>%\n  group_by(Var_) %>%\n  summarize(M_Claim=sum(M_Claim),\n            pred=sum(pred.xgb.off),\n            nbclaim = n()) %>% \n  mutate(sev = meansev.inv*M_Claim/nbclaim,\n         sev2 = meanpred.inv*M_Claim/pred)\n\n#temp$sev <- temp$sev/temp$sev[1]\n#temp$sev2 <- temp$sev2/temp$sev2[1]\n\nGraph_resTerr <- ggplot() + #start plot by by plotting bars\n  #geom_point(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev2), group = 1, color='Residuals'), size=0.7) +\n  #geom_point(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=3) +\n  geom_line(data = temp, aes(x = Var_, y = (sev), group = 1, color='Observed'), size=0.7) +\n  labs(x = 'Territory', y = 'Relativity') +\n  geom_hline(yintercept = 1, linetype='dashed')+\n  scale_x_discrete(labels = NULL, breaks = NULL)+\n  #ylim(max(temp$freq, temp$freq2)*0.95, max(temp$freq, temp$freq2)*1.05)+\n  guides(color=guide_legend(title=\"\")) +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\nprint(Graph_resTerr)\nsave(Graph_resTerr, file = \"Data/Graph_resTerr_sev.rdata\")\n\n\n```\n\n\n### XGBOOST ON RESIDUALS\n\nAs we did with the GLM-Net approach, we use the predictions of the XGBoost model as an offset variable and fit another XGBoost model on the data, using only the protected covariates.  The prediction scores are show in the table below.  We still observe a slight improvement in prediction scores, indicating that protected variables still retain some predictive power despite the use of telematic information.\n\n\n```{r}\n#| echo: false\n#| eval: false\n#| message: FALSE\n#| warning: FALSE\n\n\nvar.sens <- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\")    \n\ndtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(var.sens)]), label = train2$M_Claim)\nsetinfo(dtrain,\"base_margin\",log(train2$pred.xgb.off))\nfolds <-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n\nbounds <- list(eta = c(0.001, 0.5),\n               max_depth = c(1L, 50L),\n               subsample = c(0.1, 1),\n               min_child_weight = c(1, 175))\n\nobj_func <- function(eta, max_depth, subsample, min_child_weight) {\n  param <- list(\n    eta = eta,\n    max_depth = max_depth,\n    subsample = subsample,\n    min_child_weight = min_child_weight,\n    booster = \"gbtree\",\n    objective = \"reg:gamma\",\n    eval_metric = \"gamma-nloglik\")\n  \n  set.seed(533)\n  xgbcv <- xgb.cv(params = param,\n                  nrounds = base.rounds,\n                  data = dtrain,\n                  folds = folds,\n                  prediction = TRUE,\n                  early_stopping_rounds = 10,\n                  verbose = 0,\n                  maximize = F)\n  \n  lst <- list(\n    Score = -min(xgbcv$evaluation_log$test_gamma_nloglik_mean),\n    nrounds = xgbcv$best_iteration\n  )\n  \n  return(lst)\n}\n\nbase.rounds <- 500\nset.seed(254)\nbayes_out <- bayesOpt(FUN = obj_func, bounds = bounds, initPoints = length(bounds) + 2, iters.n = 5)\ncomp <- bayes_out$scoreSummary[which(bayes_out$scoreSummary$Score== max(bayes_out$scoreSummary$Score))]\ncomp \n\n#Epoch Iteration        eta max_depth subsample min_child_weight gpUtility acqOptimum inBounds Elapsed     Score nrounds errorMessage\n#1:     0         4 0.02337437        26 0.8097923         123.6033        NA      FALSE     TRUE       1 -8.687035     103           NA\n```\n\n\n```{r}\n#| echo: true\n#| eval: true\n#| output: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\nvar.sens <- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\")    \n\ndtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(var.sens)]), label = train2$M_Claim)\nsetinfo(dtrain,\"base_margin\",log(train2$pred.xgb.off))\nfolds <-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n\nparam <- list(\n  eta = 0.02337437,\n  max_depth = 26,\n  subsample = 0.8097923,\n  min_child_weight = 123.6033,\n  booster = \"gbtree\",\n  objective = \"reg:gamma\",\n  eval_metric = \"gamma-nloglik\")\n\nset.seed(533)\nfit.xgb2 <- xgb.train(params = param,\n                      nrounds = 103,\n                      data = dtrain)\n\nvar.sens <- c(\"Marital\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Territory\")    \n\ntrain2$pred.xgb <- predict(fit.xgb2, dtrain, type='response')*train2$NB_Claim\n\ndtest <- xgb.DMatrix(data = data.matrix(test2[, paste(var.sens)]), label = test2$M_Claim)\nsetinfo(dtest,\"base_margin\",log(test2$pred.xgb.off))\n\ntest2$pred.base <- predict(fit.xgb2, dtest, type='response')*test2$NB_Claim\n\nvariance <- (sum((train2$pred.xgb - (train2$AMT_Claim))^2)/(length(train2$AMT_Claim)))\nphi <- variance/mean(train2$AMT_Claim)^2\n```\n\n\n```{r}\n#| echo: true\n#| eval: true\n#| output: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_XGBoost_correction33_sev_tel\n#| tbl-cap: Prediction scores for the XGBoost model with telematics. (severity)\n\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('XGBoost*', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- rbind(Result_all, Result_)\n\nsave(Result_all, file='Data/ResultsSynth_sev.Rda')\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n```\n\n```{r}\n#| echo: FALSE\n#| eval: FALSE\n#| output: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\nvar.sensWO <- c(\"Marital\", \"Insured.sex\", \"Insured.age\", \"Territory\")    \n\ndtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(var.sensWO)]), label = train2$M_Claim)\nsetinfo(dtrain,\"base_margin\",log(train2$pred.xgb.off))\nfolds <-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n\nparam <- list(\n  eta = 0.02337437,\n  max_depth = 26,\n  subsample = 0.8097923,\n  min_child_weight = 123.6033,\n  booster = \"gbtree\",\n  objective = \"reg:gamma\",\n  eval_metric = \"gamma-nloglik\")\n\nset.seed(533)\nfit.xgb3 <- xgb.train(params = param,\n                      nrounds = 103,\n                      data = dtrain)\n\nvar.sensWO <- c(\"Marital\", \"Insured.sex\", \"Insured.age\", \"Territory\")    \n\ntrain2$pred.xgb <- predict(fit.xgb3, dtrain, type='response')*train2$NB_Claim\n\ndtest <- xgb.DMatrix(data = data.matrix(test2[, paste(var.sensWO)]), label = test2$M_Claim)\nsetinfo(dtest,\"base_margin\",log(test2$pred.xgb.off))\n\ntest2$pred.base <- predict(fit.xgb3, dtest, type='response')*test2$NB_Claim\n\nvariance <- (sum((train2$pred.xgb - (train2$AMT_Claim))^2)/(length(train2$AMT_Claim)))\nphi <- variance/mean(train2$AMT_Claim)^2\n```\n\n\n```{r}\n#| echo: FALSE\n#| eval: FALSE\n#| output: FALSE\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_XGBoost_correction33_sev_tel_WO\n#| tbl-cap: Prediction scores for the XGBoost model with telematics. (severity)\n\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('XGBoost**', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n```\n\n\nThe graph below illustrates the most important protected variables in the XGBoost model. Unsurprisingly, the insured's sex and marital status come at the very bottom. Additionally, the most important protected variable is credit score. These results are consistent with the conclusions of the GLM-net model.\n\n```{r}\n#| echo: true\n#| cache: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\nimportance_matrix <- xgb.importance(dimnames(dtrain)[[2]], model = fit.xgb2)\nxgb.ggplot.importance(importance_matrix,top_n=15) + theme(text = element_text(size=15))\n\n\n```\n\n\n:::\n\n\n\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","embed-resources":false,"css":["custom.css"],"toc":true,"output-file":"severityVarTelematique.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","bibliography":["bibtex.bib"],"editor":"source","theme":"sandstone","fontsize":"1.0em","linestretch":1.4,"grid":{"sidebar-width":"250px","body-width":"1000px","margin-width":"250px","gutter-width":"1.5rem"}},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"severityVarTelematique.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"block-headings":true,"bibliography":["bibtex.bib"],"editor":"source","documentclass":"scrreprt"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}