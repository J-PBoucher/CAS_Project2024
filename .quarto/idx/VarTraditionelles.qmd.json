{"title":"Traditional Covariates","markdown":{"headingText":"Traditional Covariates","containsRefs":false,"markdown":"\n## Preamble\n\n::: {.panel-tabset}\n\n### Chapter Objective\n\nUsing only traditional covariates, the objective of this chapter is to propose various statistical models for estimation and variable selection to predict the number of claims. More specifically, the following model families will be examined:  \n\n- Basic GLM,  \n- GLM family, including elastic-net,    \n- XGBoost.  \n\nAs mentioned in the theory review chapter, to compare models and strike a balance between bias and variance while avoiding overfitting, an interesting approach is to assess the prediction quality of models when applied to new data. The following R script presents a function for calculating various scores:\n\n```{r, eval=TRUE, cache=TRUE}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\nScore.pred <- function(mu, x) {\n  Sc.log  <- -sum(dpois(x, mu, log=TRUE))\n  Sc.MSE  <- sum((x - mu)^2)\n  Sc.quad <- sum(-2*dpois(x,lambda=mu) + sapply(mu, function(x){ sum(dpois(0:10,lambda=x)^2) }))\n  Sc.sph <- sum(- dpois(x,mu) / sqrt(sapply(mu, function(x){ sum(dpois(0:10,lambda=x)^2) })))\n  Sc.DSS <- sum(dss_pois(x, mu))\n  Sc.CRPS <- sum(crps_pois(x, mu))\n    \n  return(c(Sc.log, Sc.MSE, Sc.quad, Sc.sph, Sc.DSS, Sc.CRPS))\n}\n\n\n```\n\n\n\n### Packages\n\nHere is the list of packages that will be used:\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\nlibrary(tidyverse)\nlibrary(vtable)\nlibrary(rpart)\nlibrary(repr)\nlibrary(rpart.plot)\nlibrary(rfCountData)\nlibrary(gam)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(glmnet)\nlibrary(scoringRules)\nlibrary(sjPlot)\n\n```\n\n### Data\n\nThe analyses in this chapter will be conducted using the same data as in the previous chapter. However, as we concluded at the end of our overview of the data, a transformation of certain variables is also necessary.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\ndataS <- read.csv('Data/Synthetic.csv')\n\n# Modifications \ndataS <- dataS %>%\n  mutate(Territory = as.factor(Territory)) %>%\n  select(-c('Annual.pct.driven', 'Annual.miles.drive'))\ndata.select <- dataS\n\n# Train-test \nset.seed(123)\ntrain <- data.select %>% sample_frac(0.8, replace = FALSE)\ntest <- data.select %>% anti_join(train)\n\n# Modif data\ntrain2 <- train %>%\n  mutate(Miles.per.day = Total.miles.driven/Duration,\n         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),\n         expo = Duration/365.25)\n\ntransform.fct <- function(var){\n  df <- train2 %>% mutate(var_ = get(var)*Total.miles.driven/(1000*Duration))\n  q99 <- quantile(df$var_, 0.99)\n  df <- df %>% mutate(var_ = ifelse(var_ > q99, q99, var_))\n  #colnames(df)[ncol(df)] <- paste0(var, '_')\n  return(df)\n}\n\ntrain2 <- transform.fct(\"Brake.06miles\")\ntrain2 <- transform.fct(\"Brake.08miles\")\ntrain2 <- transform.fct(\"Brake.09miles\")\ntrain2 <- transform.fct(\"Brake.11miles\")\ntrain2 <- transform.fct(\"Brake.14miles\")\ntrain2 <- transform.fct(\"Accel.06miles\")\ntrain2 <- transform.fct(\"Accel.08miles\")\ntrain2 <- transform.fct(\"Accel.09miles\")\ntrain2 <- transform.fct(\"Accel.11miles\")\ntrain2 <- transform.fct(\"Accel.12miles\")\ntrain2 <- transform.fct(\"Accel.14miles\")\ntrain2 <- transform.fct(\"Left.turn.intensity08\")\ntrain2 <- transform.fct(\"Left.turn.intensity09\")\ntrain2 <- transform.fct(\"Left.turn.intensity10\")\ntrain2 <- transform.fct(\"Left.turn.intensity11\")\ntrain2 <- transform.fct(\"Left.turn.intensity12\")\ntrain2 <- transform.fct(\"Right.turn.intensity08\")\ntrain2 <- transform.fct(\"Right.turn.intensity09\")\ntrain2 <- transform.fct(\"Right.turn.intensity10\")\ntrain2 <- transform.fct(\"Right.turn.intensity11\")\ntrain2 <- transform.fct(\"Right.turn.intensity12\")\n\n# Create folds\nnb.fold <- 5\nfold <- sample(1:nb.fold, nrow(train2), replace = TRUE)\ntrain2$fold <- fold\n\n##\n\ntest2 <- test %>%\n  mutate(Miles.per.day = Total.miles.driven/Duration,\n         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),\n         expo = Duration/365.25)\n\ntransform.fct <- function(var){\n  df <- test2 %>% mutate(var_ = get(var)*Total.miles.driven/(1000*Duration))\n  q99 <- quantile(df$var_, 0.99)\n  df <- df %>% mutate(var_ = ifelse(var_ > q99, q99, var_))\n  #colnames(df)[ncol(df)] <- paste0(var, '_')\n  return(df)\n}\n\ntest2 <- transform.fct(\"Brake.06miles\")\ntest2 <- transform.fct(\"Brake.08miles\")\ntest2 <- transform.fct(\"Brake.09miles\")\ntest2 <- transform.fct(\"Brake.11miles\")\ntest2 <- transform.fct(\"Brake.14miles\")\ntest2 <- transform.fct(\"Accel.06miles\")\ntest2 <- transform.fct(\"Accel.08miles\")\ntest2 <- transform.fct(\"Accel.09miles\")\ntest2 <- transform.fct(\"Accel.11miles\")\ntest2 <- transform.fct(\"Accel.12miles\")\ntest2 <- transform.fct(\"Accel.14miles\")\ntest2 <- transform.fct(\"Left.turn.intensity08\")\ntest2 <- transform.fct(\"Left.turn.intensity09\")\ntest2 <- transform.fct(\"Left.turn.intensity10\")\ntest2 <- transform.fct(\"Left.turn.intensity11\")\ntest2 <- transform.fct(\"Left.turn.intensity12\")\ntest2 <- transform.fct(\"Right.turn.intensity08\")\ntest2 <- transform.fct(\"Right.turn.intensity09\")\ntest2 <- transform.fct(\"Right.turn.intensity10\")\ntest2 <- transform.fct(\"Right.turn.intensity11\")\ntest2 <- transform.fct(\"Right.turn.intensity12\")\n\n```\n\n:::\n\n\n\n## Basic GLM Models\n\n::: {.panel-tabset}\n\n### Single intercept\n\nA baseline model corresponding to a Generalized Linear Model (GLM) with intercept and predicting for each contract only the observed mean multiplied by the exposure is used as a point of comparison.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_base\n#| tbl-cap: Prediction scores for the base model\n\n## Model on each fold\nResult_  <- data.frame()\nResult2_  <- data.frame()\nfor(i in 1:nb.fold) {\n    learn <- train2[train2$fold != i,]\n    valid <- train2[train2$fold == i,]\n\n    mean <- sum(learn$NB_Claim)/sum(learn$expo) \n    learn$pred.base <- mean*learn$expo\n    valid$pred.base <- mean*valid$expo\n\n    Result_ <- rbind(Result_, c(i, Score.pred(valid$pred.base, valid$NB_Claim)/nrow(valid)))\n    Result2_ <- rbind(Result2_, c(i, Score.pred(valid$pred.base, valid$NB_Claim)))\n}\n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ncolnames(Result2_) <- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ntot <- colSums(Result2_)/nrow(train2)\ntot$Fold <- 'Total'\nResult_ <- rbind(Result_ , tot)\n\nResult.base <- Result_  \nBase <- Result.base[nb.fold+1,]\n\nknitr::kable(Result_, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n```\n\nThe model is then estimated on the entire training set and predicted on the *test* set, which was not used in parameter calibration.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_basetest\n#| tbl-cap: Prediction scores for the base model (testing set) \n\nmean <- sum(train2$NB_Claim)/sum(train2$expo) \ntest2$pred.base <- mean*test2$expo\n\nResult_ <- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ <- cbind('Base', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all <- Result_\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n```\n\n### Categorical covariates\n\nA first regression approach is attempted using only the traditional categorical variables, namely:    \n\n  - Insured's gender,  \n  - Marital status,  \n  - Vehicle usage,  \n  - Region.\n\nEven though territory should also be considered since it consists of more than fifty different factors, it will not be integrated into the model immediately. As we saw in the overview of variables in a previous section, the insured's gender did not appear to be an important variable for predicting the number of claims. This GLM approach confirms this observation. Therefore, this variable is excluded from the model. In the table below, we can see the impact of adding traditional variables on the prediction quality.\n\nBelow are the prediction scores of the model with all categorical covariates. As expected, the addition of segmentation variables improves the prediction scores compared to the simple baseline model with only an intercept.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_GLM1\n#| tbl-cap: Prediction scores for the GLM1 model\n\n## Model \nscore.base <- as.formula(NB_Claim ~ 1 + offset(log(expo)))\nscore.glm <- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo)))\n\n## Model on each fold\nResult_  <- data.frame()\nResult2_  <- data.frame()\nfor(i in 1:nb.fold) {\n    learn <- train2[train2$fold != i,]\n    valid <- train2[train2$fold == i,]\n    glm.fit <- glm(score.glm, family = poisson(), data = learn)\n\n    learn$pred.base <- predict(glm.fit, newdata=learn, type='response')\n    valid$pred.base <- predict(glm.fit, newdata=valid, type='response')\n\n    Result_ <- rbind(Result_, c(i, Score.pred(valid$pred.base, valid$NB_Claim)/nrow(valid)))\n    Result2_ <- rbind(Result2_, c(i, Score.pred(valid$pred.base, valid$NB_Claim)))\n}\n\n## Model on all data from train\nglm.base <- glm(score.base, family = poisson(), data = train2)\nglm.fit <- glm(score.glm, family = poisson(), data = train2)\ntrain2$pred.glm1 <- predict(glm.fit, newdata=train2, type='response')\nResult.glm1 <- Result_  \n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ncolnames(Result2_) <- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ntot <- colSums(Result2_)/nrow(train2)\ntot$Fold <- 'Total'\nResult_ <- rbind(Result_ , tot)\nResult_ <- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] <- 'Improvement'\n\nfor(i in 2:7){\n  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) <- NULL\nknitr::kable(Result_, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n```\n\nThe comparison with the test dataset is also indicated in the table below. It shows that adding traditional variables does not substantially enhance prediction on the test dataset.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_basetest2\n#| tbl-cap: Prediction scores for the GLM model with traditional covariates (testing set) \n\nscore.glm <- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo)))\n\nglm.fit <- glm(score.glm, family = poisson(), data = train2)\ntest2$pred.base <- predict(glm.fit, newdata=test2, type='response')\n\nResult_ <- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ <- cbind('GLM (trad.)', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n```\n\n### Estimated Parameters\n\nThe table below shows the estimators obtained for the GLM-Poisson approach and compares them with the baseline model having only an intercept.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-CoeffGLM\n#| tbl-cap: Estimated parameters for the GLM1 model\n\n## Model \nscore.base <- as.formula(NB_Claim ~ 1 + offset(log(expo)))\nscore.glm <- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo)))\n\n## Model on all data from train\nglm.base <- glm(score.base, family = poisson(), data = train2)\nglm.fit <- glm(score.glm, family = poisson(), data = train2)\n\ntab_model(glm.base, glm.fit, transform = NULL)\n\n```\n\n\n:::\n\n\n## GLM-Net\n\nAs we saw in the previous chapter, a series of traditional continuous segmentation variables is also available:  \n\n  - Credit score,  \n  - Age of the insured,  \n  - Age of the vehicle,  \n  - Number of claim-free years, \n  - Furthermore, as we will explain later, the territory will also be treated as a continuous variable.\n\nDirectly using a continuous variable in a GLM is usually ineffective as it assumes a linear relationship. To avoid overfitting the data, an approach using splines, utilizing the Generalized Additive Models (GAM) theory, is interesting. This approach allows visualizing the general form of the covariate to explain the number of claims. A parametric form can then be proposed to achieve the best possible correspondence with the spline obtained by the GAM.\n\nSubsequently, instead of attempting to fit a basic GLM model with all variables, we will work with a GLM-net model that allows for variable selection.\n\n### Parametric transformation of continuous covariates\n\n::: {.panel-tabset}\n\n### Credit Score\n\nThe first covariate studied is the credit score. We include all categorical variables in the analysis and apply a spline approach with a GAM.  The spline analysis indicates that the following parametric form appears to be appropriate for capturing the relationship between the number of claims:\n\n$$s(Credit.Score) \\approx Credit.Score + Credit.Score^2$$\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: fig-CS_GAM\n#| fig-cap: \"Smoothing of the credit score\"\n#| fig-width: 9\n#| fig-height: 4\n\nmin_ <- min(train2$Credit.score) \nmax_ <- max(train2$Credit.score) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'Credit.score'\n\ndb <- train2 %>%\n  select(-'Credit.score') %>%\n  slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo))\n                        + s(Credit.score) )\nscore.glm <- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo))\n                        + Credit.score +  I(Credit.score^2) )\n\ngam.fit <- gam(score.gam, family = poisson(), data = train2)\nglm.fit <- glm(score.glm, family = poisson(), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')\nbase <- db %>%\n  mutate(diff = abs(Credit.score - mean(train2$Credit.score))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=Credit.score, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=Credit.score, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Credit Score',\n       y = 'Relativity') +\n  theme_classic()\n\n\n\n```\n\n\n### Age of the insured\n\n\nA spline to examine the relationship between the age of the insured and the claim frequency has also been produced. The most appropriate parametric form is as follows:\n\n$$s(Insured.age) \\approx Insured.age + \\log(Insured.age) + Insured.age^2$$\n\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: fig-IA_GAM\n#| fig-cap: \"Smoothing of the age of the insured\"\n#| fig-width: 9\n#| fig-height: 4\n\nmin_ <- min(train2$Insured.age) \nmax_ <- max(train2$Insured.age) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'Insured.age'\n\ndb <- train2 %>%\n  select(-'Insured.age') %>%\n  slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo))\n                        + s(Insured.age) )\nscore.glm <- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo))\n                        + Insured.age +  log(Insured.age) + I(Insured.age^2) )\n\ngam.fit <- gam(score.gam, family = poisson(), data = train2)\nglm.fit <- glm(score.glm, family = poisson(), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')\nbase <- db %>%\n  mutate(diff = abs(Insured.age - mean(train2$Insured.age))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=Insured.age, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=Insured.age, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Age of the insured',\n       y = 'Relativity') +\n  theme_classic()\n\n\n\n```\n\n### Age of the car\n\nThe most appropriate parametric form is as follows:\n\n$$s(Car.age) \\approx Car.age + Car.age^2$$\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: fig-CA_GAM\n#| fig-cap: \"Smoothing of the age of the car\"\n#| fig-width: 9\n#| fig-height: 4\n\n\nmin_ <- min(train2$Car.age) \nmax_ <- max(train2$Car.age) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'Car.age'\n\ndb <- train2 %>%\n  select(-'Car.age') %>%\n  slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo))\n                        + s(Car.age) )\nscore.glm <- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo))\n                        + Car.age + I(Car.age^2) )\n\ngam.fit <- gam(score.gam, family = poisson(), data = train2)\nglm.fit <- glm(score.glm, family = poisson(), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')\nbase <- db %>%\n  mutate(diff = abs(Car.age - mean(train2$Car.age))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=Car.age, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=Car.age, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Age of the car',\n       y = 'Relativity') +\n  theme_classic()\n\n\n\n```\n\n\n### Years without claims\n\nFinally, the proposed parametric form for the covariate  indicating the years without claim is:\n\n$$s(Years.noclaims) \\approx Years.noclaims + Years.noclaims^2 + Years.noclaims^3$$\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: fig-YNC_GAM\n#| fig-cap: \"Smoothing of years without claim\"\n#| fig-width: 9\n#| fig-height: 4\n\n\nmin_ <- min(train2$Years.noclaims) \nmax_ <- max(train2$Years.noclaims) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'Years.noclaims'\n\ndb <- train2 %>%\n  select(-'Years.noclaims') %>%\n  slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo))\n                        + s(Years.noclaims) )\nscore.glm <- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo))\n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) )\n\ngam.fit <- gam(score.gam, family = poisson(), data = train2)\nglm.fit <- glm(score.glm, family = poisson(), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')\nbase <- db %>%\n  mutate(diff = abs(Years.noclaims - mean(train2$Years.noclaims))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=Years.noclaims, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=Years.noclaims, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Years.noclaims',\n       y = 'Relativity') +\n  theme_classic()\n\n\n\n```\n\n### Territory\n\nAs we saw in the previous chapter, the insured's territory code corresponds to a categorical variable with a large cardinality. In such a situation, creating a binary variable for each possible territory is not appropriate. Instead, we propose using target encoding based on the territory's rank.\n\nThis means that we first calculate the observed frequency for each territory. Then, we rank the frequencies for the 53 territories in the database. Next, the rank divided by 53 corresponds to the numerical value of the territory. This form is called rank-encoding. We believe that this transformation is justified, considering that ranking territories according to risk is an approach that could be taken in insurance companies.\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| fig-width: 9\n#| fig-height: 4\n\n\n# Mean Encoding with White Noise pour les territoires\ncardi <- length(unique(train$Territory))\n\nenc.terr <- train2 %>%\n  group_by(Territory) %>%\n  summarize(freq = sum(NB_Claim)/sum(expo)) %>%\n  arrange(freq) %>%\n  mutate(terr.code= row_number()/(cardi+1)) %>%\n  select(Territory, terr.code)\n\ntrain2 <- train2 %>%\n  group_by(Territory) %>%\n  left_join(enc.terr, by='Territory') %>%\n  ungroup()\n\ntest2 <- test2 %>%\n  group_by(Territory) %>%\n  left_join(enc.terr, by='Territory') %>%\n  ungroup()\n\n```\n\nWith the encoded form of the territory, as has been done with the other continuous variables, we propose a parametric form for the spline obtained:\n\n$$s(terr.code) \\approx terr.code + terr.code^2 + terr.code^3$$\n\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: fig-terrcode_GAM\n#| fig-cap: \"Smoothing of the territories (encoded)\"\n#| fig-width: 9\n#| fig-height: 4\n\n\nmin_ <- min(train2$terr.code) \nmax_ <- max(train2$terr.code) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'terr.code'\n\ndb <- train2 %>%\n  select(-'terr.code') %>%\n  slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo))\n                        + s(terr.code) )\nscore.glm <- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo))\n                        + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\ngam.fit <- gam(score.gam, family = poisson(), data = train2)\nglm.fit <- glm(score.glm, family = poisson(), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')\nbase <- db %>%\n  mutate(diff = abs(terr.code - mean(train2$terr.code))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=terr.code, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=terr.code, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'terr.code',\n       y = 'Relativity') +\n  theme_classic()\n\n\n\n```\n\n:::\n\n\n\n### Fitting the GLM-Net model\n\n```{r}\n#| echo: false\n#| eval: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\nglm.score <- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo))\n                                  + Credit.score +  I(Credit.score^2) \n                                  + Insured.age +  log(Insured.age) + I(Insured.age^2) \n                                  + Car.age + I(Car.age^2) \n                                  + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$NB_Claim\noffset <- log(train2$expo)\nfold.id <- train2$fold\n\nlambda_seq <- c(10^seq(0, -8, by = -.1), 0)\ncvfit0  <- cv.glmnet(matrix.x, y, relax=FALSE, family = \"poisson\", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0)\ncvfit.2 <- cv.glmnet(matrix.x, y, relax=FALSE, family = \"poisson\", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.2)\ncvfit.4 <- cv.glmnet(matrix.x, y, relax=FALSE, family = \"poisson\", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.4)\ncvfit.6 <- cv.glmnet(matrix.x, y, relax=FALSE, family = \"poisson\", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.6)\ncvfit.8 <- cv.glmnet(matrix.x, y, relax=FALSE, family = \"poisson\", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 0.8)\ncvfit1  <- cv.glmnet(matrix.x, y, relax=FALSE, family = \"poisson\", offset = offset, lambda = lambda_seq, foldid = fold, alpha = 1)\n\nc(cvfit0$lambda.min, cvfit.2$lambda.min, cvfit.4$lambda.min, cvfit.6$lambda.min, cvfit.8$lambda.min, cvfit1$lambda.min)\n\nall.min <- data.frame(c(min(cvfit0$cvm), min(cvfit.2$cvm), min(cvfit.4$cvm), min(cvfit.6$cvm), min(cvfit.8$cvm), min(cvfit1$cvm))) %>%\n  mutate(alpha = 2*(row_number()-1)/10)\ncolnames(all.min)[1] <- 'min' \nall.min %>% filter(min == min(min))\n\ncvfit1$lambda.min\ncvfit1$lambda.1se\n\n```\n\n::: {.panel-tabset}\n\n### Optimal value\n\nThe parameters of the GLM-net were calibrated using cross-validation to obtain the model's hyperparameters. Using these values, we can calculate the prediction scores of the model based on all covariates.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_GLMnet1\n#| tbl-cap: Prediction scores for the GLM-net model (alpha=1)\n\nglm.score <- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo))\n                                  + Credit.score +  I(Credit.score^2) \n                                  + Insured.age +  log(Insured.age) + I(Insured.age^2) \n                                  + Car.age + I(Car.age^2) \n                                  + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nResult_  <- data.frame()\nResult2_  <- data.frame()\nfor(i in 1:nb.fold) {\n    learn <- train2[train2$fold != i,]\n    valid <- train2[train2$fold == i,]\n    \n    matrix.x <- model.matrix(glm.score, data=learn)[,-1]\n    y <- learn$NB_Claim\n    offset <- log(learn$expo)\n\n    lambda.min <- 0\n    lambda.1se <- 0.006309573\n    \n    lambda.select <- lambda.min\n    fit <- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\n    #fit <- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n  \n    matrix.x <- model.matrix(glm.score, data=valid)[,-1]\n    y <- valid$NB_Claim\n    offset <- log(valid$expo)\n\n    valid$pred <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)\n    \n    Result_ <- rbind(Result_, c(i, Score.pred(valid$pred, valid$NB_Claim)/nrow(valid)))\n    Result2_ <- rbind(Result2_, c(i, Score.pred(valid$pred, valid$NB_Claim)))\n}\n\n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ncolnames(Result2_) <- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ntot <- colSums(Result2_)/nrow(train2)\ntot$Fold <- 'Total'\nResult_ <- rbind(Result_ , tot)\nResult_ <- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] <- 'Improvement'\n\nfor(i in 2:7){\n  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) <- NULL\nknitr::kable(Result_, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n```\n\nOn the test dataset, we obtain:\n\n```{r}\n#| echo: true\n#| cache: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_basetest3\n#| tbl-cap: Prediction scores for the GLM-net model  (testing set) \n\nglm.score <- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo))\n                                  + Credit.score +  I(Credit.score^2) \n                                  + Insured.age +  log(Insured.age) + I(Insured.age^2) \n                                  + Car.age + I(Car.age^2) \n                                  + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$NB_Claim\noffset <- log(train2$expo)\n\nlambda.min <- 0\nlambda.1se <- 0.006309573\n    \nlambda.select <- lambda.min\nfit <- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\n#fit <- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n  \nmatrix.x <- model.matrix(glm.score, data=test2)[,-1]\ny <- test2$NB_Claim\noffset <- log(test2$expo)\n\ntest2$pred.base <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)\n\nResult_ <- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ <- cbind('LASSO (optimal)', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n```\n\n\n\n### Parsimonious model\n\nInstead of using the optimal value of the penalty $\\lambda$  in the elastic-net approach, it is often advised to use a penalty value located at one standard error ($\\lambda_{1se}$). This helps to obtain a more parsimonious model. The prediction scores of such a model are displayed below.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_GLMnet2\n#| tbl-cap: Prediction scores for the GLM-net model (alpha=1)\n\nResult_  <- data.frame()\nResult2_  <- data.frame()\nfor(i in 1:nb.fold) {\n    learn <- train2[train2$fold != i,]\n    valid <- train2[train2$fold == i,]\n    \n    matrix.x <- model.matrix(glm.score, data=learn)[,-1]\n    y <- learn$NB_Claim\n    offset <- log(learn$expo)\n\n    lambda.min <- 0\n    lambda.1se <- 0.006309573\n    \n    lambda.select <- lambda.1se\n    #fit <- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\n    fit <- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n  \n    matrix.x <- model.matrix(glm.score, data=valid)[,-1]\n    y <- valid$NB_Claim\n    offset <- log(valid$expo)\n\n    valid$pred <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)\n    \n    Result_ <- rbind(Result_, c(i, Score.pred(valid$pred, valid$NB_Claim)/nrow(valid)))\n    Result2_ <- rbind(Result2_, c(i, Score.pred(valid$pred, valid$NB_Claim)))\n}\n\n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ncolnames(Result2_) <- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\ntot <- colSums(Result2_)/nrow(train2)\ntot$Fold <- 'Total'\nResult_ <- rbind(Result_ , tot)\nResult_ <- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] <- 'Improvement'\n\nfor(i in 2:7){\n  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) <- NULL\nknitr::kable(Result_, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n```\n\nOn the test dataset, we obtain:\n\n```{r}\n#| echo: true\n#| cache: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_basetest3bb\n#| tbl-cap: Prediction scores for the GLM-net model  (testing set) \n\nglm.score <- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo))\n                                  + Credit.score +  I(Credit.score^2) \n                                  + Insured.age +  log(Insured.age) + I(Insured.age^2) \n                                  + Car.age + I(Car.age^2) \n                                  + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$NB_Claim\noffset <- log(train2$expo)\n\nlambda.min <- 0\nlambda.1se <- 0.006309573\n    \nlambda.select <- lambda.1se\n#fit <- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.select)\nfit <- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.select)\n  \nmatrix.x <- model.matrix(glm.score, data=test2)[,-1]\ny <- test2$NB_Claim\noffset <- log(test2$expo)\n\ntest2$pred.base <- predict(fit, newx = matrix.x, type='response', newoffset=offset, lambda = lambda.select)\n\nResult_ <- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ <- cbind('LASSO (parsimonious)', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n\n```\n\n\n### Categorical covariates\n\nFor categorical variables, the relativity values obtained for both GLM-net approaches are displayed below.\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| layout-ncol: 2\n#| layout-nrow: 2\n#| label: fig-GLMnetcat\n#| fig-cap: \"Interpretation of the categorical variables from the GLM-net model\"\n#| fig-subcap: \n#|   - \"Sex of the insured\"\n#|   - \"Marital status of the insured\"\n#|   - \"Car use\"\n#|   - \"Region\"\n\n\nglm.score <- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo))\n                                  + Credit.score +  I(Credit.score^2) \n                                  + Insured.age +  log(Insured.age) + I(Insured.age^2) \n                                  + Car.age + I(Car.age^2) \n                                  + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$NB_Claim\noffset <- log(train2$expo)\n    \nlambda.min <- 0\nlambda.1se <- 0.006309573\n\nlasso.min <- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.min)\nlasso.1se <- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.1se)\n#cbind(coef(lasso.min),coef(lasso.1se))\n\n### Insured.sex ###\nFemale.min <- 1\nFemale.1se <- 1\nMale.min <- exp(coef(lasso.min)[2])\nMale.1se <- exp(coef(lasso.1se)[2])\n\ndf <- data.frame( Sex = c('Female', 'Male'), \n                  Relativity = c(Female.min, Male.min) ) \ndf2 <- data.frame( Sex = c('Female', 'Male'), \n                  Relativity = c(Female.1se, Male.1se) ) \nggplot() + \n  geom_line(aes(x=as.factor(Sex), y=Relativity, group=1, color='lambda.min'), stat='identity', data=df)+\n  geom_point(aes(x=as.factor(Sex), y=Relativity, group=1, color='lambda.min'), stat='identity', data=df)+\n  geom_line(aes(x=as.factor(Sex), y=Relativity, group=1, color='lambda.1se'), stat='identity', data=df2)+\n  geom_point(aes(x=as.factor(Sex), y=Relativity, group=1, color='lambda.1se'), stat='identity', data=df2)+\n  labs(x = 'Sex of the insured',\n       y = 'Relativity') +\n  ylim(0, 1.2*max(df$Relativity))+\n  guides(color = guide_legend(title = \"\")) +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Marital ###\nSingle.min <- 1\nSingle.1se <- 1\nMarried.min <- exp(coef(lasso.min)[3])\nMarried.1se <- exp(coef(lasso.1se)[3])\n\ndf <- data.frame( Marital = c('Married', 'Single'), \n                  Relativity = c(Single.min, Married.min) ) \ndf2 <- data.frame( Marital = c('Married', 'Single'), \n                  Relativity = c(Single.1se, Married.1se) ) \nggplot() + \n  geom_line(aes(x=as.factor(Marital), y=Relativity, group=1, color='lambda.min'), stat='identity', data=df)+\n  geom_point(aes(x=as.factor(Marital), y=Relativity, group=1, color='lambda.min'), stat='identity', data=df)+\n  geom_line(aes(x=as.factor(Marital), y=Relativity, group=1, color='lambda.1se'), stat='identity', data=df2)+\n  geom_point(aes(x=as.factor(Marital), y=Relativity, group=1, color='lambda.1se'), stat='identity', data=df2)+\n  labs(x = 'Marital status of the insured',\n       y = 'Relativity') +\n  ylim(0, 1.2*max(df$Relativity))+\n  guides(color = guide_legend(title = \"\")) +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Car Use ###\nCommercial.min <- 1\nCommute.min <- exp(coef(lasso.min)[4])\nFarmer.min <- exp(coef(lasso.min)[5])\nPrivate.min <- exp(coef(lasso.min)[6])\n\nCommercial.1se <- 1\nCommute.1se <- exp(coef(lasso.1se)[4])\nFarmer.1se <- exp(coef(lasso.1se)[5])\nPrivate.1se <- exp(coef(lasso.1se)[6])\n\ndf <- data.frame( Car.use = c('Commercial', 'Commute', 'Farmer', 'Private'), \n                  Relativity = c(Commercial.min, Commute.min, Farmer.min, Private.min) ) \ndf2 <- data.frame( Car.use = c('Commercial', 'Commute', 'Farmer', 'Private'), \n                  Relativity = c(Commercial.1se, Commute.1se, Farmer.1se, Private.1se) ) \nggplot() + \n  geom_line(aes(x=as.factor(Car.use), y=Relativity, group=1, color='lambda.min'), stat='identity', data=df)+ \n  geom_point(aes(x=as.factor(Car.use), y=Relativity, group=1, color='lambda.min'), stat='identity', data=df)+\n  geom_line(aes(x=as.factor(Car.use), y=Relativity, group=1, color='lambda.1se'), stat='identity', data=df2)+ \n  geom_point(aes(x=as.factor(Car.use), y=Relativity, group=1, color='lambda.1se'), stat='identity', data=df2)+\n  labs(x = 'Use of the car',\n       y = 'Relativity') +\n  ylim(0, 1.2*max(df$Relativity))+\n  guides(color = guide_legend(title = \"\")) +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n### Region ###\nRural.min <- 1\nRural.1se <- 1\nUrban.min <- exp(coef(lasso.min)[7])\nUrban.1se <- exp(coef(lasso.1se)[7])\n\ndf <- data.frame( Region = c('Rural', 'Urban'), \n                  Relativity = c(Rural.min, Urban.min) ) \ndf2 <- data.frame( Region = c('Rural', 'Urban'), \n                  Relativity = c(Rural.1se, Urban.1se) ) \nggplot() + \n  geom_line(aes(x=as.factor(Region), y=Relativity, group=1, color='lambda.min'), stat='identity', data=df)+\n  geom_point(aes(x=as.factor(Region), y=Relativity, group=1, color='lambda.min'), stat='identity', data=df)+\n  geom_line(aes(x=as.factor(Region), y=Relativity, group=1, color='lambda.1se'), stat='identity', data=df2)+\n  geom_point(aes(x=as.factor(Region), y=Relativity, group=1, color='lambda.1se'), stat='identity', data=df2)+\n  labs(x = 'Region',\n       y = 'Relativity') +\n  ylim(0, 1.2*max(df$Relativity))+\n  guides(color = guide_legend(title = \"\")) +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n\n```\n\n\n### Continuous covariates\n\nAs with categorical variables, the relativities obtained are illustrated below for continuous variables. It can be observed that the parsimonious approach tends to reduce the impact of segmentation variables on the premium.\n\n\n```{r}\n#| echo: true\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| layout-ncol: 2\n#| layout-nrow: 3\n#| label: fig-GLMnet\n#| fig-cap: \"Interpretation of the continuous variables from the GLM-net model:\"\n#| fig-subcap: \n#|   - \"Credit Score\"\n#|   - \"Age of the insured\"\n#|   - \"Age of the car\"\n#|   - \"Years without claim\"\n#|   - \"Territory Code\"\n\nglm.score <- as.formula(NB_Claim ~ Insured.sex + Marital  +  Car.use + Region + offset(log(expo))\n                                  + Credit.score +  I(Credit.score^2) \n                                  + Insured.age +  log(Insured.age) + I(Insured.age^2) \n                                  + Car.age + I(Car.age^2) \n                                  + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$NB_Claim\noffset <- log(train2$expo)\n\nlambda.min <- 0\nlambda.1se <- 0.006309573\n\nlasso.min <- glmnet(matrix.x, y, family = \"poisson\", relax=FALSE, offset = offset, alpha = 1, lambda = lambda.min)\nlasso.1se <- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 1, lambda = lambda.1se)\n#cbind(coef(lasso.min), coef(lasso.1se))\n\n### Credit Score ###\nCredit.score <- seq(from=min(train2$Credit.score), to=max(train2$Credit.score), by=1)\n\nbeta <- coef(lasso.1se)[8:9]\ncurve1 <- exp(beta[1]*Credit.score + beta[2]*Credit.score^2) \nbase1 <- exp(beta[1]*mean(train2$Credit.score) + beta[2]*mean(train2$Credit.score)^2) \n\nbeta <- coef(lasso.min)[8:9]\ncurve2 <- exp(beta[1]*Credit.score + beta[2]*Credit.score^2) \nbase2 <- exp(beta[1]*mean(train2$Credit.score) + beta[2]*mean(train2$Credit.score)^2) \n\ncurve1 <- curve1/base1\ncurve2 <- curve2/base2\ndb <- data.frame(cbind(Credit.score, curve1, curve2))\n\nggplot()+\n  geom_line(aes(x=Credit.score, y=curve1, color = 'lambda.1se' ), data=db)+\n  geom_line(aes(x=Credit.score, y=curve2, color = 'lambda.min' ), data=db)+\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Credit Score',\n       y = 'Relativity') +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Insured.age \nInsured.age <- seq(from=min(train2$Insured.age ), to=max(train2$Insured.age ), by=1)\nbeta <- coef(lasso.1se)[10:12]\ncurve1 <- exp(beta[1]*Insured.age  + beta[2]*log(Insured.age ) + beta[3]*Insured.age^2)       \nbase1  <- exp(beta[1]*mean(train2$Insured.age) + beta[2]*log(mean(train2$Insured.age)) + beta[3]*mean(train2$Insured.age)^2) \n\nbeta <- coef(lasso.min)[10:12]\ncurve2 <- exp(beta[1]*Insured.age  + beta[2]*log(Insured.age ) + beta[3]*Insured.age^2)       \nbase2  <- exp(beta[1]*mean(train2$Insured.age) + beta[2]*log(mean(train2$Insured.age)) + beta[3]*mean(train2$Insured.age)^2) \n\ncurve1 <- curve1/base1\ncurve2 <- curve2/base2\ndb <- data.frame(cbind(Insured.age, curve1, curve2))\n\nggplot()+\n  geom_line(aes(x=Insured.age, y=curve1, color = 'lambda.1se' ), data=db)+\n  geom_line(aes(x=Insured.age, y=curve2, color = 'lambda.min' ), data=db)+\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Age of the insured',\n       y = 'Relativity') +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Car Age ###\nCar.age <- seq(from=min(train2$Car.age), to=max(train2$Car.age), by=1)\nbeta <- coef(lasso.1se)[13:14]\ncurve1 <- exp(beta[1]*Car.age + beta[2]*Car.age^2)\nbase1  <- exp(beta[1]*mean(train2$Car.age) + beta[2]*mean(train2$Car.age)^2) \n\nbeta <- coef(lasso.min)[13:14]\ncurve2 <- exp(beta[1]*Car.age + beta[2]*Car.age^2)\nbase2  <- exp(beta[1]*mean(train2$Car.age) + beta[2]*mean(train2$Car.age)^2) \n\ncurve1 <- curve1/base1\ncurve2 <- curve2/base2\ndb <- data.frame(cbind(Car.age, curve1, curve2))\n\nggplot()+\n  geom_line(aes(x=Car.age, y=curve1, color = 'lambda.1se' ), data=db)+\n  geom_line(aes(x=Car.age, y=curve2, color = 'lambda.min' ), data=db)+\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Age of the car',\n       y = 'Relativity') +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### Years.noclaims \nYears.noclaims <- seq(from=min(train2$Years.noclaims ), to=max(train2$Years.noclaims ), by=1)\nbeta <- coef(lasso.1se)[15:17]\ncurve1 <- exp(beta[1]*Years.noclaims  + beta[2]*Years.noclaims^2 + beta[3]*Years.noclaims ^3)        \nbase1  <- exp(beta[1]*mean(train2$Years.noclaims) + beta[2]*mean(train2$Years.noclaims)^2 + beta[3]*mean(train2$Years.noclaims)^3) \n\nbeta <- coef(lasso.min)[15:17]\ncurve2 <- exp(beta[1]*Years.noclaims  + beta[2]*Years.noclaims^2 + beta[3]*Years.noclaims ^3)        \nbase2  <- exp(beta[1]*mean(train2$Years.noclaims) + beta[2]*mean(train2$Years.noclaims)^2 + beta[3]*mean(train2$Years.noclaims)^3) \n\ncurve1 <- curve1/base1\ncurve2 <- curve2/base2\ndb <- data.frame(cbind(Years.noclaims, curve1, curve2))\n\nggplot()+\n  geom_line(aes(x=Years.noclaims, y=curve1, color = 'lambda.1se' ), data=db)+\n  geom_line(aes(x=Years.noclaims, y=curve2, color = 'lambda.min' ), data=db)+\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Years without claim',\n       y = 'Relativity') +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n### terr.code  \nterr.code  <- seq(from=min(train2$terr.code  ), to=max(train2$terr.code  ), by=0.01)\nbeta <- coef(lasso.1se)[18:20]\ncurve1 <- exp(beta[1]*terr.code + beta[2]*terr.code^2 + beta[3]*terr.code^3)\nbase1  <- exp(beta[1]*mean(train2$terr.code) + beta[2]*mean(train2$terr.code)^2 + beta[3]*mean(train2$terr.code)^3)\n\nbeta <- coef(lasso.min)[18:20]\ncurve2 <- exp(beta[1]*terr.code + beta[2]*terr.code^2 + beta[3]*terr.code^3)\nbase2  <- exp(beta[1]*mean(train2$terr.code) + beta[2]*mean(train2$terr.code)^2 + beta[3]*mean(train2$terr.code)^3)\n\ncurve1 <- curve1/base1\ncurve2 <- curve2/base2\ndb <- data.frame(cbind(terr.code, curve1, curve2))\n\nggplot()+\n  geom_line(aes(x=terr.code, y=curve1, color = 'lambda.1se' ), data=db)+\n  geom_line(aes(x=terr.code, y=curve2, color = 'lambda.min' ), data=db)+\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Territory (encoded)',\n       y = 'Relativity') +\n      theme_classic()+    theme(legend.position = 'bottom', legend.direction = \"horizontal\")\n\n\n```\n\n:::\n\n\n## XGBoost\n\n```{r}\n#| echo: false\n#| eval: false\n#| message: FALSE\n#| warning: FALSE\n\ntrad.vars <- c(\"Marital\", \"Car.use\", \"Region\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Car.age\", \"Years.noclaims\", \"Territory\") \n\ndtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(trad.vars)]), label = train2$NB_Claim)\nsetinfo(dtrain,\"base_margin\",log(train2$expo))\nfolds <-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n\nbounds <- list(eta = c(0.001, 0.5),\n               max_depth = c(1L, 50L),\n               subsample = c(0.1, 1),\n               min_child_weight = c(1, 175))\n\nobj_func <- function(eta, max_depth, subsample, min_child_weight) {\n  param <- list(\n    eta = eta,\n    max_depth = max_depth,\n    subsample = subsample,\n    #min_child_weight = min_child_weight,\n    booster = \"gbtree\",\n    objective = \"count:poisson\",\n    eval_metric = \"poisson-nloglik\")\n  \n  set.seed(333)\n  xgbcv <- xgb.cv(params = param,\n                  nrounds = base.rounds,\n                  data = dtrain,\n                  folds = folds,\n                  prediction = TRUE,\n                  early_stopping_rounds = 10,\n                  verbose = 0,\n                  maximize = F)\n  \n  lst <- list(\n    Score = -min(xgbcv$evaluation_log$test_poisson_nloglik_mean),\n    nrounds = xgbcv$best_iteration\n  )\n  \n  return(lst)\n}\n\nbase.rounds <- 200\nset.seed(1234)\nbayes_out <- bayesOpt(FUN = obj_func, bounds = bounds, initPoints = length(bounds) + 2, iters.n = 3)\ncomp <- bayes_out$scoreSummary[which(bayes_out$scoreSummary$Score== max(bayes_out$scoreSummary$Score))]\ncomp \n\n############\n## Verif ###\n############\nobj_func(eta=comp$eta, max_depth=comp$max_depth, subsample=comp$subsample, min_child_weight=comp$min_child_weight)\n\nparam <- list(\n  eta = 0.08846194,\n  max_depth = 43,\n  subsample = 0.8389601,\n  booster = \"gbtree\",\n  objective = \"count:poisson\",\n  eval_metric = \"poisson-nloglik\")\n\nset.seed(333)\nxgbcv <- xgb.cv(params = param,\n                nrounds = 96,\n                data = dtrain,\n                folds = folds,\n                prediction = TRUE,\n                early_stopping_rounds = 10,\n                verbose = 0,\n                maximize = F)\n\n-min(xgbcv$evaluation_log$test_poisson_nloglik_mean)\n\n\nparam <- list(\n  eta = 0.08846194,\n  max_depth = 43,\n  subsample = 0.8389601,\n  min_child_weight = 1,\n  booster = \"gbtree\",\n  objective = \"count:poisson\",\n  eval_metric = \"poisson-nloglik\")\n\nset.seed(333)\nxgbcv <- xgb.cv(params = param,\n                nrounds = 96,\n                data = dtrain,\n                folds = folds,\n                prediction = TRUE,\n                early_stopping_rounds = 10,\n                verbose = 0,\n                maximize = F)\n\n-min(xgbcv$evaluation_log$test_poisson_nloglik_mean)\n\n```\n\n\n\n```{r}\n#| echo: false\n#| eval: false\n#| message: FALSE\n#| warning: FALSE\n\ntrad.vars <- c(\"Marital\", \"Car.use\", \"Region\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Car.age\", \"Years.noclaims\", \"Territory\") \n\ndtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(trad.vars)]), label = train2$NB_Claim)\nsetinfo(dtrain,\"base_margin\",log(train2$expo))\nfolds <-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n\nbounds <- list(eta = c(0.08, 0.1),\n               max_depth = c(40L, 50L),\n               subsample = c(0.8, 1),\n               min_child_weight = c(1, 175))\n\nobj_func <- function(eta, max_depth, subsample, min_child_weight) {\n  param <- list(\n    eta = eta,\n    max_depth = max_depth,\n    subsample = subsample,\n    min_child_weight = min_child_weight,\n    booster = \"gbtree\",\n    objective = \"count:poisson\",\n    eval_metric = \"poisson-nloglik\")\n  \n  set.seed(233)\n  xgbcv <- xgb.cv(params = param,\n                  nrounds = base.rounds,\n                  data = dtrain,\n                  folds = folds,\n                  prediction = TRUE,\n                  early_stopping_rounds = 10,\n                  verbose = 0,\n                  maximize = F)\n  \n  lst <- list(\n    Score = -min(xgbcv$evaluation_log$test_poisson_nloglik_mean),\n    nrounds = xgbcv$best_iteration\n  )\n  \n  return(lst)\n}\n\nbase.rounds <- 200\nset.seed(1234)\nbayes_out <- bayesOpt(FUN = obj_func, bounds = bounds, initPoints = length(bounds) + 2, iters.n = 5)\ncomp <- bayes_out$scoreSummary[which(bayes_out$scoreSummary$Score== max(bayes_out$scoreSummary$Score))]\ncomp \n\n############\n## Verif ###\n############\nobj_func(eta=comp$eta, max_depth=comp$max_depth, subsample=comp$subsample, min_child_weight=comp$min_child_weight)\n\nparam <- list(\n  eta = 0.08846194,\n  max_depth = 43,\n  subsample = 0.8389601,\n  min_child_weight = min_child_weight,\n  booster = \"gbtree\",\n  objective = \"count:poisson\",\n  eval_metric = \"poisson-nloglik\")\n\nset.seed(233)\nxgbcv <- xgb.cv(params = param,\n                nrounds = 96,\n                data = dtrain,\n                folds = folds,\n                prediction = TRUE,\n                early_stopping_rounds = 10,\n                verbose = 0,\n                maximize = F)\n\n-min(xgbcv$evaluation_log$test_poisson_nloglik_mean)\n\n\n```\n\nAnother approach to consider is XGBoost. However, through cross-validation, this method requires finely tuning its hyperparameters to be effective. We utilized a grid search approach coupled with Bayesian optimization for the dataset used in the project.\n\n::: {.panel-tabset}\n\n### Prediction Scores\n\nWith the hyperparameters we discovered, we can compute the model's prediction scores. The scores obtained show a significant improvement compared to other tested approaches.\n\n```{r}\n#| echo: true\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| output: false\n#| code-fold: true\n\nlibrary(xgboost)\nlibrary(Ckmeans.1d.dp)\nlibrary(SHAPforxgboost)\n\ntrad.vars <- c(\"Marital\", \"Car.use\", \"Region\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Car.age\", \"Years.noclaims\", \"Territory\") \n\ndtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(trad.vars)]), label = train2$NB_Claim)\nsetinfo(dtrain,\"base_margin\",log(train2$expo))\nfolds <-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n\n```\n\n\n```{r}\n#| echo: true\n#| cache: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_XGBoost\n#| tbl-cap: Prediction scores for the XGBoost model\n\n\nparam <- list(\n  eta = 0.08846194,\n  max_depth = 43,\n  subsample = 0.8389601,\n  min_child_weight = 1,\n  booster = \"gbtree\",\n  objective = \"count:poisson\",\n  eval_metric = \"poisson-nloglik\")\n\nset.seed(333)\nxgbcv <- xgb.cv(params = param,\n                nrounds = 96,\n                data = dtrain,\n                folds = folds,\n                prediction = TRUE,\n                early_stopping_rounds = 10,\n                verbose = 0,\n                maximize = F)\n  \nSc.log <- sapply(xgbcv$folds, function(x){-dpois(train2$NB_Claim[x], unlist(xgbcv$pred[x]), log=TRUE)})\nSc.MSE <- sapply(xgbcv$folds, function(x){(train2$NB_Claim[x]-unlist(xgbcv$pred[x]))^2})\nSc.quad <- sapply(xgbcv$folds, function(x){\n  nb <- train2$NB_Claim[x]\n  mu <- unlist(xgbcv$pred[x])\n  -2*dpois(nb,lambda=mu) + dpois(0,lambda=mu)^2 + dpois(1,lambda=mu)^2 + dpois(2,lambda=mu)^2+ dpois(3,lambda=mu)^2 + dpois(4,lambda=mu)^2 + dpois(5,lambda=mu)^2 \n})  \nSc.sph <- sapply(xgbcv$folds, function(x){\n  nb <- train2$NB_Claim[x]\n  mu <- unlist(xgbcv$pred[x])\n  -dpois(nb,lambda=mu) / sqrt(dpois(0,lambda=mu)^2 + dpois(1,lambda=mu)^2 + dpois(2,lambda=mu)^2+ dpois(3,lambda=mu)^2 + dpois(4,lambda=mu)^2 + dpois(5,lambda=mu)^2 ) \n})\nSc.DSS <- sapply(xgbcv$folds, function(x){dss_pois(train2$NB_Claim[x], unlist(xgbcv$pred[x]))})\nSc.CRPS <- sapply(xgbcv$folds, function(x){crps_pois(train2$NB_Claim[x], unlist(xgbcv$pred[x]))})\n\nResult_  <- rbind(\nc(1,mean(Sc.log[1]$fold1), mean(Sc.MSE[1]$fold1), mean(Sc.quad[1]$fold1), mean(Sc.sph[1]$fold1), mean(Sc.DSS[1]$fold1), mean(Sc.CRPS[1]$fold1)),\nc(2,mean(Sc.log[2]$fold2), mean(Sc.MSE[2]$fold2), mean(Sc.quad[2]$fold2), mean(Sc.sph[2]$fold2), mean(Sc.DSS[2]$fold2), mean(Sc.CRPS[2]$fold2)),\nc(3,mean(Sc.log[3]$fold3), mean(Sc.MSE[3]$fold3), mean(Sc.quad[3]$fold3), mean(Sc.sph[3]$fold3), mean(Sc.DSS[3]$fold3), mean(Sc.CRPS[3]$fold3)),\nc(4,mean(Sc.log[4]$fold4), mean(Sc.MSE[4]$fold4), mean(Sc.quad[4]$fold4), mean(Sc.sph[4]$fold4), mean(Sc.DSS[4]$fold4), mean(Sc.CRPS[4]$fold4)),\nc(5,mean(Sc.log[5]$fold5), mean(Sc.MSE[5]$fold5), mean(Sc.quad[5]$fold5), mean(Sc.sph[5]$fold5), mean(Sc.DSS[5]$fold5), mean(Sc.CRPS[5]$fold5))\n)\n\nRes.sum  <- rbind(\nc(sum(Sc.log[1]$fold1), sum(Sc.MSE[1]$fold1), sum(Sc.quad[1]$fold1), sum(Sc.sph[1]$fold1), sum(Sc.DSS[1]$fold1), sum(Sc.CRPS[1]$fold1)),\nc(sum(Sc.log[2]$fold2), sum(Sc.MSE[2]$fold2), sum(Sc.quad[2]$fold2), sum(Sc.sph[2]$fold2), sum(Sc.DSS[2]$fold2), sum(Sc.CRPS[2]$fold2)),\nc(sum(Sc.log[3]$fold3), sum(Sc.MSE[3]$fold3), sum(Sc.quad[3]$fold3), sum(Sc.sph[3]$fold3), sum(Sc.DSS[3]$fold3), sum(Sc.CRPS[3]$fold3)),\nc(sum(Sc.log[4]$fold4), sum(Sc.MSE[4]$fold4), sum(Sc.quad[4]$fold4), sum(Sc.sph[4]$fold4), sum(Sc.DSS[4]$fold4), sum(Sc.CRPS[4]$fold4)),\nc(sum(Sc.log[5]$fold5), sum(Sc.MSE[5]$fold5), sum(Sc.quad[5]$fold5), sum(Sc.sph[5]$fold5), sum(Sc.DSS[5]$fold5), sum(Sc.CRPS[5]$fold5))\n)\nsum <- c('Total', colSums(Res.sum)/nrow(train2))\n\nResult_  <- data.frame(rbind(Result_, sum)) \n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\nResult_ <- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] <- 'Improvement'\n\nfor(i in 2:7){\n  Result_[,i] <- as.numeric(Result_[,i])  \n  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\nrownames(Result_) <- NULL\nknitr::kable(Result_, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n```\n\nWe can use the same model to compute scores on the *test* set. Furthermore, it's evident that the XGBoost approach is the most effective.\n\n```{r}\n#| echo: true\n#| cache: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| output: false\n\ndtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(trad.vars)]), label = train2$NB_Claim)\nsetinfo(dtrain,\"base_margin\",log(train2$expo))\nfolds <-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\ndtest <- xgb.DMatrix(data = data.matrix(test2[, paste(trad.vars)]), label = test2$NB_Claim)\nsetinfo(dtest,\"base_margin\",log(test2$expo))\n```\n\n```{r}\n#| echo: true\n#| cache: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_XGBoost_correction333\n#| tbl-cap: Prediction scores for the XGBoost model with traditional covariates\n\nparam <- list(\n  eta = 0.08846194,\n  max_depth = 43,\n  subsample = 0.8389601,\n  min_child_weight = 1,\n  booster = \"gbtree\",\n  objective = \"count:poisson\",\n  eval_metric = \"poisson-nloglik\")\n\nset.seed(333)\nfit.xgb <- xgb.train(params = param,\n                     nrounds = 96,\n                     data = dtrain)\n\ntrain2$pred.xgb <- predict(fit.xgb, dtrain, type='response')\ntest2$pred.xgb <- predict(fit.xgb, dtest, type='response')\n\ntest2$pred.base <- test2$pred.xgb\nResult_ <- data.frame(t(Score.pred(test2$pred.base, test2$NB_Claim)/nrow(test2)))\nResult_ <- cbind('XGBoost', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\", \"Sc.quad\", \"Sc.sph\", \"Sc.DSS\", \"Sc.CRPS\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccccccc\", digits = c(0, 5, 5, 5, 5, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = T)  \n\n\n```\n\n### Variables Importance\n\nA challenge associated with the XGBoost approach is comprehending the full impact of each segmentation variable. The following graph depicts the most crucial variables in the XGBoost model. We observe that the credit score, the territory, the age of the insured, and the years without claim are the most significant covariates in the XGBoost model.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\nparam <- list(\n  eta = 0.08846194,\n  max_depth = 43,\n  subsample = 0.8389601,\n  min_child_weight = 1,\n  booster = \"gbtree\",\n  objective = \"count:poisson\",\n  eval_metric = \"poisson-nloglik\")\n\nset.seed(333)\nfit.xgb <- xgb.train(params = param,\n                     nrounds = 96,\n                     data = dtrain)\n\nimportance_matrix <- xgb.importance(dimnames(dtrain)[[2]], model = fit.xgb)\nxgb.ggplot.importance(importance_matrix,top_n=10) + theme(text = element_text(size=15))\n\n\n```\n\n\n:::\n\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","embed-resources":false,"css":["custom.css"],"toc":true,"output-file":"VarTraditionelles.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","bibliography":["bibtex.bib"],"editor":"source","theme":"sandstone","fontsize":"1.0em","linestretch":1.4,"grid":{"sidebar-width":"250px","body-width":"1000px","margin-width":"250px","gutter-width":"1.5rem"}},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"VarTraditionelles.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"block-headings":true,"bibliography":["bibtex.bib"],"editor":"source","documentclass":"scrreprt"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}