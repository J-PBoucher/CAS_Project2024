{"title":"Traditional Covariates","markdown":{"headingText":"Traditional Covariates","containsRefs":false,"markdown":"\n## Preamble\n\n::: {.panel-tabset}\n\n### Chapter Objective\n\nUsing only traditional covariates, the objective of this chapter is to propose various statistical models for estimation and variable selection to predict the number of claims. More specifically, the following model families will be examined:  \n\n- Basic GLM,  \n- GLM family, including elastic-net,    \n- XGBoost.  \n\nAs mentioned in the theory review chapter, to compare models and strike a balance between bias and variance while avoiding overfitting, an interesting approach is to assess the prediction quality of models when applied to new data. The following R script presents a function for calculating various scores:\n\nTo analyze severities, we define two scores:   \n\n1) the negative loglikelihood based on the Gamma distribution with shape parameter = 1/phi and scale = (mu)(phi), where mu is the expected value and phi is the dispersion parameter;   \n2) the mean squared error (MSE) divided by 1,000,000.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\nScore.pred.sev <- function(mu, x, phi) {\n  Sc.log  <- -sum(dgamma(x, shape = 1/phi, scale = mu*phi, log=TRUE))\n  Sc.MSE  <- sum((x - mu)^2)/1000000\n  return(c(Sc.log, Sc.MSE))\n}\n\n\n```\n\n\n\n### Packages\n\nThe analyses in this chapter will be conducted using the same data as in the previous chapter. Here is the list of packages that will be used:\n  \n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\nlibrary(tidyverse)\nlibrary(vtable)\nlibrary(rpart)\nlibrary(repr)\nlibrary(rpart.plot)\n#library(rfCountData)\nlibrary(gam)\nlibrary(sjPlot)\nlibrary(glmnet)\n\n```\n\n### Data\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\ndataS <- read.csv('Data/Synthetic.csv')\n#dataS <- read.csv(\"~/Library/CloudStorage/Dropbox/AquiLoss/CAS/Telematics/telematics_syn-032021.csv\")\n\ndata <- dataS[dataS$AMT_Claim > 0,]\ndata$M_Claim <- data$AMT_Claim/data$NB_Claim\n\n# Modifications \ndata <- data %>%\n  mutate(Territory = as.factor(Territory)) %>%\n  select(-c('Annual.pct.driven', 'Annual.miles.drive'))\n\ndata.select <- data\n\n# Train-test et folds\nset.seed(123)\ntrain <- data.select %>% sample_frac(0.8, replace = FALSE)\ntest <- data.select %>% anti_join(train)\ntest <- test[-640,]\n\ntrain2 <- train %>%\n  mutate(Miles.per.day = Total.miles.driven/Duration,\n         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),\n         expo = Duration/365.25)\n\ntransform.fct <- function(var){\n  df <- train2 %>% mutate(var_ = get(var)*Total.miles.driven/(1000*Duration))\n  q99 <- quantile(df$var_, 0.99)\n  df <- df %>% mutate(var_ = ifelse(var_ > q99, q99, var_))\n  #colnames(df)[ncol(df)] <- paste0(var, '_')\n  return(df)\n}\n\ntrain2 <- transform.fct(\"Brake.06miles\")\ntrain2 <- transform.fct(\"Brake.08miles\")\ntrain2 <- transform.fct(\"Brake.09miles\")\ntrain2 <- transform.fct(\"Brake.11miles\")\ntrain2 <- transform.fct(\"Brake.14miles\")\ntrain2 <- transform.fct(\"Accel.06miles\")\ntrain2 <- transform.fct(\"Accel.08miles\")\ntrain2 <- transform.fct(\"Accel.09miles\")\ntrain2 <- transform.fct(\"Accel.11miles\")\ntrain2 <- transform.fct(\"Accel.12miles\")\ntrain2 <- transform.fct(\"Accel.14miles\")\ntrain2 <- transform.fct(\"Left.turn.intensity08\")\ntrain2 <- transform.fct(\"Left.turn.intensity09\")\ntrain2 <- transform.fct(\"Left.turn.intensity10\")\ntrain2 <- transform.fct(\"Left.turn.intensity11\")\ntrain2 <- transform.fct(\"Left.turn.intensity12\")\ntrain2 <- transform.fct(\"Right.turn.intensity08\")\ntrain2 <- transform.fct(\"Right.turn.intensity09\")\ntrain2 <- transform.fct(\"Right.turn.intensity10\")\ntrain2 <- transform.fct(\"Right.turn.intensity11\")\ntrain2 <- transform.fct(\"Right.turn.intensity12\")\n\n# Create folds\nnb.fold <- 5\nfold <- sample(1:nb.fold, nrow(train2), replace = TRUE)\ntrain2$fold <- fold\n\n##\n\ntest2 <- test %>%\n  mutate(Miles.per.day = Total.miles.driven/Duration,\n         max.day = pmax(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         min.day = pmin(Pct.drive.mon, Pct.drive.tue, Pct.drive.wed, Pct.drive.thr, Pct.drive.fri, Pct.drive.sat, Pct.drive.sun),\n         max.min = max.day - min.day,\n         Dayformax = 'Monday', \n         Dayformax = ifelse(max.day == Pct.drive.tue, 'Tuesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.wed, 'Wednesday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.thr, 'Thursday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.fri, 'Friday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sat, 'Saturday', Dayformax),\n         Dayformax = ifelse(max.day == Pct.drive.sun, 'Sunday', Dayformax),\n         Dayformin = 'Monday', \n         Dayformin = ifelse(min.day == Pct.drive.tue, 'Tuesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.wed, 'Wednesday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.thr, 'Thursday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.fri, 'Friday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sat, 'Saturday', Dayformin),\n         Dayformin = ifelse(min.day == Pct.drive.sun, 'Sunday', Dayformin),\n         expo = Duration/365.25)\n\ntransform.fct <- function(var){\n  df <- test2 %>% mutate(var_ = get(var)*Total.miles.driven/(1000*Duration))\n  q99 <- quantile(df$var_, 0.99)\n  df <- df %>% mutate(var_ = ifelse(var_ > q99, q99, var_))\n  #colnames(df)[ncol(df)] <- paste0(var, '_')\n  return(df)\n}\n\ntest2 <- transform.fct(\"Brake.06miles\")\ntest2 <- transform.fct(\"Brake.08miles\")\ntest2 <- transform.fct(\"Brake.09miles\")\ntest2 <- transform.fct(\"Brake.11miles\")\ntest2 <- transform.fct(\"Brake.14miles\")\ntest2 <- transform.fct(\"Accel.06miles\")\ntest2 <- transform.fct(\"Accel.08miles\")\ntest2 <- transform.fct(\"Accel.09miles\")\ntest2 <- transform.fct(\"Accel.11miles\")\ntest2 <- transform.fct(\"Accel.12miles\")\ntest2 <- transform.fct(\"Accel.14miles\")\ntest2 <- transform.fct(\"Left.turn.intensity08\")\ntest2 <- transform.fct(\"Left.turn.intensity09\")\ntest2 <- transform.fct(\"Left.turn.intensity10\")\ntest2 <- transform.fct(\"Left.turn.intensity11\")\ntest2 <- transform.fct(\"Left.turn.intensity12\")\ntest2 <- transform.fct(\"Right.turn.intensity08\")\ntest2 <- transform.fct(\"Right.turn.intensity09\")\ntest2 <- transform.fct(\"Right.turn.intensity10\")\ntest2 <- transform.fct(\"Right.turn.intensity11\")\ntest2 <- transform.fct(\"Right.turn.intensity12\")\n\n```\n\n:::\n\n\n## Basic GLM Models\n\n::: {.panel-tabset}\n\n### Single intercept\n\nA baseline model corresponding to a Generalized Linear Model (GLM) with intercept and predicting for each contract only the observed mean multiplied by the observed frequency is used as a point of comparison.\n\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_base_sev\n#| tbl-cap: Prediction scores for the base model (severity)\n\n## Model on each fold\nResult_  <- data.frame()\nResult2_  <- data.frame()\nfor(i in 1:nb.fold) {\n  learn <- train2[train2$fold != i,]\n  valid <- train2[train2$fold == i,]\n  \n  mean <- sum(learn$AMT_Claim)/sum(learn$NB_Claim) \n  variance <- sd(learn$AMT_Claim)^2\n  phi <- variance/mean(learn$AMT_Claim)^2\n  learn$pred.base <- mean*learn$NB_Claim\n  valid$pred.base <- mean*valid$NB_Claim\n  \n  Result_ <- rbind(Result_, c(i, Score.pred.sev(valid$pred.base, valid$AMT_Claim, phi)/nrow(valid)))\n  Result2_ <- rbind(Result2_, c(i, Score.pred.sev(valid$pred.base, valid$AMT_Claim, phi)))\n}\n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ncolnames(Result2_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ntot <- colSums(Result2_)/nrow(train2)\ntot$Fold <- 'Total'\nResult_ <- rbind(Result_ , tot)\n\nResult.base <- Result_  \nBase <- Result.base[nb.fold+1,]\n\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n```\n\nThe model is, therefore, estimated on the entire *train* database, and the predictions are made on the *test* database, which was not used during the calibration phase.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_basetest_sev_tel\n#| tbl-cap: Prediction scores for the base model (testing set) (severity)\n\nmean <- sum(train2$AMT_Claim)/sum(train2$NB_Claim) \nvariance <- sd(train2$AMT_Claim)^2\nphi <- variance/mean(train2$AMT_Claim)^2 \n  \ntest2$pred.base <- mean*test2$NB_Claim\n\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('Base', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- Result_\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n```\n\n\n### Categorical covariates\n  \nA first regression approach is attempted using only the traditional categorical variables, namely:  \n  \n- Gender,  \n- Marital status,  \n- Vehicle usage,  \n- Region.\n\nEven though territory should also be considered since it consists of more than fifty different factors, it will not be integrated into the model immediately.  \nAs we saw in the overview of variables in a previous section, the insured gender did not appear to be an important variable for predicting the number of claims. \nThis GLM approach confirms this observation. Therefore, this variable is excluded from the model. \nIn the table below, we can see the impact of adding traditional variables on the prediction quality.\n\nBelow are the prediction scores of the model with all categorical covariates. As expected, the addition of segmentation variables improves the prediction scores compared to the simple baseline model with only an intercept.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_GLM1_sev\n#| tbl-cap: Prediction scores for the GLM1 model (severity)\n\n## Model \nscore.base <- as.formula(M_Claim ~ 1)\nscore.glm <- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region )\n\n## Model on each fold\nResult_  <- data.frame()\nResult2_  <- data.frame()\nfor(i in 1:nb.fold) {\n  learn <- train2[train2$fold != i,]\n  valid <- train2[train2$fold == i,]\n  glm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = learn)\n  \n  learn$pred.base <- predict(glm.fit, newdata=learn, type='response')*learn$NB_Claim\n  valid$pred.base <- predict(glm.fit, newdata=valid, type='response')*valid$NB_Claim\n  phi <- summary(glm.fit)$dispersion\n  \n  Result_ <- rbind(Result_, c(i, Score.pred.sev(valid$pred.base, valid$AMT_Claim, phi)/nrow(valid)))\n  Result2_ <- rbind(Result2_, c(i, Score.pred.sev(valid$pred.base, valid$AMT_Claim, phi)))\n}\n\n## Model on all data from train\nglm.base <- glm(score.base, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\ntrain2$pred.glm1 <- predict(glm.fit, newdata=train2, type='response')*train2$NB_Claim\nResult.glm1 <- Result_  \n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ncolnames(Result2_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ntot <- colSums(Result2_)/nrow(train2)\ntot$Fold <- 'Total'\nResult_ <- rbind(Result_ , tot)\nResult_ <- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] <- 'Improvement'\n\nfor(i in 2:3){\n  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) <- NULL\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n```\n\n\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_basetest2_sev\n#| tbl-cap: Prediction scores for the GLM model with traditional covariates (testing set) (severity)\n\nscore.glm <- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region )\n\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\ntest2$pred.base <- predict(glm.fit, newdata=test2, type='response')*test2$NB_Claim\nphi <- summary(glm.fit)$dispersion\n\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('GLM (trad.)', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n```\n\n\n\n\n### Estimated Parameters\n\nThe table below shows the estimators obtained for the GLM-Gamma approach and compares them with the baseline model having only an intercept.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-CoeffGLM_sev\n#| tbl-cap: Estimated parameters for the GLM1 model (severity)\n\n## Model \nscore.base <- as.formula(M_Claim ~ 1)\nscore.glm <- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region)\n\n## Model on all data from train\nglm.base <- glm(score.base, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ntab_model(glm.base, glm.fit, transform = NULL)\n\n```\n\n:::\n  \n## GLM-Net\n  \nSome traditional continuous segmentation variables are available:  \n\n  - Credit score,   \n  - Age of the insured,  \n  - Age of the vehicle,  \n  - Number of claim-free years.\n\nFurthermore, the territory is also treated as a continuous variable.\n\nAn approach using Generalized Additive Models (GAM) theory will first be introduced for all these continuous variables. \nThis will allow us to observe the general form of the covariate to explain the number of claims. \nA parametric form will then be proposed to achieve the best possible correspondence with the spline obtained by the GAM.\n\n### Parametric transformation of continuous covariates\n\n::: {.panel-tabset}\n\n### Credit Score\n\nThe first covariate studied is the credit score. We include all categorical variables in the analysis and apply a spline approach with a GAM. The spline analysis indicates that the following parametric form appears to be appropriate for capturing the relationship:\n\n  $$s(Credit.Score) \\approx Credit.Score + Credit.Score^2$$\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: fig-CS_GAM_sev\n#| fig-cap: \"Smoothing of the credit score (severity)\"\n#| fig-width: 9\n#| fig-height: 4\n\nmin_ <- min(train2$Credit.score) \nmax_ <- max(train2$Credit.score) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'Credit.score'\n\ndb <- train2 %>%\n  select(-'Credit.score') %>%\n  dplyr::slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region\n                        + s(Credit.score) )\nscore.glm <- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region\n                        + Credit.score +  I(Credit.score^2))\n\ngam.fit <- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase <- db %>%\n  mutate(diff = abs(Credit.score - mean(train2$Credit.score))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=Credit.score, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=Credit.score, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Credit Score',\n       y = 'Relativity') +\n  theme_classic()\n\n```\n\n### Age of the insured\n\nA spline to examine the relationship between the age of the insured and the claim deverity has also been produced. The most appropriate parametric form is as follows:\n  $$s(Insured.age) \\approx Insured.age  + Insured.age^2$$\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: fig-IA_GAM_sev\n#| fig-cap: \"Smoothing of the age of the insured (severity)\"\n#| fig-width: 9\n#| fig-height: 4\n\nmin_ <- min(train2$Insured.age) \nmax_ <- max(train2$Insured.age) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'Insured.age'\n\ndb <- train2 %>%\n  select(-'Insured.age') %>%\n  dplyr::slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region\n                        + s(Insured.age) )\nscore.glm <- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region\n                        + Insured.age  + I(Insured.age^2)  )\n\ngam.fit <- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase <- db %>%\n  mutate(diff = abs(Insured.age - mean(train2$Insured.age))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=Insured.age, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=Insured.age, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Age of the insured',\n       y = 'Relativity') +\n  theme_classic()\n```\n\n### Age of the car\n\nThe link between the response variable and the car age is approximated by\n\n$$s(Car.age) \\approx Car.age + Car.age^2 + Car.age^3$$\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: fig-CA_GAM_sev\n#| fig-cap: \"Smoothing of the age of the car (severity)\"\n#| fig-width: 9\n#| fig-height: 4\n\n\nmin_ <- min(train2$Car.age) \nmax_ <- max(train2$Car.age) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'Car.age'\n\ndb <- train2 %>%\n  select(-'Car.age') %>%\n  dplyr::slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region\n                        + s(Car.age) )\nscore.glm <- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region\n                        + Car.age + I(Car.age^2)+ I(Car.age^3) )\n\ngam.fit <- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase <- db %>%\n  mutate(diff = abs(Car.age - mean(train2$Car.age))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=Car.age, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=Car.age, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Age of the car',\n       y = 'Relativity') +\n  theme_classic()\n\n```\n\n### Years without claims\n\nFinally, the link between the response variable and the number of year(s) without claims is best approximated by\n\n  $$s(Years.noclaims) \\approx Years.noclaims + Years.noclaims^2 + Years.noclaims^3$$\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: fig-YNC_GAM_sev\n#| fig-cap: \"Smoothing of years without claim (severity)\"\n#| fig-width: 9\n#| fig-height: 4\n\n\nmin_ <- min(train2$Years.noclaims) \nmax_ <- max(train2$Years.noclaims) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'Years.noclaims'\n\ndb <- train2 %>%\n  select(-'Years.noclaims') %>%\n  dplyr::slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region \n                        + s(Years.noclaims) )\nscore.glm <- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region\n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) )\n\ngam.fit <- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase <- db %>%\n  mutate(diff = abs(Years.noclaims - mean(train2$Years.noclaims))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=Years.noclaims, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=Years.noclaims, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Years.noclaims',\n       y = 'Relativity') +\n  theme_classic()\n\n```\n\n### Territory\n\nWe proceed with the covariate Territory as we did for the analysis of the frequency. \nThe parametric function is:\n\n  $$s(terr.code) \\approx terr.code + terr.code^2 + terr.code^3$$\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| fig-width: 9\n#| fig-height: 4\n\n\n# Mean Encoding with White Noise pour les territoires\ncardi <- length(unique(train$Territory))\n\nenc.terr <- train2 %>%\n  group_by(Territory) %>%\n  summarize(sev = sum(AMT_Claim)/sum(NB_Claim)) %>%\n  arrange(sev) %>%\n  mutate(terr.code= row_number()/(cardi+1)) %>%\n  select(Territory, terr.code)\n\ntrain2 <- train2 %>%\n  group_by(Territory) %>%\n  left_join(enc.terr, by='Territory') %>%\n  ungroup()\n\ntest2 <- test2 %>%\n  group_by(Territory) %>%\n  left_join(enc.terr, by='Territory') %>%\n  ungroup()\n\n\n```\n\n\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: fig-terrcode_GAM_sev\n#| fig-cap: \"Smoothing of the territories (encoded) (severity)\"\n#| fig-width: 9\n#| fig-height: 4\n\n\nmin_ <- min(train2$terr.code) \nmax_ <- max(train2$terr.code) \nby_ <-  (max_ - min_)/(nrow(train2)-1) \nadd <- data.frame(seq(min_, max_, by_)) \ncolnames(add) <- 'terr.code'\n\ndb <- train2 %>%\n  select(-'terr.code') %>%\n  dplyr::slice(1) \ndb <- bind_rows(replicate(nrow(train2), db, simplify = FALSE))\ndb <- cbind(db, add)\n\n##\n\nscore.gam <- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region \n                        + s(terr.code) )\nscore.glm <- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region\n                        + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\ngam.fit <- gam(score.gam, family = Gamma(link = \"log\"), data = train2)\nglm.fit <- glm(score.glm, family = Gamma(link = \"log\"), data = train2)\n\ndb$pred.gam <- predict(gam.fit, newdata=db, type='response')*db$NB_Claim\ndb$pred.glm <- predict(glm.fit, newdata=db, type='response')*db$NB_Claim\nbase <- db %>%\n  mutate(diff = abs(terr.code - mean(train2$terr.code))) %>%\n  filter(diff == min(diff))\ndb$pred.gam <- db$pred.gam/base$pred.gam[1]\ndb$pred.glm <- db$pred.glm/base$pred.glm[1]\n\nggplot()+\n  geom_line(aes(x=terr.code, y=pred.gam, color='GAM'), data=db) + \n  geom_line(aes(x=terr.code, y=pred.glm, color='Parametric GLM'), data=db) +\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'terr.code',\n       y = 'Relativity') +\n  theme_classic()\n\n\n\n```\n\n:::\n\n\n### Fitting the GLM-Net model\n\n```{r}\n#| echo: false\n#| eval: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n\nglm.score <- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region \n                        + Credit.score +  I(Credit.score^2) \n                        + Insured.age  + I(Insured.age^2) \n                        + Car.age + I(Car.age^2) + I(Car.age^3)\n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$M_Claim\nfold.id <- train2$fold\n\nlambda_seq <- c(10^seq(0, -8, by = -.1), 0)\ncvfit0  <- cv.glmnet(matrix.x, y, relax=FALSE, family = Gamma(link = \"log\"), lambda = lambda_seq, foldid = fold, alpha = 0)\ncvfit.2 <- cv.glmnet(matrix.x, y, relax=FALSE, family = Gamma(link = \"log\"), lambda = lambda_seq, foldid = fold, alpha = 0.2)\ncvfit.4 <- cv.glmnet(matrix.x, y, relax=FALSE, family = Gamma(link = \"log\"), lambda = lambda_seq, foldid = fold, alpha = 0.4)\ncvfit.6 <- cv.glmnet(matrix.x, y, relax=FALSE, family = Gamma(link = \"log\"), lambda = lambda_seq, foldid = fold, alpha = 0.6)\ncvfit.8 <- cv.glmnet(matrix.x, y, relax=FALSE, family = Gamma(link = \"log\"), lambda = lambda_seq, foldid = fold, alpha = 0.8)\ncvfit1  <- cv.glmnet(matrix.x, y, relax=FALSE, family = Gamma(link = \"log\"), lambda = lambda_seq, foldid = fold, alpha = 1)\n\nc(cvfit0$lambda.min, cvfit.2$lambda.min, cvfit.4$lambda.min, cvfit.6$lambda.min, cvfit.8$lambda.min, cvfit1$lambda.min)\n\nall.min <- data.frame(c(min(cvfit0$cvm), min(cvfit.2$cvm), min(cvfit.4$cvm), min(cvfit.6$cvm), min(cvfit.8$cvm), min(cvfit1$cvm))) %>%\n  mutate(alpha = 2*(row_number()-1)/10)\ncolnames(all.min)[1] <- 'min' \nall.min %>% filter(min == min(min))\n\ncvfit1$lambda.min\ncvfit1$lambda.1se\n\n```\n\n\n::: {.panel-tabset}\n\n### Optimal value\n\nThe parameters of the GLM-net were calibrated using cross-validation to obtain the model hyperparameters. \nUsing these values, we can calculate the prediction scores of the model based on all covariates.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_GLMnet1_sev\n#| tbl-cap: Prediction scores for the GLM-net model (alpha=1) (severity)\n\nglm.score <- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region\n                                  + Credit.score +  I(Credit.score^2) \n                                  + Insured.age + I(Insured.age^2) \n                                  + Car.age + I(Car.age^2) + I(Car.age^3)\n                                  + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nResult_  <- data.frame()\nResult2_  <- data.frame()\nfor(i in 1:nb.fold) {\n    learn <- train2[train2$fold != i,]\n    valid <- train2[train2$fold == i,]\n    \n    matrix.x <- model.matrix(glm.score, data=learn)[,-1]\n    y <- learn$M_Claim\n\n    lambda.min <- 0.01995262\n    lambda.1se <- 0.07943282\n    \n    lambda.select <- lambda.min\n    fit <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, alpha = 0.6, lambda = lambda.select)\n    learn$pred <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*learn$NB_Claim\n    \n    \n    matrix.x <- model.matrix(glm.score, data=valid)[,-1]\n    y <- valid$M_Claim\n\n    valid$pred <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*valid$NB_Claim\n    variance <- (sum((learn$AMT_Claim - learn$pred)^2)/(nrow(learn) - length(fit$beta)))\n    phi <- variance/mean(learn$AMT_Claim)^2\n    \n    Result_ <- rbind(Result_, c(i, Score.pred.sev(valid$pred, valid$AMT_Claim, phi)/nrow(valid)))\n    Result2_ <- rbind(Result2_, c(i, Score.pred.sev(valid$pred, valid$AMT_Claim, phi)))\n}\n\n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ncolnames(Result2_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ntot <- colSums(Result2_)/nrow(train2)\ntot$Fold <- 'Total'\nResult_ <- rbind(Result_ , tot)\nResult_ <- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] <- 'Improvement'\n\nfor(i in 2:3){\n  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) <- NULL\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n```\n\n\nThe same model can be used to compute the scores on the *test* set.\n  \n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_basetest3_sev_tel\n#| tbl-cap: Prediction scores for the GLM-net model  (testing set) \n\nglm.score <- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region\n                                  + Credit.score +  I(Credit.score^2) \n                                  + Insured.age + I(Insured.age^2) \n                                  + Car.age + I(Car.age^2) + I(Car.age^3)\n                                  + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$M_Claim\n\nlambda.min <- 0.01995262\nlambda.1se <- 0.07943282\n\nlambda.select <- lambda.min\nfit <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, alpha = 0.6, lambda = lambda.select)\n#fit <- glmnet(matrix.x, y, family = \"poisson\", relax=TRUE, offset = offset, alpha = 0.6, lambda = lambda.select)\n\ntrain2$pred <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*train2$NB_Claim\ntrain2$pred.tele <- train2$pred\n\nmatrix.x <- model.matrix(glm.score, data=test2)[,-1]\ny <- test2$M_Claim\n\ntest2$pred.base <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*test2$NB_Claim\nvariance <- (sum((train2$AMT_Claim - train2$pred)^2)/(nrow(train2) - length(fit$beta)))\nphi <- variance/mean(train2$AMT_Claim)^2\n\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('LASSO (optimal)', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n```\n\n\n\n### Parsimonious model\n\nInstead of using the optimal value of the penalty $\\lambda$  in the elastic-net approach, it is often advised to use a penalty value located at one standard error ($\\lambda_{1se}$). \nThis helps to obtain a more parsimonious model. The prediction scores of such a model are displayed below.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_GLMnet2_sev\n#| tbl-cap: Prediction scores for the GLM-net model (alpha=1) (severity)\n\nResult_  <- data.frame()\nResult2_  <- data.frame()\nfor(i in 1:nb.fold) {\n  learn <- train2[train2$fold != i,]\n  valid <- train2[train2$fold == i,]\n  \n  matrix.x <- model.matrix(glm.score, data=learn)[,-1]\n  y <- learn$M_Claim\n  \n  lambda.min <- 0.01995262\n  lambda.1se <- 0.07943282\n  \n  lambda.select <- lambda.1se\n  fit <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=TRUE, alpha = 0.6, lambda = lambda.select)\n  learn$pred <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*learn$NB_Claim\n  \n  matrix.x <- model.matrix(glm.score, data=valid)[,-1]\n  y <- valid$M_Claim\n\n  \n  valid$pred <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*valid$NB_Claim\n  variance <- (sum((learn$AMT_Claim - learn$pred)^2)/(nrow(learn) - length(fit$beta)))\n  phi <- variance/mean(learn$AMT_Claim)^2\n  \n  Result_ <- rbind(Result_, c(i, Score.pred.sev(valid$pred, valid$AMT_Claim, phi)/nrow(valid)))\n  Result2_ <- rbind(Result2_, c(i, Score.pred.sev(valid$pred, valid$AMT_Claim, phi)))\n}\n\n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ncolnames(Result2_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\ntot <- colSums(Result2_)/nrow(train2)\ntot$Fold <- 'Total'\nResult_ <- rbind(Result_ , tot)\nResult_ <- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] <- 'Improvement'\n\nfor(i in 2:3){\n  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\n\nrownames(Result_) <- NULL\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n```\n\nThe same model can be used to compute the scores on the *test* set.\n  \n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_basetest3_sev_tel2\n#| tbl-cap: Prediction scores for the GLM-net model  (testing set) \n\nglm.score <- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region\n                                  + Credit.score +  I(Credit.score^2) \n                                  + Insured.age + I(Insured.age^2) \n                                  + Car.age + I(Car.age^2) + I(Car.age^3)\n                                  + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                                  + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$M_Claim\n\nlambda.min <- 0.01995262\nlambda.1se <- 0.07943282\n\nlambda.select <- lambda.1se\n#fit <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, alpha = 0.6, lambda = lambda.select)\nfit <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=TRUE, alpha = 0.6, lambda = lambda.select)\n\ntrain2$pred <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*train2$NB_Claim\ntrain2$pred.tele <- train2$pred\n\nmatrix.x <- model.matrix(glm.score, data=test2)[,-1]\ny <- test2$M_Claim\n\ntest2$pred.base <- predict(fit, newx = matrix.x, type='response', lambda = lambda.select)*test2$NB_Claim\nvariance <- (sum((train2$AMT_Claim - train2$pred)^2)/(nrow(train2) - length(fit$beta)))\nphi <- variance/mean(train2$AMT_Claim)^2\n\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('LASSO (parsimonious)', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n```\n\n\n\n### Categorical covariates\n\nFor categorical variables, the relativity values obtained for both GLM-net approaches are displayed below.\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| layout-ncol: 2\n#| layout-nrow: 2\n#| label: fig-GLMnetcat_sev\n#| fig-cap: \"Interprétation des variables catégorielles du GLM-net (severity)\"\n#| fig-subcap: \n#|   - \"Sex of the insured\"\n#|   - \"Marital status of the insured\"\n#|   - \"Car use\"\n#|   - \"Region\"\n\n\nglm.score <- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region \n                        + Credit.score +  I(Credit.score^2) \n                        + Insured.age + I(Insured.age^2) \n                        + Car.age + I(Car.age^2) + I(Car.age^3) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$M_Claim\n\nlambda.min <- 0.01995262\nlambda.1se <- 0.07943282\n\nlasso.min <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, alpha = 0.6, lambda = lambda.min)\nlasso.1se <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=TRUE, alpha = 0.6, lambda = lambda.1se)\n#cbind(coef(lasso.min),coef(lasso.1se))\n\n### Insured.sex ###\nFemale.min <- 1\nFemale.1se <- 1\nMale.min <- exp(coef(lasso.min)[2])\nMale.1se <- exp(coef(lasso.1se)[2])\n\ndf <- data.frame( Sex = c('Female', 'Male'), \n                  Relativity = c(Female.min, Male.min) ) \ndf2 <- data.frame( Sex = c('Female', 'Male'), \n                   Relativity = c(Female.1se, Male.1se) ) \nggplot() + \n  geom_line(aes(x=as.factor(Sex), y=Relativity, group=1, color='lambda.min'), stat='identity', data=df)+\n  geom_point(aes(x=as.factor(Sex), y=Relativity, group=1, color='lambda.min'), stat='identity', data=df)+\n  geom_line(aes(x=as.factor(Sex), y=Relativity, group=1, color='lambda.1se'), stat='identity', data=df2)+\n  geom_point(aes(x=as.factor(Sex), y=Relativity, group=1, color='lambda.1se'), stat='identity', data=df2)+\n  labs(x = 'Sex of the insured',\n       y = 'Relativity') +\n  ylim(0, 1.2*max(df$Relativity))+\n  guides(color = guide_legend(title = \"\")) +\n  theme_light()\n\n\n### Marital ###\nSingle.min <- 1\nSingle.1se <- 1\nMarried.min <- exp(coef(lasso.min)[3])\nMarried.1se <- exp(coef(lasso.1se)[3])\n\ndf <- data.frame( Marital = c('Married', 'Single'), \n                  Relativity = c(Single.min, Married.min) ) \ndf2 <- data.frame( Marital = c('Married', 'Single'), \n                   Relativity = c(Single.1se, Married.1se) ) \nggplot() + \n  geom_line(aes(x=as.factor(Marital), y=Relativity, group=1, color='lambda.min'), stat='identity', data=df)+\n  geom_point(aes(x=as.factor(Marital), y=Relativity, group=1, color='lambda.min'), stat='identity', data=df)+\n  geom_line(aes(x=as.factor(Marital), y=Relativity, group=1, color='lambda.1se'), stat='identity', data=df2)+\n  geom_point(aes(x=as.factor(Marital), y=Relativity, group=1, color='lambda.1se'), stat='identity', data=df2)+\n  labs(x = 'Marital status of the insured',\n       y = 'Relativity') +\n  ylim(0, 1.2*max(df$Relativity))+\n  guides(color = guide_legend(title = \"\")) +\n  theme_light()\n\n\n### Car Use ###\nCommercial.min <- 1\nCommute.min <- exp(coef(lasso.min)[4])\nFarmer.min <- exp(coef(lasso.min)[5])\nPrivate.min <- exp(coef(lasso.min)[6])\n\nCommercial.1se <- 1\nCommute.1se <- exp(coef(lasso.1se)[4])\nFarmer.1se <- exp(coef(lasso.1se)[5])\nPrivate.1se <- exp(coef(lasso.1se)[6])\n\ndf <- data.frame( Car.use = c('Commercial', 'Commute', 'Farmer', 'Private'), \n                  Relativity = c(Commercial.min, Commute.min, Farmer.min, Private.min) ) \ndf2 <- data.frame( Car.use = c('Commercial', 'Commute', 'Farmer', 'Private'), \n                   Relativity = c(Commercial.1se, Commute.1se, Farmer.1se, Private.1se) ) \nggplot() + \n  geom_line(aes(x=as.factor(Car.use), y=Relativity, group=1, color='lambda.min'), stat='identity', data=df)+ \n  geom_point(aes(x=as.factor(Car.use), y=Relativity, group=1, color='lambda.min'), stat='identity', data=df)+\n  geom_line(aes(x=as.factor(Car.use), y=Relativity, group=1, color='lambda.1se'), stat='identity', data=df2)+ \n  geom_point(aes(x=as.factor(Car.use), y=Relativity, group=1, color='lambda.1se'), stat='identity', data=df2)+\n  labs(x = 'Use of the car',\n       y = 'Relativity') +\n  ylim(0, 1.2*max(df$Relativity))+\n  guides(color = guide_legend(title = \"\")) +\n  theme_light()\n\n\n\n### Region ###\nRural.min <- 1\nRural.1se <- 1\nUrban.min <- exp(coef(lasso.min)[7])\nUrban.1se <- exp(coef(lasso.1se)[7])\n\ndf <- data.frame( Region = c('Rural', 'Urban'), \n                  Relativity = c(Rural.min, Urban.min) ) \ndf2 <- data.frame( Region = c('Rural', 'Urban'), \n                   Relativity = c(Rural.1se, Urban.1se) ) \nggplot() + \n  geom_line(aes(x=as.factor(Region), y=Relativity, group=1, color='lambda.min'), stat='identity', data=df)+\n  geom_point(aes(x=as.factor(Region), y=Relativity, group=1, color='lambda.min'), stat='identity', data=df)+\n  geom_line(aes(x=as.factor(Region), y=Relativity, group=1, color='lambda.1se'), stat='identity', data=df2)+\n  geom_point(aes(x=as.factor(Region), y=Relativity, group=1, color='lambda.1se'), stat='identity', data=df2)+\n  labs(x = 'Region',\n       y = 'Relativity') +\n  ylim(0, 1.2*max(df$Relativity))+\n  guides(color = guide_legend(title = \"\")) +\n  theme_light()\n\n```\n\n### Continuous covariates\n\nAs with categorical variables, the relativities obtained are illustrated below for continuous variables. \n\n```{r}\n#| echo: true\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| layout-ncol: 2\n#| layout-nrow: 3\n#| label: fig-GLMnet_sev\n#| fig-cap: \"Interprétation des variables continues du GLM-net (severity)\"\n#| fig-subcap: \n#|   - \"Credit Score\"\n#|   - \"Annal Miles Drive\"\n#|   - \"Age of the car\"\n#|   - \"Age of the insured\"\n#|   - \"Years without claim\"\n#|   - \"Territory Code\"\n\nglm.score <- as.formula(M_Claim ~ Insured.sex + Marital  +  Car.use + Region\n                        + Credit.score +  I(Credit.score^2) \n                        + Insured.age + I(Insured.age^2) \n                        + Car.age + I(Car.age^2) + I(Car.age^3) \n                        + Years.noclaims + I(Years.noclaims^2)  + I(Years.noclaims^3) \n                        + terr.code + I(terr.code^2)  + I(terr.code^3) )\n\nmatrix.x <- model.matrix(glm.score, data=train2)[,-1]\ny <- train2$M_Claim\n\n\nlambda.min <- 0.01995262\nlambda.1se <- 0.07943282\n\nlasso.min <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=FALSE, alpha = 0.6, lambda = lambda.min)\nlasso.1se <- glmnet(matrix.x, y, family = Gamma(link = \"log\"), relax=TRUE, alpha = 0.6, lambda = lambda.1se)\n#cbind(coef(lasso.min), coef(lasso.1se))\n\n### Credit Score ###\nCredit.score <- seq(from=min(train2$Credit.score), to=max(train2$Credit.score), by=1)\n\nbeta <- coef(lasso.1se)[8:9]\ncurve1 <- exp(beta[1]*Credit.score + beta[2]*Credit.score^2) \nbase1 <- exp(beta[1]*mean(train2$Credit.score) + beta[2]*mean(train2$Credit.score)^2) \n\nbeta <- coef(lasso.min)[8:9]\ncurve2 <- exp(beta[1]*Credit.score + beta[2]*Credit.score^2) \nbase2 <- exp(beta[1]*mean(train2$Credit.score) + beta[2]*mean(train2$Credit.score)^2) \n\ncurve1 <- curve1/base1\ncurve2 <- curve2/base2\ndb <- data.frame(cbind(Credit.score, curve1, curve2))\n\nggplot()+\n  geom_line(aes(x=Credit.score, y=curve1, color = 'lambda.1se' ), data=db)+\n  geom_line(aes(x=Credit.score, y=curve2, color = 'lambda.min' ), data=db)+\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Credit Score',\n       y = 'Relativity') +\n  theme_bw()\n\n### Insured.age \nInsured.age <- seq(from=min(train2$Insured.age ), to=max(train2$Insured.age ), by=1)\nbeta <- coef(lasso.1se)[10:11]\ncurve1 <- exp(beta[1]*Insured.age  + beta[2]*Insured.age^2)       \nbase1  <- exp(beta[1]*mean(train2$Insured.age) + beta[2]*mean(train2$Insured.age)^2) \n\nbeta <- coef(lasso.min)[10:11]\ncurve2 <- exp(beta[1]*Insured.age  + beta[2]*Insured.age^2)       \nbase2  <- exp(beta[1]*mean(train2$Insured.age) + beta[2]*mean(train2$Insured.age)^2) \n\ncurve1 <- curve1/base1\ncurve2 <- curve2/base2\ndb <- data.frame(cbind(Insured.age, curve1, curve2))\n\nggplot()+\n  geom_line(aes(x=Insured.age, y=curve1, color = 'lambda.1se' ), data=db)+\n  geom_line(aes(x=Insured.age, y=curve2, color = 'lambda.min' ), data=db)+\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Age of the insured',\n       y = 'Relativity') +\n  theme_bw()\n\n\n### Car Age ###\nCar.age <- seq(from=min(train2$Car.age), to=max(train2$Car.age), by=1)\nbeta <- coef(lasso.1se)[12:14]\ncurve1 <- exp(beta[1]*Car.age + beta[2]*Car.age^2 + beta[3]*Car.age^3)\nbase1  <- exp(beta[1]*mean(train2$Car.age) + beta[2]*mean(train2$Car.age)^2 + beta[3]*mean(train2$Car.age)^3) \n\nbeta <- coef(lasso.min)[12:14]\ncurve2 <- exp(beta[1]*Car.age + beta[2]*Car.age^2 + beta[3]*Car.age^3)\nbase2  <- exp(beta[1]*mean(train2$Car.age) + beta[2]*mean(train2$Car.age)^2 + beta[3]*mean(train2$Car.age)^3) \n\ncurve1 <- curve1/base1\ncurve2 <- curve2/base2\ndb <- data.frame(cbind(Car.age, curve1, curve2))\n\nggplot()+\n  geom_line(aes(x=Car.age, y=curve1, color = 'lambda.1se' ), data=db)+\n  geom_line(aes(x=Car.age, y=curve2, color = 'lambda.min' ), data=db)+\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Age of the car',\n       y = 'Relativity') +\n  theme_bw()\n\n### Years.noclaims \nYears.noclaims <- seq(from=min(train2$Years.noclaims ), to=max(train2$Years.noclaims ), by=1)\nbeta <- coef(lasso.1se)[15:17]\ncurve1 <- exp(beta[1]*Years.noclaims  + beta[2]*Years.noclaims^2 + beta[3]*Years.noclaims ^3)        \nbase1  <- exp(beta[1]*mean(train2$Years.noclaims) + beta[2]*mean(train2$Years.noclaims)^2 + beta[3]*mean(train2$Years.noclaims)^3) \n\nbeta <- coef(lasso.min)[15:17]\ncurve2 <- exp(beta[1]*Years.noclaims  + beta[2]*Years.noclaims^2 + beta[3]*Years.noclaims ^3)        \nbase2  <- exp(beta[1]*mean(train2$Years.noclaims) + beta[2]*mean(train2$Years.noclaims)^2 + beta[3]*mean(train2$Years.noclaims)^3) \n\ncurve1 <- curve1/base1\ncurve2 <- curve2/base2\ndb <- data.frame(cbind(Years.noclaims, curve1, curve2))\n\nggplot()+\n  geom_line(aes(x=Years.noclaims, y=curve1, color = 'lambda.1se' ), data=db)+\n  geom_line(aes(x=Years.noclaims, y=curve2, color = 'lambda.min' ), data=db)+\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Years without claim',\n       y = 'Relativity') +\n  theme_bw()\n\n### terr.code  \nterr.code  <- seq(from=min(train2$terr.code  ), to=max(train2$terr.code  ), by=0.01)\nbeta <- coef(lasso.1se)[18:20]\ncurve1 <- exp(beta[1]*terr.code + beta[2]*terr.code^2 + beta[3]*terr.code^3)\nbase1  <- exp(beta[1]*mean(train2$terr.code) + beta[2]*mean(train2$terr.code)^2 + beta[3]*mean(train2$terr.code)^3)\n\nbeta <- coef(lasso.min)[18:20]\ncurve2 <- exp(beta[1]*terr.code + beta[2]*terr.code^2 + beta[3]*terr.code^3)\nbase2  <- exp(beta[1]*mean(train2$terr.code) + beta[2]*mean(train2$terr.code)^2 + beta[3]*mean(train2$terr.code)^3)\n\ncurve1 <- curve1/base1\ncurve2 <- curve2/base2\ndb <- data.frame(cbind(terr.code, curve1, curve2))\n\nggplot()+\n  geom_line(aes(x=terr.code, y=curve1, color = 'lambda.1se' ), data=db)+\n  geom_line(aes(x=terr.code, y=curve2, color = 'lambda.min' ), data=db)+\n  guides(color = guide_legend(title = \"\")) +\n  labs(x = 'Territory (encoded)',\n       y = 'Relativity') +\n  theme_bw()\n\n```\n\n:::\n  \n## XGBoost\n\nAs we did for the analysis of claim frequency, we also consider the XGBoost approach for claim severity. We utilized Bayesian optimization to find the hyperparameters of the model.\n  \n```{r}\n#| echo: false\n#| eval: false\n#| message: FALSE\n#| warning: FALSE\n\ntrad.vars <- c(\"Marital\", \"Car.use\", \"Region\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Car.age\", \"Years.noclaims\", \"Territory\") \n\ndtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(trad.vars)]), label = train2$M_Claim)\n#setinfo(dtrain,\"base_margin\",log(train2$expo))\nfolds <-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n\nbounds <- list(eta = c(0.001, 0.5),\n               max_depth = c(1L, 50L),\n               subsample = c(0.1, 1),\n               min_child_weight = c(1, 175))\n\nobj_func <- function(eta, max_depth, subsample, min_child_weight) {\n  param <- list(\n    eta = eta,\n    max_depth = max_depth,\n    subsample = subsample,\n    #min_child_weight = min_child_weight,\n    booster = \"gbtree\",\n    objective = \"reg:gamma\",\n    eval_metric = \"gamma-nloglik\")\n  \n  set.seed(333)\n  xgbcv <- xgb.cv(params = param,\n                  nrounds = base.rounds,\n                  data = dtrain,\n                  folds = folds,\n                  prediction = TRUE,\n                  early_stopping_rounds = 10,\n                  verbose = 0,\n                  maximize = F)\n  \n  lst <- list(\n    Score = -min(xgbcv$evaluation_log$test_gamma_nloglik_mean),\n    nrounds = xgbcv$best_iteration\n  )\n  \n  return(lst)\n}\n\nbase.rounds <- 200\nset.seed(1234)\nbayes_out <- bayesOpt(FUN = obj_func, bounds = bounds, initPoints = length(bounds) + 2, iters.n = 3)\ncomp <- bayes_out$scoreSummary[which(bayes_out$scoreSummary$Score== max(bayes_out$scoreSummary$Score))]\ncomp \n\n```\n\n\n```{r}\n#| echo: false\n#| eval: false\n#| message: FALSE\n#| warning: FALSE\n\nlibrary(pacman)\n\n# p_load automatically installs packages if needed\np_load(xgboost, ParBayesianOptimization, mlbench, dplyr, skimr, recipes, resample)\n\ntrad.vars <- c(\"Marital\", \"Car.use\", \"Region\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Car.age\", \"Years.noclaims\", \"Territory\") \n\ndtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(trad.vars)]), label = train2$M_Claim)\n#setinfo(dtrain,\"base_margin\",log(train2$expo))\nfolds <-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n\nbounds <- list(eta = c(0.01, 0.2),\n               max_depth = c(1L, 10L),\n               subsample = c(0.1, 0.5),\n               min_child_weight = c(1, 175))\n\nobj_func <- function(eta, max_depth, subsample, min_child_weight) {\n  param <- list(\n    eta = eta,\n    max_depth = max_depth,\n    subsample = subsample,\n    min_child_weight = min_child_weight,\n    booster = \"gbtree\",\n    objective = \"reg:gamma\",\n    eval_metric = \"gamma-nloglik\")\n  \n  set.seed(233)\n  xgbcv <- xgb.cv(params = param,\n                  nrounds = base.rounds,\n                  data = dtrain,\n                  folds = folds,\n                  prediction = TRUE,\n                  early_stopping_rounds = 10,\n                  verbose = 0,\n                  maximize = F)\n  \n  lst <- list(\n    Score = -min(xgbcv$evaluation_log$test_gamma_nloglik_mean),\n    nrounds = xgbcv$best_iteration\n  )\n  \n  return(lst)\n}\n\nbase.rounds <- 200\nset.seed(1234)\nbayes_out <- bayesOpt(FUN = obj_func, bounds = bounds, initPoints = length(bounds) + 2, iters.n = 5)\ncomp <- bayes_out$scoreSummary[which(bayes_out$scoreSummary$Score== max(bayes_out$scoreSummary$Score))]\ncomp \n\n```\n\n::: {.panel-tabset}\n\n### Prediction Scores\n\nUsing the hyperparameters we identified, we can calculate the model's prediction scores. The obtained scores demonstrate a notable enhancement compared to alternative approaches evaluated.\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: FALSE\n#| warning: FALSE\n\nlibrary(xgboost)\nlibrary(Ckmeans.1d.dp)\nlibrary(SHAPforxgboost)\n\n\ntrad.vars <- c(\"Marital\", \"Car.use\", \"Region\", \"Insured.sex\", \"Credit.score\", \"Insured.age\", \"Car.age\", \"Years.noclaims\", \"Territory\") \n\ndtrain <- xgb.DMatrix(data = data.matrix(train2[, paste(trad.vars)]), label = train2$M_Claim)\n#setinfo(dtrain,\"base_margin\",log(train2$expo))\nfolds <-list(fold1 = which(train2$fold == 1),\n             fold2 = which(train2$fold == 2),\n             fold3 = which(train2$fold == 3),\n             fold4 = which(train2$fold == 4),\n             fold5 = which(train2$fold == 5))\n\n\n```\n\n\n```{r}\n#| echo: true\n#| cache: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_XGBoost_sev\n#| tbl-cap: Prediction scores for the XGBoost model (severity)\n\n\nparam <- list(\n  eta = 0.1442358,\n  max_depth = 4,\n  subsample = 0.4589511,\n  min_child_weight = 121.1358,\n  booster = \"gbtree\",\n  objective = \"reg:gamma\",\n  eval_metric = \"gamma-nloglik\")\n\nset.seed(333)\nxgbcv <- xgb.cv(params = param,\n                nrounds = 120,\n                data = dtrain,\n                folds = folds,\n                prediction = TRUE,\n                early_stopping_rounds = 10,\n                verbose = 0,\n                maximize = F)\n\n\nvariance <- sapply(xgbcv$folds, function(x){sum((train2$AMT_Claim[x]-unlist(xgbcv$pred[x])*train2$NB_Claim[x])^2)/length(train2$AMT_Claim[x])})  \nmean <- sapply(xgbcv$folds, function(x){mean(train2$AMT_Claim[x])})\nphi <- unlist(variance)/mean^2\n\nSc.log1 <- -dgamma(train2$AMT_Claim[xgbcv$folds$fold1], shape = 1/phi[1], scale = unlist(xgbcv$pred[xgbcv$folds$fold1])*train2$NB_Claim[xgbcv$folds$fold1]*phi[1], log=TRUE)\nSc.log2 <- -dgamma(train2$AMT_Claim[xgbcv$folds$fold2], shape = 1/phi[2], scale = unlist(xgbcv$pred[xgbcv$folds$fold2])*train2$NB_Claim[xgbcv$folds$fold2]*phi[2], log=TRUE)\nSc.log3 <- -dgamma(train2$AMT_Claim[xgbcv$folds$fold3], shape = 1/phi[3], scale = unlist(xgbcv$pred[xgbcv$folds$fold3])*train2$NB_Claim[xgbcv$folds$fold3]*phi[3], log=TRUE)\nSc.log4 <- -dgamma(train2$AMT_Claim[xgbcv$folds$fold4], shape = 1/phi[4], scale = unlist(xgbcv$pred[xgbcv$folds$fold4])*train2$NB_Claim[xgbcv$folds$fold4]*phi[4], log=TRUE)\nSc.log5 <- -dgamma(train2$AMT_Claim[xgbcv$folds$fold5], shape = 1/phi[5], scale = unlist(xgbcv$pred[xgbcv$folds$fold5])*train2$NB_Claim[xgbcv$folds$fold5]*phi[5], log=TRUE)\n\nSc.MSE <- sapply(xgbcv$folds, function(x){(train2$AMT_Claim[x]-unlist(xgbcv$pred[x])*train2$NB_Claim[x])^2/1000000})\n\n\nResult_  <- rbind(\n  c(1,mean(Sc.log1), mean(Sc.MSE[1]$fold1)),\n  c(2,mean(Sc.log2), mean(Sc.MSE[2]$fold2)),\n  c(3,mean(Sc.log3), mean(Sc.MSE[3]$fold3)),\n  c(4,mean(Sc.log4), mean(Sc.MSE[4]$fold4)),\n  c(5,mean(Sc.log5), mean(Sc.MSE[5]$fold5))\n)\n\nRes.sum  <- rbind(\n  c(sum(Sc.log1), sum(Sc.MSE[1]$fold1)),\n  c(sum(Sc.log2), sum(Sc.MSE[2]$fold2)),\n  c(sum(Sc.log3), sum(Sc.MSE[3]$fold3)),\n  c(sum(Sc.log4), sum(Sc.MSE[4]$fold4)),\n  c(sum(Sc.log5), sum(Sc.MSE[5]$fold5))\n)\nsum <- c('Total', colSums(Res.sum)/nrow(train2))\n\nResult_  <- data.frame(rbind(Result_, sum)) \n\n## Show results\ncolnames(Result_) <- c('Fold', \"Sc.log\", \"Sc.MSE\")\nResult_ <- rbind(Result_, Base)\n\nResult_[nb.fold+2,1] <- 'Improvement'\n\nfor(i in 2:3){\n  Result_[,i] <- as.numeric(Result_[,i])  \n  Result_[nb.fold+2,i] <-  Result_[nb.fold+1,i] - Result_[nb.fold+2,i]\n}\n\nrownames(Result_) <- NULL\nknitr::kable(Result_, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n```\n\n\n\nThe same model can be used to compute the scores on the *test* set.  We also observe that the XGBoost approach is the most effective.\n\n```{r}\n#| echo: true\n#| cache: false\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-Pscore_XGBoost_correction333_sev_tel\n#| tbl-cap: Prediction scores for the XGBoost model with traditional covariates (severity)\n\nparam <- list(\n  eta = 0.1442358,\n  max_depth = 4,\n  subsample = 0.4589511,\n  min_child_weight = 121.1358,\n  booster = \"gbtree\",\n  objective = \"reg:gamma\",\n  eval_metric = \"gamma-nloglik\")\n\nset.seed(633)\nfit.xgb <- xgb.train(params = param,\n                     nrounds = 120,\n                     data = dtrain)\n\ntrain2$pred.xgb <- predict(fit.xgb, dtrain, type='response')*train2$NB_Claim\ntrain2$pred.xgb.off <- predict(fit.xgb, dtrain, type='response')\n\ndtest <- xgb.DMatrix(data = data.matrix(test2[, paste(trad.vars)]), label = test2$M_Claim)\n#setinfo(dtest,\"base_margin\",log(test2$expo))\ntest2$pred.xgb <- predict(fit.xgb, dtest, type='response')*test2$NB_Claim\ntest2$pred.xgb.off <- predict(fit.xgb, dtest, type='response')\n\ntest2$pred.base <- test2$pred.xgb\n\nvariance <- (sum((train2$pred.xgb - (train2$AMT_Claim))^2)/(length(train2$AMT_Claim)))\nphi <- variance/mean(train2$AMT_Claim)^2\n\nResult_ <- data.frame(t(Score.pred.sev(test2$pred.base, test2$AMT_Claim, phi)/nrow(test2)))\nResult_ <- cbind('XGBoost', Result_)\ncolnames(Result_) <- c(\"Model\", \"Sc.log\", \"Sc.MSE\")\n\nResult_all <- rbind(Result_all, Result_)\n\nknitr::kable(Result_all, align = \"ccc\", digits = c(0, 5, 5), format.args = list(big.mark = \",\"))%>%   \n  kable_styling(bootstrap_options = \"striped\", full_width = F)  \n\n\n```\n\n\n\n\n\n### Variables Importance\n\nThe following graph depicts the most crucial variables in the XGBoost model.\n\n```{r}\n#| echo: true\n#| message: FALSE\n#| warning: FALSE\n#| code-fold: true\n#| label: tbl-VI_XGBoost_sev\n#| tbl-cap: Variables importance for the XGBoost model (severity)\n\nparam <- list(\n  eta = 0.1442358,\n  max_depth = 4,\n  subsample = 0.4589511,\n  min_child_weight = 121.1358,\n  booster = \"gbtree\",\n  objective = \"reg:gamma\",\n  eval_metric = \"gamma-nloglik\")\n\nset.seed(333)\nfit.xgb <- xgb.train(params = param,\n                     nrounds = 120,\n                     data = dtrain,\n#                     prediction = TRUE,\n                     verbose = 0,\n                     maximize = F)\n\nimportance_matrix <- xgb.importance(dimnames(dtrain)[[2]], model = fit.xgb)\nxgb.ggplot.importance(importance_matrix,top_n=10) + theme(text = element_text(size=15))\n\n\n```\n\n:::\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","embed-resources":false,"css":["custom.css"],"toc":true,"output-file":"severityVarTrad.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","bibliography":["bibtex.bib"],"editor":"source","theme":"sandstone","fontsize":"1.0em","linestretch":1.4,"grid":{"sidebar-width":"250px","body-width":"1000px","margin-width":"250px","gutter-width":"1.5rem"}},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"severityVarTrad.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"block-headings":true,"bibliography":["bibtex.bib"],"editor":"source","documentclass":"scrreprt"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}